"""
TriviaQA dataset loader.

TriviaQA is an open-domain question answering dataset with 95K question-answer pairs.
Questions are written by trivia enthusiasts and have multiple acceptable answers.

Reference: Joshi et al. 2017 - "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset"
"""

from typing import List, Optional, Dict, Any
from datasets import load_dataset
from .base import BaseDataset, DatasetExample


class TriviaQADataset(BaseDataset):
    """
    TriviaQA dataset loader.
    
    Note: TriviaQA is originally a free-form QA dataset. For confidence probing,
    we need to convert it to a format where we can label correctness. This loader:
    1. Loads the question and gold answers
    2. For evaluation, you'll need to generate model answers and compare them
    
    Usage:
        dataset = TriviaQADataset(split="validation", subset="unfiltered")
        
        for example in dataset:
            # Get question
            question = example.question
            
            # Generate answer with your LLM
            model_answer = generate(question)
            
            # Check if correct (flexible matching)
            is_correct = dataset.is_correct(example, model_answer)
    """
    
    def __init__(
        self,
        split: str = "validation",
        subset: str = "unfiltered",
        max_examples: Optional[int] = None
    ):
        """
        Initialize TriviaQA dataset.
        
        Args:
            split: Dataset split ("train", "validation", "test")
            subset: "unfiltered" (full) or "rc" (reading comprehension with context)
            max_examples: Maximum number of examples to load (None = all)
        """
        self.subset = subset
        self.max_examples = max_examples
        super().__init__(split)
    
    def _load_data(self) -> List[DatasetExample]:
        """Load TriviaQA data from HuggingFace datasets."""
        # Load dataset
        dataset = load_dataset(
            "trivia_qa",
            self.subset,
            split=self.split,
            trust_remote_code=True,
            streaming=True
        )

        if self.max_examples is not None:
            dataset = list(dataset)[: self.max_examples]
        else:
            dataset = list(dataset)
        
        # Limit examples if specified
        if self.max_examples is not None:
            dataset = dataset.select(range(min(self.max_examples, len(dataset))))
        
        examples = []
        for item in dataset:
            # TriviaQA has multiple acceptable answers
            gold_answers = []
            if "answer" in item:
                gold_answers.append(item["answer"]["value"])
                if "aliases" in item["answer"]:
                    gold_answers.extend(item["answer"]["aliases"])
            
            # Normalize answers (lowercase, strip)
            gold_answers = [ans.lower().strip() for ans in gold_answers]
            gold_answers = list(set(gold_answers))  # Remove duplicates
            
            example = DatasetExample(
                question=item["question"].strip(),
                choices=gold_answers,  # Store gold answers as "choices"
                answer=0,  # Dummy value (we'll check with is_correct method)
                metadata={
                    "question_id": item.get("question_id", ""),
                    "question_source": item.get("question_source", ""),
                    "entity_pages": item.get("entity_pages", {}),
                    "gold_answers": gold_answers,
                    "subset": self.subset
                }
            )
            examples.append(example)
        
        return examples
    
    def is_correct(
        self, 
        example: DatasetExample, 
        model_answer: str,
        fuzzy_match: bool = True
    ) -> bool:
        """
        Check if model answer matches any gold answer.
        
        Args:
            example: The dataset example
            model_answer: Answer generated by the model
            fuzzy_match: If True, use flexible matching (substring, normalization)
            
        Returns:
            True if answer is correct, False otherwise
        """
        model_answer = model_answer.lower().strip()
        gold_answers = example.metadata["gold_answers"]

        # CHANGE: If the model produces an empty / whitespace-only answer,
        # we should never treat it as correct, even under fuzzy matching.
        if not model_answer:
            return False
        
        if not fuzzy_match:
            # Exact match
            return model_answer in gold_answers
        
        # CHANGE: Pre-compute normalized model answer once instead of per-gold.
        model_clean = self._normalize_answer(model_answer)
        
        for gold in gold_answers:
            # CHANGE: Normalize each gold answer once and compare equality first.
            gold_clean = self._normalize_answer(gold)

            # Exact normalized match (robust to articles/punctuation/etc).
            if model_clean == gold_clean:
                return True
            
            # CHANGE: Make substring matching strictly one-directional and safer:
            # we only allow "gold in model" and require the gold string to have
            # some minimum length to avoid tiny/generic tokens ("of", "to", etc.).
            if len(gold_clean) >= 3 and gold_clean in model_clean:
                return True
        
        return False
    
    @staticmethod
    def _normalize_answer(text: str) -> str:
        """Normalize answer for comparison."""
        import re
        import string
        
        # Lowercase
        text = text.lower()
        
        # Remove articles
        text = re.sub(r'\b(a|an|the)\b', ' ', text)
        
        # Remove punctuation
        text = text.translate(str.maketrans('', '', string.punctuation))
        
        # Remove extra whitespace
        text = ' '.join(text.split())
        
        return text.strip()
    
    def get_statistics(self) -> Dict[str, Any]:
        """Get TriviaQA-specific statistics."""
        base_stats = super().get_statistics()
        
        # Average number of gold answers per question
        num_answers = [len(ex.metadata["gold_answers"]) for ex in self.data]
        avg_answers = sum(num_answers) / len(num_answers) if num_answers else 0
        
        return {
            **base_stats,
            "subset": self.subset,
            "avg_gold_answers": avg_answers,
            "max_gold_answers": max(num_answers) if num_answers else 0,
            "min_gold_answers": min(num_answers) if num_answers else 0
        }


# Helper function for generating confidence labels
def generate_trivia_labels(
    dataset: TriviaQADataset,
    model_answers: List[str],
    fuzzy_match: bool = True
) -> List[int]:
    """
    Generate correctness labels for TriviaQA given model answers.
    
    Args:
        dataset: TriviaQA dataset
        model_answers: List of model-generated answers (same length as dataset)
        fuzzy_match: Use flexible matching
        
    Returns:
        List of labels (1 = correct, 0 = incorrect)
    """
    if len(model_answers) != len(dataset):
        raise ValueError(
            f"Number of answers ({len(model_answers)}) must match "
            f"dataset size ({len(dataset)})"
        )
    
    labels = []
    for example, answer in zip(dataset, model_answers):
        is_correct = dataset.is_correct(example, answer, fuzzy_match)
        labels.append(1 if is_correct else 0)
    
    return labels
