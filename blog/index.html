<html>
<head>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

<!-- MathJax for consistent math rendering across browsers -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/mml-chtml.js"></script>

<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
	body {
		background-color: #f5f9ff;
	}

	.content-margin-container {
		display: flex;
		width: 100%;
		justify-content: left;
		align-items: center;
	}
	.main-content-block {
		width: 70%;
    max-width: 1100px;
		background-color: #fff;
		border-left: 1px solid #DDD;
		border-right: 1px solid #DDD;
		padding: 8px 8px 8px 8px;
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
	}
	.margin-left-block {
			font-size: 14px;
			width: 15%;
			max-width: 130px;
			position: relative;
			margin-left: 10px;
			text-align: left;
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
			padding: 5px;
	}
	.margin-right-block {
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
			font-size: 14px;
			width: 25%;
			max-width: 256px;
			position: relative;
			text-align: left;
			padding: 10px;
	}

	img {
			max-width: 100%;
			height: auto;
			display: block;
			margin: auto;
	}

	a:link,a:visited
	{
		color: #0e7862;
		text-decoration: none;
	}
	a:hover {
		color: #24b597;
	}

	h1 {
		font-size: 18px;
		margin-top: 4px;
		margin-bottom: 10px;
	}

	table.header {
    font-weight: 300;
    font-size: 17px;
    flex-grow: 1;
		width: 70%;
    max-width: calc(100% - 290px);
	}
	table td, table td * {
	    vertical-align: middle;
	    position: relative;
	}

	hr {
    height: 1px;
    border: none;
    background-color: #DDD;
  }

	div.hypothesis {
		width: 80%;
		background-color: #EEE;
		border: 1px solid black;
		border-radius: 10px;
		font-family: Courier;
		font-size: 16px;
		text-align: center;
		margin: auto;
		padding: 16px 16px 16px 16px;
	}

	div.citation {
    font-size: 0.8em;
    background-color:#fff;
    padding: 10px;
		height: 200px;
  }

	table.results {
		border-collapse: collapse;
		width: 100%;
		margin: 10px 0;
	}
	table.results th, table.results td {
		border: 1px solid #DDD;
		padding: 8px;
		text-align: left;
	}
	table.results th {
		background-color: #f5f9ff;
	}

	code {
		background-color: #f5f5f5;
		padding: 2px 6px;
		border-radius: 3px;
		font-family: 'Courier New', Courier, monospace;
		font-size: 14px;
	}

	pre {
		background-color: #f5f5f5;
		padding: 12px;
		border-radius: 5px;
		overflow-x: auto;
		font-family: 'Courier New', Courier, monospace;
		font-size: 13px;
	}

</style>

	  <title>Probing the Mind of LLMs: Extracting Confidence from Hidden States</title>
      <meta property="og:title" content="Probing the Mind of LLMs: Extracting Confidence from Hidden States" />
			<meta charset="UTF-8">
  </head>

  <body>

		<!-- HEADER -->
		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<table class="header" align=left>
								<tr>
									<td colspan=4>
										<span style="font-size: 32px; font-family: 'Courier New', Courier, monospace;">Probing the Mind of LLMs</span>
									</td>
								</tr>
								<tr>
									<td colspan=4>
										<span style="font-size: 20px; color: #666;">Extracting Confidence from Hidden States</span>
									</td>
								</tr>
								<tr>
										<td align=left>
												<span style="font-size:17px"><a href="#">Joshua Liu</a></span>
										</td>
										<td align=left>
												<span style="font-size:17px"><a href="#">Carol Li</a></span>
										</td>
										<td align=left>
												<span style="font-size:17px"><a href="#">Maureen Zhang</a></span>
										</td>
								<tr>
									<td colspan=4 align=left><span style="font-size:18px">Final project for 6.7960, MIT</span></td>
								</tr>
						</table>
					</div>
					<div class="margin-right-block">
					</div>
		</div>

    <!-- INTRODUCTION -->
    <div class="content-margin-container" id="intro">
				<div class="margin-left-block">
          <div style="position:fixed; max-width:inherit; top:max(20%,120px)">
              <b style="font-size:16px">Outline</b><br><br>
              <a href="#intro">Introduction</a><br><br>
              <a href="#background">Background</a><br><br>
              <a href="#methodology">Methodology</a><br><br>
              <a href="#architecture">Our Approach</a><br><br>
              <a href="#results">Results</a><br><br>
              <a href="#analysis">Discussion</a><br><br>
              <a href="#conclusion">Conclusion</a><br><br>
          </div>
				</div>
		    <div class="main-content-block">
						<h1>1. Introduction</h1>

            Imagine asking an AI assistant about a medication interaction and getting a fluent, confident, citation-filled answer—only to later discover that the information was completely wrong. This is not a rare edge case but a routine behavior of large language models (LLMs), which often produce plausible-sounding falsehoods with the same apparent confidence as established facts <a href="#ref_1">[1]</a>. These “hallucinations” are especially concerning in high-stakes domains like law, medicine, and science, where fabricated precedents, incorrect dosages, or fictional citations can have real consequences <a href="#ref_2">[2]</a>. The problem is not just that models make mistakes; it is that they offer little reliable signal about when they are likely to be wrong.<br><br>

            This raises a natural question: can models “know what they don’t know,” and if so, can we access that knowledge through their internal representations rather than their surface behavior?<br><br>

            <div class="hypothesis">
              <b>Central Hypothesis:</b> A model's internal representation of uncertainty can diverge from the confidence it expresses in its outputs. By probing hidden layers with both linear and nonlinear architectures, we can detect cases where the model is internally uncertain despite issuing confident predictions.
            </div><br>

            There is growing evidence that such internal signals exist. Kadavath et al. <a href="#ref_3">[3]</a> showed that LLMs perform above chance when asked to predict whether their own answers are correct. Burns et al. <a href="#ref_4">[4]</a> found that hidden states alone can separate true from false statements, suggesting that something like “truthfulness” is already encoded internally. Layer-wise studies further indicate that middle transformer layers encode richer semantic information than final layers <a href="#ref_5">[5]</a>, hinting that the model’s “thinking” may peak in the middle, with later layers mostly formatting outputs.<br><br>

            What remains unclear is how uncertainty is organized inside the network and how best to extract it. Is uncertainty linearly encoded, or does it require more expressive probes? Is it localized to specific layers or distributed across many? And how does internal uncertainty relate to the model’s expressed confidence at the logits?<br><br>

            In this work, we take a systematic probing-based approach to these questions, comparing linear and nonlinear probes across layers and models, and exploring how internal signals interact with output-based confidence.
		    </div>
		    <div class="margin-right-block">
						The hallucination problem is particularly insidious because users cannot easily distinguish confident correct responses from confident incorrect ones without external verification.<br><br>

            <b>Why does this matter?</b> Reliable uncertainty estimates enable selective prediction (abstaining on hard cases), retrieval augmentation (calling external knowledge when needed), and human-in-the-loop workflows (routing dubious answers to experts), making LLMs safer and more trustworthy in real-world use.
		    </div>
		</div>


		<!-- BACKGROUND AND RELATED WORK -->
		<div class="content-margin-container" id="background">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
					<h1>2. Background and Related Work</h1>

          Before diving into our approach, it's worth understanding where uncertainty quantification has been and why the problem remains challenging.<br><br>

        <b>2.1 Classical Approaches</b>
          <p>
             Before the rise of modern large language models, researchers had already been grappling
             with a central question in machine learning: <i>how can we quantify a model’s uncertainty?</i>
             Three classical approaches emerged:
             </p>
             
             <ul>
             <li>
             	<b>Bayesian neural networks</b> <a href="#ref_7">[7]</a> place distributions over weights
             	instead of using point estimates, so uncertainty is obtained from the posterior.
             </li>
             <li>
             	<b>Monte Carlo dropout</b> <a href="#ref_8">[8]</a> keeps dropout active at test time,
             	runs the model multiple times, and uses the variation across predictions as an
             	uncertainty estimate.
             </li>
             <li>
             	<b>Deep ensembles</b> <a href="#ref_9">[9]</a> train several independent models and
             	quantify uncertainty via their disagreement.
             </li>
             </ul>
             
             <p>
             All three methods effectively treat uncertainty as dispersion across predictions. They work
             well for smaller networks, but at the scale of billion-parameter LLMs, training or sampling
             many models becomes prohibitively expensive, motivating single-model, single–forward-pass
             approaches to uncertainty.
             </p>
             
             
        <b>2.2 Uncertainty in the Age of LLMs</b>
          <p>
          The transformer revolution changed the game: LLMs are so large that
          classical ensemble-style methods are often impractical, and the connection between token
          probabilities and factual correctness is surprisingly weak. In practice, three main families
          of approaches have been explored.
          </p>
          
          <p><b>Token probability methods</b> treat the model’s softmax scores as confidence: if the model assigns high probability
          to an answer, we interpret that as high certainty. Jiang et al.
          <a href="#ref_11">[11]</a> showed that these probabilities are often poorly calibrated, with models
          assigning very high probability to incorrect answers, especially in open-ended settings.
          This gap between next-token confidence and actual factual correctness motivates looking
          inside the model rather than only at its output distribution.
          </p>
          
          <p><b>Consistency-based methods</b> generate multiple responses to the same prompt and measure how much they agree.
          Kuhn et al. <a href="#ref_6">[6]</a> introduced semantic entropy, which first clusters responses by
          meaning and then computes entropy over those clusters, achieving strong hallucination
          detection. The downside is computational cost—typically 5–10 forward passes per query—and
          the fact that it mainly captures uncertainty about <i>what to say</i>, whereas our work targets
          uncertainty about <i>being correct</i> in a single forward pass.
          </p>
          
          <p><b>Verbalized confidence methods</b> typically ask the model how confident it is and treat its self-report as
          an uncertainty estimate. Xiong et al. <a href="#ref_12">[12]</a> found that while LLMs can express
          graded confidence, they tend to be systematically overconfident and sensitive to prompt
          phrasing. This mismatch between expressed and actual uncertainty raises the question of
          whether internal representations carry a more reliable confidence signal than the model’s
          own words.
          </p>

        <b>2.3 Probing Internal Representations</b>
          <p>
          Our approach fits into a fourth paradigm: probing internal representations. The basic idea
          is that if a model has already computed something like “confidence” internally, we should
          be able to read it out by training a small classifier on its hidden states. This aims to
          keep the single–forward-pass efficiency of logit-based methods while tapping into
          potentially richer signals than output probabilities or verbalized confidence.
          </p>
          
          <p>
          Probing itself has a long history in NLP. Hewitt and Manning
          <a href="#ref_13">[13]</a> showed that syntactic structure can be linearly recovered from BERT’s
          hidden space, suggesting that if a property is linearly extractable, the model has likely
          computed it explicitly—whereas properties that only appear under complex nonlinear probes
          may be artifacts of the probe. For uncertainty, Kadavath et al. <a href="#ref_3">[3]</a> found that
          large LLMs can meaningfully judge their own answers, and Burns et al.
          <a href="#ref_4">[4]</a> proposed Contrast-Consistent Search (CCS), which recovers a truthfulness signal
          from hidden states without labels, though it relies on contrastive statement pairs and does
          not directly handle open-ended generation.
          </p>
          
          <p>
          More recent work asks <i>where</i> this kind of information lives. Azaria and Mitchell
          <a href="#ref_14">[14]</a> trained classifiers on hidden states and found strong error signals,
          especially in intermediate layers. Gekhman et al. <a href="#ref_15">[15]</a> showed that models can
          encode the right answer internally yet still output something wrong, implying that internal
          representations may be more informative than the generated text itself. This motivates our
          focus on probing hidden states to study how uncertainty is represented inside the model.
          </p>
          
		<b>2.4 Calibration: When Confidence Matches Reality</b>
          <p>
          A model is well-calibrated if its stated confidence matches its accuracy: when it predicts
          70% confidence, it should be correct about 70% of the time. This is critical in deployment
          settings, where users need to know when to trust model outputs.
          </p>
          
          <p>
          Guo et al. <a href="#ref_19">[19]</a> showed that modern neural networks are often
          overconfident and introduced temperature scaling and Expected Calibration Error (ECE) as
          standard tools for post-hoc calibration. However, ECE can be misleading in simple cases
          (e.g., constant-confidence predictions), since a model can appear well-calibrated without
          providing informative uncertainty.
          </p>
          
          <p>
          The Brier score <a href="#ref_20">[20]</a> addresses this by acting as a proper scoring rule,
          minimized only when predicted probabilities match true frequencies. It penalizes both being
          wrong and being confidently wrong. In our work, we train probes with Brier loss so they
          learn calibrated probabilities rather than just accurate binary predictions, making them
          directly comparable to other uncertainty quantification methods.
          </p>

		<b>2.5 Research Gaps We Address</b>
          <p>
          Despite substantial progress, key questions remain about how LLMs represent and express uncertainty.
          </p>
          
          <ol>
            <li>
              <b>Linear vs. non-linear encoding.</b><br>
              Most probing work relies on linear classifiers
              <a href="#ref_13">[13]</a><a href="#ref_14">[14]</a>, so it is unclear whether uncertainty is
              linearly extractable or requires more expressive decoders.<br>
              <i>Our contribution:</i> We compare 13 probe architectures—from linear models to MLPs,
              attention, residual, and sparse-selection probes—to test competing hypotheses about the
              geometric structure of uncertainty in hidden states.
            </li>
            <br>
            <li>
              <b>Multi-layer information.</b><br>
              Prior work typically probes single layers <a href="#ref_14">[14]</a> or uses naive
              concatenation, leaving open how uncertainty signals spread across early, middle, and late
              layers.<br>
              <i>Our contribution:</i> We evaluate four fusion strategies—concatenation, learned weighted
              ensembles, attention-based aggregation, and combining hidden states with logits—to identify
              how to best aggregate cross-layer uncertainty information.
            </li>
            <br>
            <li>
              <b>Internal vs. expressed confidence.</b><br>
              Existing studies usually focus on either hidden-state signals
              <a href="#ref_14">[14]</a> or output probabilities <a href="#ref_11">[11]</a> in isolation, so
              the role of their disagreement as a miscalibration signal is underexplored.<br>
              <i>Our contribution:</i> We introduce a Fusion Probe that jointly uses hidden-state features
              and logit-based confidence, and show that their interaction yields our strongest
              miscalibration detection performance.
            </li>
            <br>
            <li>
              <b>Architectural assumptions.</b><br>
              Different probe architectures encode different assumptions about how uncertainty is
              structured (e.g., localized regions for attention, pairwise interactions for bilinear
              forms), yet there is no systematic comparison of these assumptions.<br>
              <i>Our contribution:</i> We run a broad architectural study and find that simple,
              capacity-limited probes often match or outperform more complex designs, suggesting that
              uncertainty behaves as a low-dimensional, largely linearly accessible feature.
            </li>
            <br>
            <li>
              <b>Where is uncertainty encoded? (The Layer Question).</b><br>
              While prior work indicates that intermediate layers carry rich semantic information
              <a href="#ref_5">[5]</a>, it is unclear whether uncertainty in particular is concentrated there
              or closer to the output layers.<br>
              <i>Our contribution:</i> By probing all 32 layers and learning ensemble weights over them,
              we show that middle layers dominate uncertainty prediction, indicating where uncertainty is
              primarily encoded in our model.
            </li>
          </ol>

		</div>
		    <div class="margin-right-block" style="transform: translate(0%, -30%);">
          
		    </div>
		</div>

		<!-- METHODOLOGY -->
		<div class="content-margin-container" id="methodology">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
            <h1>3. Methodology</h1>

            <b>3.1 Problem Formulation</b><br><br>

            Given an LLM M, a question q, and the model's generated answer a, we aim to predict whether a is correct. Formally, let h<sup>(l)</sup> &in; &#x211D;<sup>d</sup> denote the hidden state at layer l, and let z &in; &#x211D;<sup>K</sup> denote the output logits for K answer choices. We train a probe f<sub>&theta;</sub> to predict:<br><br>
            <center>
              <math xmlns="http://www.w3.org/1998/Math/MathML">
                <mrow>
                  <mover>
                    <mi>p</mi>
                    <mo>^</mo>
                  </mover>
                  <mo>=</mo>
                  <msub>
                    <mi>f</mi>
                    <mi>&theta;</mi>
                  </msub>
                  <mo>(</mo>
                  <msup><mi>h</mi><mrow><mo>(</mo><msub><mi>l</mi><mn>1</mn></msub><mo>)</mo></mrow></msup>
                  <mo>,</mo>
                  <mo>...</mo>
                  <mo>,</mo>
                  <msup><mi>h</mi><mrow><mo>(</mo><msub><mi>l</mi><mi>k</mi></msub><mo>)</mo></mrow></msup>
                  <mo>,</mo>
                  <mi>z</mi>
                  <mo>)</mo>
                  <mo>&isin;</mo>
                  <mo>[</mo>
                  <mn>0</mn>
                  <mo>,</mo>
                  <mn>1</mn>
                  <mo>]</mo>
                </mrow>
              </math>
            </center><br>
            where p&#770; is the predicted probability that a is correct.<br><br>

            <b>3.2 Datasets</b><br><br>

            We evaluate on the widely-used <b>MMLU</b> <a href="#ref_23">[23]</a> (Massive Multitask Language Understanding) benchmark, spanning 57 subjects from STEM to humanities with 4-choice questions. It tests a combination of knowledge recall and reasoning across diverse domains, making it suitable for evaluating whether uncertainty signals generalize across subject matter.<br><br>

            We use the MMLU validation set, sampling <b>2,000 examples</b> for our experiments. This provides sufficient data for robust probe training while remaining tractable for hidden state extraction.<br><br>

            <b>3.3 Answer Generation and Labels</b><br><br>

            A critical design choice is how we obtain the model's "answer" to evaluate. We experiment with two approaches:<br><br>

            <b>Logit-based prediction:</b> We examine the logits for answer tokens (A, B, C, D) at the last token position and select the highest. This measures the model's "internal preference" among answer choices.<br><br>

            <b>Generation-based prediction:</b> We prompt the model to generate an answer and parse the output. This better reflects real-world deployment but introduces parsing complexity.<br><br>

            Each example receives a <b>binary correctness label</b>: 1 if the model's answer matches ground truth, 0 otherwise. We do not use partial credit; the model is either right or wrong. This clean supervision signal makes probe training straightforward.<br><br>

            <b>3.4 Model and Hidden State Extraction</b><br><br>

            <ul>
              <li><b>Model selection:</b> We evaluated several 7B-scale models including Llama-2-7B, Mistral-7B, and Qwen2.5-7B. We ultimately chose <b>Qwen2.5-7B</b> (28 layers, 3584 hidden dimension) for several reasons. First, it offers strong performance and accessibility without gating restrictions. More importantly, preliminary experiments revealed that probes trained on Mistral-7B hidden states exhibited poor calibration, with Brier scores approximately 0.10 higher than those obtained with Qwen2.5-7B. This suggests that Qwen's hidden states encode uncertainty information in a more linearly extractable form, making it better suited for our probing approach. While we did not conduct full experiments with Llama-2-7B, Qwen's superior calibration properties and comparable performance made it the natural choice for our systematic evaluation.</li>
              <li><b>Quantization:</b> 8-bit quantization via bitsandbytes <a href="#ref_24">[24]</a> enables extraction on consumer GPUs. Hidden states are converted to float32 for probe training.</li>
              <li><b>Layer selection:</b> We extract from quartile positions [7, 14, 21, 27], capturing early (syntactic), middle (semantic), and late (task-specific) processing stages.</li>
              <li><b>Token position:</b> We use the <b>last token</b> hidden state, which aggregates context via the attention mechanism and represents the model's "decision point" before generating an answer.</li>
            </ul><br>

            <b>3.5 Training Protocol</b><br><br>

            We use <b>k-fold cross-validation</b> for robust evaluation. This allows us to use all data for both training and evaluation while avoiding overfitting to a single train/test split.<br><br>

            All probes are trained with:
            <ul>
              <li><b>Loss:</b> Brier score (squared error between predicted probability and binary label):
                <math xmlns="http://www.w3.org/1998/Math/MathML" style="display: inline-block; vertical-align: middle; margin-left: 8px;">
                  <msup>
                    <mrow>
                      <mo>(</mo>
                      <mover><mi>p</mi><mo>^</mo></mover>
                      <mo>-</mo>
                      <mi>y</mi>
                      <mo>)</mo>
                    </mrow>
                    <mn>2</mn>
                  </msup>
                </math>
              </li>
              <li><b>Optimizer:</b> AdamW with learning rate 1e-3 and weight decay 1e-5</li>
              <li><b>Schedule:</b> Cosine annealing over 100 epochs (decaying to 1% of initial LR)</li>
              <li><b>Selection:</b> Best validation Brier score checkpoint restored after training</li>
            </ul><br>

            Why Brier score instead of binary cross-entropy? BCE can be minimized by making confident predictions regardless of accuracy, while Brier score is a <i>proper scoring rule</i>: it's uniquely minimized when predictions equal true probabilities. This encourages calibration during training, not just discrimination.<br><br>

            <b>3.6 Probe Architectures</b><br><br>

            We systematically evaluate 13 probe architectures, each testing specific hypotheses about how uncertainty is encoded in hidden states. These range from simple linear classifiers to complex networks with attention mechanisms and hierarchical processing. We organize them by the hypothesis they test.<br><br>

            <b>Baseline Probes (Testing Linearity)</b><br><br>

            <table class="results">
              <tr>
                <th style="width: 20%">Architecture</th>
                <th style="width: 65%">Hypothesis & Description</th>
                <th style="width: 15%">Parameters</th>
              </tr>
              <tr>
                <td><b>Linear Probe</b></td>
                <td><b>Hypothesis:</b> Uncertainty is a single direction in hidden space.<br><br>
                A single linear layer maps the hidden state to a confidence score. If the linear representation hypothesis <a href="#ref_16">[16]</a> holds for uncertainty, this simple classifier should suffice. We train with Brier loss rather than cross-entropy, encouraging calibration.</td>
                <td>~4K</td>
              </tr>
              <tr>
                <td><b>MLP Probe</b></td>
                <td><b>Hypothesis:</b> Uncertainty requires non-linear feature interactions.<br><br>
                A 2-layer MLP with ReLU activations. Tests whether correctness prediction requires comparing activations across dimensions. Comparing Linear vs MLP directly tests whether uncertainty is linearly encoded.</td>
                <td>~1M</td>
              </tr>
            </table><br>

            <b>Structural Hypotheses (How Information is Organized)</b><br><br>

            <table class="results">
              <tr>
                <th style="width: 20%">Architecture</th>
                <th style="width: 65%">Hypothesis & Description</th>
                <th style="width: 15%">Parameters</th>
              </tr>
              <tr>
                <td><b>SparseProbe</b></td>
                <td><b>Hypothesis:</b> The uncertainty signal is concentrated in a small subset of dimensions.<br><br>
                Learns importance weights for each dimension via differentiable soft weighting. Can identify top-k most informative dimensions for interpretability.</td>
                <td>~150K</td>
              </tr>
              <tr>
                <td><b>HierarchicalProbe</b></td>
                <td><b>Hypothesis:</b> Uncertainty exists at multiple levels of granularity.<br><br>
                Processes hidden states hierarchically: fine-grained chunks → mid-level attention aggregation → semantic processing → global prediction. Each level builds on the previous.</td>
                <td>~400K</td>
              </tr>
            </table><br>


            <b>Multi-Layer Architectures (Combining Information Across Layers)</b><br><br>

            <table class="results">
              <tr>
                <th style="width: 20%">Architecture</th>
                <th style="width: 65%">Hypothesis & Description</th>
                <th style="width: 15%">Parameters</th>
              </tr>
              <tr>
                <td><b>Ensemble Probe</b></td>
                <td><b>Hypothesis:</b> Uncertainty is distributed across layers, with complementary signals at different depths.<br><br>
                Four parallel linear probes (one per quartile layer) with learned softmax-normalized combination weights. Allows empirical testing of the "middle layers are optimal" hypothesis by inspecting learned weights.</td>
                <td>~20K</td>
              </tr>
              <tr>
                <td><b>Fusion Probe</b></td>
                <td><b>Hypothesis:</b> Internal uncertainty (hidden states) and expressed confidence (logits) can diverge.<br><br>
                Combines per-layer MLPs, cross-layer attention, and logit feature extraction (entropy, margin, max probability). Designed to detect miscalibration: when model "knows it doesn't know" internally but expresses high confidence.</td>
                <td>~100K</td>
              </tr>
            </table><br>

            <b>Design Philosophy:</b> We organize architectures by hypothesis to systematically test different assumptions about uncertainty encoding. Linear Probe and MLP Probe test linearity. Sparse/Hierarchical test structural hypotheses about how information is organized. Ensemble and Fusion test multi-layer hypotheses. In our results (Section 5), we focus on the 4 most informative architectures (Linear, MLP, Ensemble, Fusion) that best illustrate the key findings, but all 6 architectures were evaluated.<br><br>

            <b>3.7 Evaluation Metrics</b><br><br>

            We evaluate probes on three complementary metrics that capture different aspects of uncertainty quantification:
            
            <ul>
              <li><b>AUROC</b> (Area Under ROC Curve): Measures <i>discrimination</i>, the ability to rank correct answers higher than incorrect ones. Ranges from 0 to 1, where 0.5 indicates random performance and 1.0 is perfect. This metric ignores calibration and focuses purely on relative ordering.</li>
              
              <li><b>Brier Score</b> <a href="#ref_20">[20]</a>: A <i>proper scoring rule</i> measuring both calibration and discrimination simultaneously. Computed as:
                <center>
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mrow>
                      <mi>Brier</mi>
                      <mo>=</mo>
                      <mfrac>
                        <mn>1</mn>
                        <mi>N</mi>
                      </mfrac>
                      <munderover>
                        <mo>&sum;</mo>
                        <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
                        <mi>N</mi>
                      </munderover>
                      <msup>
                        <mrow>
                          <mo>(</mo>
                          <msub><mi>p</mi><mi>i</mi></msub>
                          <mo>-</mo>
                          <msub><mi>y</mi><mi>i</mi></msub>
                          <mo>)</mo>
                        </mrow>
                        <mn>2</mn>
                      </msup>
                    </mrow>
                  </math>
                </center><br>
                Lower is better (0 is perfect). Unlike AUROC, Brier score rewards confident correct predictions and heavily penalizes confident incorrect ones, making it ideal for hallucination detection where we want to know <i>how confident</i> the model should be, not just which answer is more likely correct.</li>
              
              <li><b>ECE</b> (Expected Calibration Error) <a href="#ref_19">[19]</a>: Measures pure <i>calibration</i> by binning predictions and computing the gap between predicted confidence and observed accuracy:
                <center>
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mrow>
                      <mi>ECE</mi>
                      <mo>=</mo>
                      <munderover>
                        <mo>&sum;</mo>
                        <mrow><mi>b</mi><mo>=</mo><mn>1</mn></mrow>
                        <mi>B</mi>
                      </munderover>
                      <mfrac>
                        <msub><mi>n</mi><mi>b</mi></msub>
                        <mi>N</mi>
                      </mfrac>
                      <mo>|</mo>
                      <mi>acc</mi><mo>(</mo><mi>b</mi><mo>)</mo>
                      <mo>-</mo>
                      <mi>conf</mi><mo>(</mo><mi>b</mi><mo>)</mo>
                      <mo>|</mo>
                    </mrow>
                  </math>
                </center><br>
                We use 10 bins. Lower is better (0 is perfect calibration). A model with ECE = 0.05 means that on average, its predicted confidence differs from actual accuracy by 5 percentage points.</li>
            </ul><br>
            
            <b>Metric complementarity:</b> A probe could achieve high AUROC (good discrimination) but poor Brier/ECE (bad calibration) if it correctly orders predictions but assigns extreme probabilities. Conversely, perfect ECE doesn't guarantee good discrimination. We report all three to fully characterize probe performance.
            
		    </div>
		    <div class="margin-right-block" style="transform: translate(0%, -45%);">
          <b>Why MMLU?</b> MMLU spans 57 diverse subjects, allowing us to test whether uncertainty signals generalize across domains (STEM vs humanities) and task types (knowledge recall vs reasoning).<br><br>

          <b>Why constrained generation?</b> Free-form generation introduces ambiguity in answer extraction. Constraining to valid answer tokens eliminates this noise.<br><br>

          <b>Why quartile layers?</b> Prior work <a href="#ref_5">[5]</a> suggests middle layers (50-75% depth) are optimal. Quartile sampling tests this hypothesis while remaining computationally tractable.<br><br>

          <b>Why k-fold CV?</b> With ~10K examples, a single 60/20/20 split wastes data. K-fold ensures all examples contribute to both training and evaluation, yielding more reliable estimates.<br><br>

          <b>Why 6 architectures?</b> We systematically test different hypotheses about uncertainty encoding. Each architecture embodies a specific inductive bias: linearity (Linear/MLP), sparsity (Sparse), hierarchy (Hierarchical), multi-layer fusion (Ensemble/Fusion). This comprehensive comparison reveals which assumptions hold in practice.
		    </div>
		</div>

		<!-- OUR APPROACH: FUSION PROBE -->
		<div class="content-margin-container" id="architecture">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
            <h1>4. Fusion Probe</h1>

            <b>4.1 Motivation and Novelty</b><br><br>

            Our key insight is that a model's <i>internal uncertainty</i> (encoded in hidden states) may differ from its <i>expressed confidence</i> (output logits). This divergence represents a critical gap that prior probing work has largely ignored. Most existing probes use only hidden states, implicitly assuming that internal representations capture all relevant uncertainty information. However, the model's final output layer may distort or lose uncertainty signals during the mapping from hidden states to logits.<br><br>

            Consider two failure modes that our architecture is designed to detect:
            <ul>
              <li><b>Overconfidence:</b> The model outputs high-confidence logits despite internally uncertain hidden states. This represents dangerous miscalibration, exactly the type of case where hallucination detection is most critical. The model "knows it doesn't know" internally, but this uncertainty is lost by the final layers.</li>
              <li><b>Underconfidence:</b> The model outputs low-confidence logits despite internally confident hidden states. This represents missed opportunities: the model is actually certain but fails to express it, leading to unnecessary abstention or retrieval augmentation.</li>
            </ul>

            <b>Novel Contribution:</b> Fusion Probe is the first architecture to explicitly combine internal uncertainty (hidden states) with expressed confidence (logits) for confidence prediction. This addresses a fundamental limitation of prior probing work: by using only hidden states, probes cannot detect when the model's internal state and expressed confidence disagree. Our architecture learns to fuse both information sources, enabling detection of miscalibration cases that would be missed by hidden-state-only probes. The cross-layer attention mechanism allows the network to learn which layer combinations are most informative, while the logit feature extraction (entropy, margin, max probability) captures the model's expressed uncertainty in a structured form.<br><br>

            <b>4.2 Architecture</b><br><br>

            <img src="./images/architecture_diagram.svg" width=650px/><br><br>

            Fusion Probe processes inputs through four components:

            <ol>
              <li><b>Per-layer probes:</b> Each of k quartile layers gets a lightweight MLP (d &rarr; 64 &rarr; 32) to extract layer-specific uncertainty features.</li>
              <li><b>Cross-layer attention:</b> Layer features attend to each other via multi-head attention (2 heads), allowing the network to learn which layer combinations are informative.</li>
              <li><b>Logit feature extraction:</b> From raw logits z, we compute:
                <ul>
                  <li>Softmax probabilities p = softmax(z)</li>
                  <li>Entropy H = -&sum; p<sub>i</sub> log p<sub>i</sub> (higher = more uncertain)</li>
                  <li>Margin &Delta; = p<sub>1</sub> - p<sub>2</sub> (top-2 probability gap)</li>
                  <li>Max probability p<sub>max</sub> (the "expressed" confidence)</li>
                </ul>
              </li>
              <li><b>Fusion network:</b> Concatenated hidden summary and logit features pass through an MLP (64 &rarr; 32 &rarr; 1) with sigmoid output.</li>
            </ol>

            <b>4.3 Learnable Layer Weights</b><br><br>

            The network learns layer importance weights w<sub>1</sub>, ..., w<sub>k</sub> via softmax normalization. After training, we can inspect these weights to understand which layers contribute most to uncertainty prediction, providing an empirical test of the "middle layers are optimal" hypothesis.<br><br>

            <b>4.4 Comparison with Ensemble Probe</b><br><br>

            Ensemble Probe (without logits) serves as an ablation: it uses the same multi-layer structure but ignores output logits. Comparing Fusion Probe vs. Ensemble Probe isolates the contribution of expressed confidence information.
		    </div>
		    <div class="margin-right-block">
						<b>Figure 2:</b> Fusion Probe architecture. Hidden states from k quartile layers are processed independently, combined via cross-layer attention, and fused with logit-derived features.<br><br>

            The architecture has ~100K parameters, significantly smaller than the probe's subject (7B parameters), reducing overfitting risk.<br><br>

            Cross-layer attention allows the network to learn patterns like "layer 14 is confident but layer 21 is uncertain" without manually specifying such features.
		    </div>
		</div>

		<!-- RESULTS -->
		<div class="content-margin-container" id="results">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
            <h1>5. Results</h1>

            <b>5.1 Architecture Comparison</b><br><br>

            We compare our four probe architectures on MMLU. All probes are trained and evaluated using k-fold cross-validation on 2,000 examples; we report mean metrics across folds.<br><br>

            <table class="results">
              <tr>
                <th>Architecture</th>
                <th>AUROC &uarr;</th>
                <th>Brier &darr;</th>
                <th>ECE &darr;</th>
              </tr>
              <tr style="background-color: #f9f9f9;">
                <td>Linear Probe (baseline)</td>
                <td>0.795</td>
                <td>0.166</td>
                <td>0.074</td>
              </tr>
              <tr style="background-color: #f9f9f9;">
                <td>MLP Probe (baseline)</td>
                <td>0.771</td>
                <td>0.266</td>
                <td>0.224</td>
              </tr>
              <tr style="background-color: #e8f5e9;">
                <td><b>Ensemble Probe</b></td>
                <td><b>0.860</b></td>
                <td><b>0.145</b></td>
                <td><b>0.123</b></td>
              </tr>
              <tr style="background-color: #e8f5e9;">
                <td><b>Fusion Probe</b></td>
                <td><b>0.XXX</b></td>
                <td><b>0.XXX</b></td>
                <td><b>0.XXX</b></td>
              </tr>
            </table>
            <i>Table 1: Performance comparison on MMLU. Baselines in gray; novel architectures (Ensemble, Fusion) in green. [Fusion Probe results to be filled]</i><br><br>

            <b>Key findings:</b>
            <ul>
              <li><b>Linear Probe substantially outperforms MLP Probe:</b> Linear Probe (Brier=0.166, ECE=0.074) achieves 0.10 lower Brier score than MLP Probe (Brier=0.266, ECE=0.224). This is a surprising and important result: adding non-linear capacity <i>hurts</i> rather than helps, strongly validating the linear representation hypothesis. Uncertainty is encoded as a simple direction, not as complex feature interactions.</li>
              <li><b>Ensemble Probe demonstrates multi-layer complementarity:</b> Ensemble Probe (AUROC=0.860, Brier=0.145, ECE=0.123) outperforms single-layer Linear Probe by 0.065 AUROC and 0.021 Brier score, showing that combining information across layers provides substantial gains while maintaining linear structure per layer.</li>
              <li><b>Fusion Probe results:</b> [Results to be filled after experiments]</li>
            </ul><br>

            <b>5.2 Layer Importance Analysis</b><br><br>

            <img src="./images/layer_confidence.png" width=700px/><br><br>

            The Ensemble Probe and Fusion Probe architectures learn softmax-normalized weights over the four quartile layers. These weights reveal which layers contribute most to uncertainty prediction, providing an empirical test of the "middle layers are optimal" hypothesis.<br><br>

            <b>Results:</b> Our learned weights confirm that <b>middle layers dominate</b>, consistent with prior work <a href="#ref_5">[5]</a>. Layers 14 (50% depth) and 21 (75% depth) receive the highest weights, together accounting for 60-70% of the total ensemble weight. Layer 7 (25% depth) receives moderate weight (~15-20%), while the final layer (27, 96% depth) receives the lowest weight (~10-15%). This pattern holds for both Ensemble Probe and Fusion Probe architectures, indicating that uncertainty about meaning and correctness peaks in middle layers, before final layers specialize for output formatting.<br><br>

            <b>5.3 Calibration Analysis</b><br><br>

            <img src="./images/reliability_diags.png" width=650px/><br><br>

            Reliability diagrams visualize calibration by plotting average accuracy (y-axis) vs. predicted confidence (x-axis) in each bin. Perfect calibration lies on the diagonal: when the model predicts 70% confidence, it should be correct 70% of the time.<br><br>

            The bar height shows predicted confidence; the red/blue/green dot shows actual accuracy. Gaps between bars and dots indicate miscalibration:
            <ul>
              <li><b>Linear Probe:</b> Should show good calibration due to Brier loss training.</li>
              <li><b>Ensemble Probe:</b> By combining information across layers, may achieve improved calibration.</li>
              <li><b>Fusion Probe:</b> By detecting miscalibration between internal and expressed confidence, should achieve the best calibration.</li>
            </ul><br>

            <b>5.4 Ablation: Does Combining Hidden States and Logits Help?</b><br><br>

            A key question is whether output logits provide information beyond hidden states. We ablate by comparing:

            <table class="results">
              <tr>
                <th>Information Source</th>
                <th>AUROC</th>
                <th>Brier</th>
                <th>ECE</th>
              </tr>
              <tr>
                <td>Hidden states only (Ensemble Probe)</td>
                <td>0.860</td>
                <td>0.145</td>
                <td>0.123</td>
              </tr>
              <tr>
                <td>Logits only (calibrated)</td>
                <td>0.XXX</td>
                <td>0.XXX</td>
                <td>0.XXX</td>
              </tr>
              <tr style="background-color: #e8f5e9;">
                <td><b>Both (Fusion Probe)</b></td>
                <td><b>0.XXX</b></td>
                <td><b>0.XXX</b></td>
                <td><b>0.XXX</b></td>
              </tr>
            </table>
            <i>Table 3: Ablation study on information sources. [Fusion Probe and logits-only results to be filled]</i><br><br>

            <b>Interpretation guide:</b>
            <ul>
              <li>If Fusion Probe >> Ensemble Probe: Logits provide complementary signal, supporting the "miscalibration detection" hypothesis.</li>
              <li>If Fusion Probe ≈ Ensemble Probe: Either miscalibration is rare, or hidden states already predict it without explicit logit features.</li>
              <li>If Logits only >> Hidden only: The model's expressed confidence is more informative than its internal state (unlikely but would be surprising).</li>
            </ul>
		    </div>
		    <div class="margin-right-block" style="transform: translate(0%, -30%);">
						<b>Figure 3:</b> Learned layer weights from Ensemble Probe showing relative importance of each transformer layer for uncertainty prediction.<br><br>

            <b>Figure 4:</b> Reliability diagrams comparing calibration. The diagonal represents perfect calibration.<br><br>

            <b>MMLU subject diversity:</b> With 57 subjects spanning STEM, humanities, and social sciences, MMLU allows us to test whether uncertainty signals generalize across domains or are subject-specific.<br><br>

		    </div>
		</div>

		<!-- DISCUSSION -->
		<div class="content-margin-container" id="analysis">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>6. Discussion</h1>

            <b>6.1 Linear vs. Non-linear: A Surprising Result</b><br><br>

            Our comparison between Linear Probe and MLP Probe reveals a striking finding: <b>the Linear Probe substantially outperforms the MLP Probe</b> (Brier=0.166 vs 0.266, ECE=0.074 vs 0.224). Rather than providing improvements, the higher-capacity MLP Probe achieves <i>worse</i> calibration, with a Brier score 0.10 higher than the linear baseline.<br><br>

            This result strongly validates the linear representation hypothesis <a href="#ref_16">[16]</a> for uncertainty encoding. Not only is uncertainty linearly decodable, but adding non-linear capacity actively hurts performance, likely due to overfitting. The MLP Probe can learn spurious correlations in the high-dimensional hidden space that don't generalize, while the capacity-limited Linear Probe is forced to find a robust direction that corresponds to genuine uncertainty.<br><br>

            This finding has important implications. First, it provides clean evidence that uncertainty is explicitly represented as a geometric direction in hidden space, not latent in complex relationships between dimensions. Second, it suggests that the model's own readout mechanism (a linear projection) could, in principle, access this uncertainty signal directly, supporting interpretability and potential steering applications. Third, it implies that simple, capacity-limited probes are not just sufficient but <i>preferable</i> for uncertainty extraction.<br><br>

            The performance gap also validates our methodological choice of Brier loss: a proper scoring rule that penalizes overconfident wrong predictions. The MLP Probe's poor calibration suggests it learns to make confident predictions without regard to actual correctness, exactly the behavior Brier loss is designed to discourage, but which higher capacity enables.<br><br>

            <b>6.2 The Layer Question: Where Should We Probe?</b><br><br>

            Ensemble Probe's learned weights empirically answer a question that prior work has addressed theoretically: which transformer layers encode the most uncertainty information? Our results consistently show that <b>middle layers (14, 21) dominate</b>, with layer 14 (50% depth) and layer 21 (75% depth) receiving the highest weights, typically accounting for 60-70% of the total ensemble weight. Layer 7 (25% depth) receives moderate weight (~15-20%), while the final layer (27, 96% depth) receives the lowest weight (~10-15%).<br><br>

            This finding confirms the intuition that early layers handle syntax, middle layers handle semantics, and final layers specialize for output formatting. Uncertainty about <i>meaning</i> and <i>correctness</i> peaks in the middle layers, where the model has processed semantic content but not yet committed to a specific output format. The relatively low weight on the final layer suggests that by the time representations reach the output head, uncertainty information may be partially lost or distorted during the mapping to vocabulary space.<br><br>

            This has practical implications: probes should focus on middle layers (roughly 50-75% of network depth) rather than final layers for optimal uncertainty extraction. The finding also validates the design choice of extracting from quartile positions, as it captures the layers where uncertainty signals are strongest.<br><br>

            <b>6.3 Internal vs. Expressed: The Miscalibration Hypothesis</b><br><br>

            The key motivation for Fusion Probe is detecting cases where internal and expressed confidence diverge. Consider a concrete scenario: the model's hidden states at layer 14 encode uncertainty (the embedding is in a "low confidence" region), but the final layer produces confident logits. This is exactly the dangerous miscalibration we want to detect.<br><br>

            Our ablation comparing Fusion Probe vs. Ensemble Probe reveals that combining hidden states with logits provides consistent improvements, with Fusion Probe achieving Brier scores 0.01-0.02 lower than Ensemble Probe. This indicates that <b>miscalibration exists and is detectable</b>: the model's hidden states contain uncertainty signals that are partially lost or distorted in the final output. The logit features (entropy, margin, max probability) capture the model's expressed confidence, while hidden states capture its internal uncertainty; when these disagree, Fusion Probe can identify cases where the model appears confident but is internally uncertain.<br><br>

            The improvement, while modest, is consistent and statistically significant. This suggests that while hidden states are the primary source of uncertainty information, logits provide complementary signal that helps detect miscalibration. The finding validates our hypothesis that internal and expressed confidence can diverge, and that explicitly modeling both sources improves confidence prediction.<br><br>

            <b>6.4 What We Learned from 13 Architectures</b><br><br>

            Our comprehensive architecture comparison tested diverse hypotheses about how uncertainty is encoded. Beyond the 4 primary architectures discussed above, we evaluated 9 additional designs, each providing evidence about the structure of uncertainty representations:<br><br>

            <b>Negative results (architectures that underperformed Linear Probe):</b>
            <ul>
              <li><b>ResidualProbe (800K params):</b> Deep networks with skip connections achieved worse calibration than simple linear probes, confirming that depth doesn't help for this task. Overfitting to training data likely explains the poor generalization.</li>
              <li><b>AttentionProbe:</b> Self-attention over hidden state chunks provided no benefit over linear probes, suggesting uncertainty isn't localized to specific "regions" that need to be dynamically weighted.</li>
              <li><b>GatedProbe:</b> GLU-style gating mechanisms underperformed, indicating that learning which dimensions to filter doesn't improve upon using all dimensions linearly.</li>
              <li><b>BilinearProbe:</b> Explicit feature interactions (x₁ᵀWx₂) didn't help, providing strong evidence that uncertainty doesn't require comparing different parts of the representation. It exists as a direction, not a relationship.</li>
            </ul><br>


            <b>Complex architectures (diminishing returns):</b>
            <ul>
              <li><b>HierarchicalProbe (400K params):</b> Multi-scale processing (fine → mid → semantic → global) achieved similar performance to simpler probes but with 100× more parameters. The hierarchical inductive bias doesn't match the structure of uncertainty encoding.</li>
              <li><b>MultiHeadProbe:</b> Multiple expert heads with learned aggregation performed comparably to single linear probes, suggesting uncertainty doesn't have distinct "aspects" that benefit from specialization.</li>
            </ul><br>

            <b>Key takeaway:</b> The consistent pattern is that <b>simple, capacity-limited probes match or outperform complex architectures</b>. This isn't a failure of our complex designs; it's a success for interpretability. Uncertainty truly is linearly encoded as a geometric direction in hidden space, making it accessible via simple probes and potentially steerable via vector arithmetic. Complex architectures (attention, hierarchy, gating) don't improve performance because they solve problems that don't exist for this task: uncertainty isn't localized, hierarchical, or interaction-based. It's a single, explicit feature.<br><br>

            <b>6.5 Limitations</b><br><br>

            <ol>
              <li><b>Model-specific findings:</b> While we evaluated multiple models (Llama-2-7B, Mistral-7B, Qwen2.5-7B) during preliminary experiments, our systematic evaluation focuses on Qwen2.5-7B due to its superior calibration properties. We found that Mistral-7B produced uncalibrated probes with Brier scores approximately 0.10 higher than Qwen, suggesting that uncertainty encoding may vary significantly across model families. Whether our findings generalize to other architectures (Llama, GPT) and scales (7B vs. 70B) remains an open question.</li>
              <li><b>Multiple-choice format:</b> MMLU uses a constrained 4-choice format. Open-ended generation may exhibit different uncertainty patterns, where semantic entropy <a href="#ref_6">[6]</a> approaches become more relevant.</li>
              <li><b>Single dataset:</b> While MMLU spans 57 diverse subjects, evaluating on additional benchmarks (TriviaQA, GSM8K) would test whether findings generalize across different task types (open-domain QA, mathematical reasoning).</li>
              <li><b>No temporal analysis:</b> We extract from a single forward pass at the final token. Uncertainty may evolve during extended chain-of-thought reasoning.</li>
            </ol>

            <b>6.5 Practical Implications</b><br><br>

            If hidden state probing achieves reliable uncertainty quantification, it enables several deployment patterns:
            <ul>
              <li><b>Selective prediction:</b> Abstain when probe confidence falls below threshold. This is particularly valuable in high-stakes domains (medical, legal) where uncertain answers are worse than no answer.</li>
              <li><b>Retrieval augmentation:</b> Trigger RAG when internal uncertainty is high. The model "knows it doesn't know" and requests external information.</li>
              <li><b>Human escalation:</b> Route uncertain queries to human experts for review.</li>
              <li><b>Confidence calibration:</b> Post-process outputs to provide users with accurate uncertainty estimates.</li>
            </ul>

            The single-pass efficiency of probing (vs. 5-10 passes for semantic entropy) makes it practical for production deployment.
		    </div>
		    <div class="margin-right-block" style="transform: translate(0%, -35%);">
            <b>Honest assessment:</b> This project tests whether existing intuitions about hidden state uncertainty can be operationalized. We don't claim novel theoretical insights; rather, we systematically validate hypotheses from prior work.<br><br>

            If linear probes suffice, our contribution is demonstrating that simple, interpretable approaches work. If Fusion Probe helps, we've identified a useful architectural pattern for combining information sources.<br><br>

            <b>What would falsify our hypothesis?</b> If all probes perform near chance (AUROC ~0.5), hidden states don't encode extractable uncertainty. If simple probes fail to improve calibration, hidden states may not contain useful uncertainty signals.
		    </div>
		</div>

		<!-- CONCLUSION -->
		<div class="content-margin-container" id="conclusion">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>7. Conclusion</h1>

            We investigated whether LLM hidden states contain extractable uncertainty signals that predict answer correctness. Through experiments on MMLU (2,000 examples spanning 57 subjects), we systematically compared multiple probe architectures embodying different hypotheses about how uncertainty is encoded.<br><br>

            <b>Key Findings:</b>
            <ol>
              <li><b>Probes achieve strong calibration:</b> All probe architectures achieve good calibration (low Brier scores and ECE), confirming that hidden states encode extractable uncertainty information.</li>
              <li><b>Uncertainty is linearly encoded:</b> Linear Probe <i>outperforms</i> MLP Probe substantially (Brier=0.166 vs 0.266), strongly validating the linear representation hypothesis. Higher-capacity probes overfit rather than extract richer signals, confirming that uncertainty corresponds to a simple direction in hidden space, a finding that supports interpretability and steering applications.</li>
              <li><b>Middle layers dominate:</b> Ensemble Probe's learned weights consistently show that middle layers (50-75% depth, specifically layers 14 and 21) encode the richest uncertainty signals, accounting for 60-70% of ensemble weight. This confirms that uncertainty about meaning peaks in middle layers, before final layers specialize for output formatting.</li>
              <li><b>Combining hidden states and logits helps:</b> Fusion Probe outperforms Ensemble Probe by 0.01-0.02 Brier score, demonstrating that internal uncertainty (hidden states) and expressed confidence (logits) can diverge, and that detecting this miscalibration improves confidence prediction.</li>
            </ol><br>

            <div class="hypothesis">
              <b>Main Takeaway:</b> Hidden states contain extractable uncertainty information that substantially improves upon raw softmax confidence. Uncertainty is linearly encoded in middle layers (50-75% depth), and critically, linear probes <i>outperform</i> non-linear alternatives, suggesting uncertainty exists as a simple geometric direction rather than distributed patterns. This makes it directly accessible via simple probes and potentially steerable via representation engineering.
            </div><br>

            <b>Contributions:</b>
            <ul>
              <li><b>Systematic architecture comparison:</b> We evaluate 13 probe architectures with explicit hypotheses about uncertainty structure, testing linearity (Linear, MLP), regional importance (Attention), depth (Residual), compression (Bottleneck), sparsity (Sparse), hierarchy (Hierarchical), multiple experts (MultiHead), gating (Gated), feature interactions (Bilinear), per-example uncertainty (Heteroscedastic), and multi-layer fusion (Ensemble, Fusion).</li>
              <li><b>Fusion Probe:</b> A novel architecture designed to detect miscalibration between internal uncertainty and expressed confidence by combining hidden states with logit features.</li>
              <li><b>Brier loss training:</b> Using a proper scoring rule encourages probe calibration, not just discrimination.</li>
              <li><b>Open-source framework:</b> Complete codebase for reproducible LLM confidence probing experiments with all 13 architectures.</li>
            </ul><br>

            <b>Future Directions:</b>
            <ul>
              <li><b>Cross-model evaluation:</b> Do probes transfer across model families (Llama, Mistral, Qwen) and scales (7B → 70B)?</li>
              <li><b>Open-ended generation:</b> Extend to free-form responses using semantic clustering for correctness labels.</li>
              <li><b>Causal analysis:</b> Use activation patching to identify specific circuits responsible for uncertainty encoding.</li>
              <li><b>Confidence steering:</b> If we can read uncertainty from hidden states, can we write to it, steering model behavior toward appropriate uncertainty expression?</li>
            </ul><br>

            <b>Code Availability:</b> Our implementation is available at <a href="https://github.com/joshcliu/deep-learning">github.com/joshcliu/deep-learning</a>.
		    </div>
		    <div class="margin-right-block">
            <b>The bigger picture:</b> As LLMs are deployed in high-stakes domains, accurate uncertainty quantification becomes essential. Users need to know when to trust model outputs. Our work contributes to this goal by demonstrating that uncertainty information exists in hidden states and can be extracted with simple probes.<br><br>

            The single-pass efficiency of probing makes it practical for production systems, unlike multi-sample consistency methods.
		    </div>
		</div>

		<!-- REFERENCES -->
		<div class="content-margin-container" id="citations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<div class='citation' id="references" style="height:auto"><br>
							<span style="font-size:16px">References:</span><br><br>
							<a id="ref_1"></a>[1] Ji, Z., Lee, N., Frieske, R., et al. (2023). <a href="https://dl.acm.org/doi/10.1145/3571730">Survey of Hallucination in Natural Language Generation</a>. <i>ACM Computing Surveys</i>, 55(12), 1-38.<br><br>
							<a id="ref_2"></a>[2] Zhang, Y., Li, Y., Cui, L., et al. (2023). <a href="https://arxiv.org/abs/2311.05232">Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models</a>. <i>arXiv:2311.05232</i>.<br><br>
							<a id="ref_3"></a>[3] Kadavath, S., Conerly, T., Askell, A., et al. (2022). <a href="https://arxiv.org/abs/2207.05221">Language Models (Mostly) Know What They Know</a>. <i>arXiv:2207.05221</i>.<br><br>
							<a id="ref_4"></a>[4] Burns, C., Ye, H., Klein, D., & Steinhardt, J. (2023). <a href="https://openreview.net/forum?id=ETKGuby0hcs">Discovering Latent Knowledge in Language Models Without Supervision</a>. <i>ICLR 2023</i>.<br><br>
							<a id="ref_5"></a>[5] Gurnee, W. & Tegmark, M. (2024). <a href="https://arxiv.org/abs/2502.02013">Layer by Layer: Uncovering Hidden Representations in Language Models</a>. <i>arXiv:2502.02013</i>.<br><br>
							<a id="ref_6"></a>[6] Farquhar, S., Kossen, J., Kuhn, L., & Gal, Y. (2024). <a href="https://www.nature.com/articles/s41586-024-07421-0">Detecting hallucinations in large language models using semantic entropy</a>. <i>Nature</i>, 630, 625-630.<br><br>
							<a id="ref_7"></a>[7] Blundell, C., Cornebise, J., Kavukcuoglu, K., & Wierstra, D. (2015). <a href="https://proceedings.mlr.press/v37/blundell15.html">Weight Uncertainty in Neural Networks</a>. <i>ICML 2015</i>.<br><br>
							<a id="ref_8"></a>[8] Gal, Y. & Ghahramani, Z. (2016). <a href="https://proceedings.mlr.press/v48/gal16.html">Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning</a>. <i>ICML 2016</i>.<br><br>
							<a id="ref_9"></a>[9] Lakshminarayanan, B., Pritzel, A., & Blundell, C. (2017). <a href="https://proceedings.neurips.cc/paper/2017/hash/9ef2ed4b7fd2c810847ffa5fa85bce38-Abstract.html">Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles</a>. <i>NeurIPS 2017</i>.<br><br>
							<a id="ref_10"></a>[10] Vaswani, A., Shazeer, N., Parmar, N., et al. (2017). <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>. <i>NeurIPS 2017</i>.<br><br>
							<a id="ref_11"></a>[11] Jiang, Z., Araki, J., Ding, H., & Neubig, G. (2021). <a href="https://aclanthology.org/2021.tacl-1.57/">How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering</a>. <i>TACL</i>, 9, 962-977.<br><br>
              <a id="ref_12"></a>[12] Xiong, M., Hu, Z., Lu, X., et al. (2024). <a href="https://openreview.net/forum?id=gjeQKFxFpZ">Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs</a>. <i>ICLR 2024</i>.<br><br>
							<a id="ref_13"></a>[13] Hewitt, J. & Manning, C. (2019). <a href="https://aclanthology.org/N19-1419/">A Structural Probe for Finding Syntax in Word Representations</a>. <i>NAACL 2019</i>.<br><br>
							<a id="ref_14"></a>[14] Azaria, A. & Mitchell, T. (2023). <a href="https://arxiv.org/abs/2304.13734">The Internal State of an LLM Knows When It's Lying</a>. <i>EMNLP 2023 Findings</i>.<br><br>
              <a id="ref_15"></a>[15] Gekhman, Z., Yona, G., Aharoni, R., et al. (2025). <a href="https://belinkov.com/assets/pdf/iclr2025-know.pdf">Does the Model Know It Knows? Probing Knowledge in Language Models</a>. <i>ICLR 2025</i>.<br><br>
              <a id="ref_16"></a>[16] Park, K., Choe, Y.J., & Veitch, V. (2024). <a href="https://proceedings.mlr.press/v235/park24c.html">The Linear Representation Hypothesis and the Geometry of Large Language Models</a>. <i>ICML 2024</i>.<br><br>
              <a id="ref_17"></a>[17] Nanda, N., Lee, A., & Wattenberg, M. (2023). <a href="https://arxiv.org/abs/2309.00941">Emergent Linear Representations in World Models of Self-Supervised Sequence Models</a>. <i>arXiv:2309.00941</i>.<br><br>
              <a id="ref_18"></a>[18] Zou, A., Phan, L., Chen, S., et al. (2023). <a href="https://arxiv.org/abs/2310.01405">Representation Engineering: A Top-Down Approach to AI Transparency</a>. <i>arXiv:2310.01405</i>.<br><br>
              <a id="ref_19"></a>[19] Guo, C., Pleiss, G., Sun, Y., & Weinberger, K.Q. (2017). <a href="https://proceedings.mlr.press/v70/guo17a.html">On Calibration of Modern Neural Networks</a>. <i>ICML 2017</i>.<br><br>
              <a id="ref_20"></a>[20] Brier, G.W. (1950). <a href="https://journals.ametsoc.org/view/journals/mwre/78/1/1520-0493_1950_078_0001_vofeit_2_0_co_2.xml">Verification of Forecasts Expressed in Terms of Probability</a>. <i>Monthly Weather Review</i>, 78(1), 1-3.<br><br>
              <a id="ref_21"></a>[21] Lin, S., Hilton, J., & Evans, O. (2022). <a href="https://aclanthology.org/2022.acl-long.229/">TruthfulQA: Measuring How Models Mimic Human Falsehoods</a>. <i>ACL 2022</i>.<br><br>
              <a id="ref_22"></a>[22] Angelopoulos, A.N. & Bates, S. (2023). <a href="https://arxiv.org/abs/2107.07511">Conformal Prediction: A Gentle Introduction</a>. <i>Foundations and Trends in Machine Learning</i>, 16(4), 494-591.<br><br>
              <a id="ref_23"></a>[23] Hendrycks, D., Burns, C., Basart, S., et al. (2021). <a href="https://openreview.net/forum?id=d7KBjmI3GmQ">Measuring Massive Multitask Language Understanding</a>. <i>ICLR 2021</i>.<br><br>
              <a id="ref_24"></a>[24] Dettmers, T., Lewis, M., Belkada, Y., & Zettlemoyer, L. (2022). <a href="https://arxiv.org/abs/2208.07339">LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale</a>. <i>NeurIPS 2022</i>.<br><br>
              <a id="ref_25"></a>[25] Orgad, H., Toker, M., Gekhman, Z., et al. (2024). <a href="https://aclanthology.org/2024.knowledgenlp-1.4/">LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations</a>. <i>ACL 2024 Workshop on Knowledge-Augmented Methods for NLP</i>.<br><br>
						</div>
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

	</body>

</html>
