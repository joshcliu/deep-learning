<html>
<head>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

<!-- MathJax for consistent math rendering across browsers -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/mml-chtml.js"></script>

<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
	body {
		background-color: #f5f9ff;
	}

	.content-margin-container {
		display: flex;
		width: 100%;
		justify-content: left;
		align-items: center;
	}
	.main-content-block {
		width: 70%;
    max-width: 1100px;
		background-color: #fff;
		border-left: 1px solid #DDD;
		border-right: 1px solid #DDD;
		padding: 8px 8px 8px 8px;
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
	}
	.margin-left-block {
			font-size: 14px;
			width: 15%;
			max-width: 130px;
			position: relative;
			margin-left: 10px;
			text-align: left;
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
			padding: 5px;
	}
	.margin-right-block {
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
			font-size: 14px;
			width: 25%;
			max-width: 256px;
			position: relative;
			text-align: left;
			padding: 10px;
	}

	img {
			max-width: 100%;
			height: auto;
			display: block;
			margin: auto;
	}

	a:link,a:visited
	{
		color: #0e7862;
		text-decoration: none;
	}
	a:hover {
		color: #24b597;
	}

	h1 {
		font-size: 18px;
		margin-top: 4px;
		margin-bottom: 10px;
	}

	table.header {
    font-weight: 300;
    font-size: 17px;
    flex-grow: 1;
		width: 70%;
    max-width: calc(100% - 290px);
	}
	table td, table td * {
	    vertical-align: middle;
	    position: relative;
	}

	hr {
    height: 1px;
    border: none;
    background-color: #DDD;
  }

	div.hypothesis {
		width: 80%;
		background-color: #EEE;
		border: 1px solid black;
		border-radius: 10px;
		font-family: Courier;
		font-size: 16px;
		text-align: center;
		margin: auto;
		padding: 16px 16px 16px 16px;
	}

	div.citation {
    font-size: 0.8em;
    background-color:#fff;
    padding: 10px;
		height: 200px;
  }

	table.results {
		border-collapse: collapse;
		width: 100%;
		margin: 10px 0;
	}
	table.results th, table.results td {
		border: 1px solid #DDD;
		padding: 8px;
		text-align: left;
	}
	table.results th {
		background-color: #f5f9ff;
	}

	code {
		background-color: #f5f5f5;
		padding: 2px 6px;
		border-radius: 3px;
		font-family: 'Courier New', Courier, monospace;
		font-size: 14px;
	}

	pre {
		background-color: #f5f5f5;
		padding: 12px;
		border-radius: 5px;
		overflow-x: auto;
		font-family: 'Courier New', Courier, monospace;
		font-size: 13px;
	}

</style>

	  <title>Probing through LLMs: Extracting Confidence from Hidden States</title>
      <meta property="og:title" content="Probing through LLMs: Extracting Confidence from Hidden States" />
			<meta charset="UTF-8">
  </head>

  <body>

		<!-- HEADER -->
		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<table class="header" align=left>
								<tr>
									<td colspan=4>
										<span style="font-size: 32px; font-family: 'Courier New', Courier, monospace;">Probing through LLMs</span>
									</td>
								</tr>
								<tr>
									<td colspan=4>
										<span style="font-size: 20px; color: #666;">Extracting Confidence from Hidden States</span>
									</td>
								</tr>
								<tr>
										<td align=left>
												<span style="font-size:17px"><a href="#">Joshua Liu</a></span>
										</td>
										<td align=left>
												<span style="font-size:17px"><a href="#">Carol Li</a></span>
										</td>
										<td align=left>
												<span style="font-size:17px"><a href="#">Maureen Zhang</a></span>
										</td>
								<tr>
									<td colspan=4 align=left><span style="font-size:18px">Final project for 6.7960, MIT</span></td>
								</tr>
						</table>
					</div>
					<div class="margin-right-block">
					</div>
		</div>

    <!-- INTRODUCTION -->
    <div class="content-margin-container" id="intro">
				<div class="margin-left-block">
          <div style="position:fixed; max-width:inherit; top:max(20%,120px)">
              <b style="font-size:16px">Outline</b><br><br>
              <a href="#intro">Introduction</a><br><br>
              <a href="#background">Background</a><br><br>
              <a href="#methodology">Methodology</a><br><br>
              <a href="#architecture">Our Approach</a><br><br>
              <a href="#results">Results</a><br><br>
              <a href="#analysis">Discussion</a><br><br>
              <a href="#conclusion">Conclusion</a><br><br>
          </div>
				</div>
		    <div class="main-content-block">
						<h1>1. Introduction</h1>

            Large language models (LLMs) frequently produce confident incorrect answers, particularly in high-stakes domains like medicine, law, and finance <a href="#ref_1">[1]</a><a href="#ref_2">[2]</a>. The core challenge is not just that models make mistakes, but that they provide little reliable signal about when they are likely to be wrong. Reliable uncertainty estimates would enable selective prediction, retrieval augmentation, and human escalation—making LLMs safer for real-world deployment. Can models "know what they don't know," and if so, can we extract that knowledge from their internal representations?<br><br>

            <div class="hypothesis">
              <b>Central Hypothesis:</b> A model's internal representation of uncertainty can diverge from the confidence it expresses in its outputs. By probing hidden layers with both linear and nonlinear architectures, we can accurately measure the model's confidence and uncertainty.
            </div><br>

            Evidence suggests such internal signals exist: Kadavath et al. <a href="#ref_3">[3]</a> showed LLMs can predict their own correctness well above chance, Burns et al. <a href="#ref_4">[4]</a> found that hidden states encode truthfulness even without supervised labels, and Gurnee & Tegmark <a href="#ref_5">[5]</a> demonstrated that middle transformer layers encode richer semantic information than final layers. We use <b>probing</b>—training lightweight classifiers on frozen hidden states, as pioneered by Hewitt & Manning <a href="#ref_13">[13]</a> and applied to LLM error detection by Azaria & Mitchell <a href="#ref_14">[14]</a>—to systematically investigate: (1) Is uncertainty linearly encoded, or does it require complex non-linear extraction? (2) Which layers encode uncertainty most strongly? (3) Does combining hidden states with output logits improve calibration?
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>


		<!-- BACKGROUND AND RELATED WORK -->
		<div class="content-margin-container" id="background">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
					<h1>2. Background and Related Work</h1>

          Existing uncertainty quantification methods for LLMs fall into three categories. <b>Token probability methods</b> use the model's softmax scores as confidence, but Jiang et al. <a href="#ref_11">[11]</a> showed these are often poorly calibrated. <b>Consistency-based methods</b> like semantic entropy <a href="#ref_6">[6]</a> generate multiple responses and measure agreement, achieving strong hallucination detection but requiring 5-10 forward passes per query. <b>Verbalized confidence</b> asks models to self-report uncertainty, but Xiong et al. <a href="#ref_12">[12]</a> found they tend to be systematically overconfident. These limitations motivate looking inside the model's internal representations.<br><br>

        <b>2.3 Probing Internal Representations</b>
          <p>
          Our approach fits into a fourth paradigm: probing internal representations. The basic idea
          is that if a model has already computed something like “confidence” internally, we should
          be able to read it out by training a small classifier on its hidden states. This aims to
          keep the single-forward-pass efficiency of logit-based methods while tapping into
          potentially richer signals than output probabilities or verbalized confidence.
          </p>
          
          <p>
          Probing itself has a long history in NLP. Hewitt and Manning
          <a href="#ref_13">[13]</a> showed that syntactic structure can be linearly recovered from BERT’s
          hidden space, suggesting that if a property is linearly extractable, the model has likely
          computed it explicitly, whereas properties that only appear under complex nonlinear probes
          may be artifacts of the probe. For uncertainty, Kadavath et al. <a href="#ref_3">[3]</a> found that
          large LLMs can meaningfully judge their own answers, and Burns et al.
          <a href="#ref_4">[4]</a> proposed Contrast-Consistent Search (CCS), which recovers a truthfulness signal
          from hidden states without labels, though it relies on contrastive statement pairs and does
          not directly handle open-ended generation.
          </p>
          
          <p>
          More recent work asks <i>where</i> this kind of information lives. Azaria and Mitchell
          <a href="#ref_14">[14]</a> trained classifiers on hidden states and found strong error signals,
          especially in intermediate layers. Gekhman et al. <a href="#ref_15">[15]</a> showed that models can
          encode the right answer internally yet still output something wrong, implying that internal
          representations may be more informative than the generated text itself. This motivates our
          focus on probing hidden states to study how uncertainty is represented inside the model.
          </p>
          
		<b>2.4 Calibration</b>
          <p>
          A well-calibrated model's confidence matches its accuracy (70% confidence → 70% correct). Guo et al. <a href="#ref_19">[19]</a> showed neural networks are often overconfident and introduced Expected Calibration Error (ECE) as a metric. We train probes with Brier score <a href="#ref_20">[20]</a>, a proper scoring rule that penalizes confident wrong predictions, encouraging calibrated probabilities rather than just accurate classifications.
          </p>

		<b>2.5 Research Gaps We Address</b>
          <p>
          Despite substantial progress, key questions remain about how LLMs represent and express uncertainty.
          </p>
          
          <ol>
            <li>
              <b>Linear vs. non-linear encoding.</b><br>
              Most probing work relies on linear classifiers
              <a href="#ref_13">[13]</a><a href="#ref_14">[14]</a>, so it is unclear whether uncertainty is
              linearly extractable or requires more expressive decoders.<br>
              <i>Our contribution:</i> We compare four probe architectures (Linear, MLP, Hierarchical, Sparse) to test competing hypotheses about the geometric structure of uncertainty in hidden states, then evaluate how combining information across layers and with logits improves confidence-correction correlation.
            </li>
            <br>
            <li>
              <b>Multi-layer information and layer localization.</b><br>
              Prior work typically probes single layers <a href="#ref_14">[14]</a> or uses naive
              concatenation, leaving open both how to combine uncertainty signals across layers and
              where uncertainty is primarily encoded (early vs. middle vs. final layers).<br>
              <i>Our contribution:</i> We introduce a multi-layer ensemble approach with learned weighted
              aggregation that automatically discovers which layers encode the most uncertainty information.
              By training softmax-normalized combination weights over quartile layers (25%, 50%, 75%, 96% depth),
              we empirically demonstrate that middle layers (50-75% depth) dominate uncertainty prediction,
              receiving 60-70% of the learned ensemble weight.
            </li>
            <br>
            <li>
              <b>Internal vs. expressed confidence.</b><br>
              Existing studies usually focus on either hidden-state signals
              <a href="#ref_14">[14]</a> or output probabilities <a href="#ref_11">[11]</a> in isolation, so
              the role of their disagreement as a miscalibration signal is underexplored.<br>
              <i>Our contribution:</i> We introduce a Fusion Probe architecture that combines hidden states
              from multiple layers with output logit features (entropy, margin, max probability) using
              cross-layer attention and learned weighted aggregation. This approach detects miscalibration
              by identifying cases where internal uncertainty (hidden states) and expressed confidence (logits)
              diverge.
            </li>
            <br>
            <li>
              <b>Architectural assumptions.</b><br>
              Different probe architectures encode different assumptions about how uncertainty is
              structured (e.g., sparsity, hierarchy), yet there is no systematic comparison of these assumptions.<br>
              <i>Our contribution:</i> We systematically compare four probe architectures (Linear, MLP,
              Hierarchical, Sparse) and find that simple linear probes match or outperform complex designs
              despite having 100× fewer parameters, suggesting uncertainty is linearly accessible.
            </li>
          </ol>

		</div>
		    <div class="margin-right-block" style="transform: translate(0%, -30%);">
          
		    </div>
		</div>

		<!-- METHODOLOGY -->
		<div class="content-margin-container" id="methodology">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
            <h1>3. Methodology</h1>

            <b>3.1 Problem Formulation</b><br><br>

            Given an LLM M, a question q, and the model's generated answer a, we aim to predict whether a is correct. Formally, let h<sup>(l)</sup> &in; &#x211D;<sup>d</sup> denote the hidden state at layer l, and let z &in; &#x211D;<sup>K</sup> denote the output logits for K answer choices. We train a probe f<sub>&theta;</sub> to predict:<br><br>
            <center>
              <math xmlns="http://www.w3.org/1998/Math/MathML">
                <mrow>
                  <mover>
                    <mi>p</mi>
                    <mo>^</mo>
                  </mover>
                  <mo>=</mo>
                  <msub>
                    <mi>f</mi>
                    <mi>&theta;</mi>
                  </msub>
                  <mo>(</mo>
                  <msup><mi>h</mi><mrow><mo>(</mo><msub><mi>l</mi><mn>1</mn></msub><mo>)</mo></mrow></msup>
                  <mo>,</mo>
                  <mo>...</mo>
                  <mo>,</mo>
                  <msup><mi>h</mi><mrow><mo>(</mo><msub><mi>l</mi><mi>k</mi></msub><mo>)</mo></mrow></msup>
                  <mo>,</mo>
                  <mi>z</mi>
                  <mo>)</mo>
                  <mo>&isin;</mo>
                  <mo>[</mo>
                  <mn>0</mn>
                  <mo>,</mo>
                  <mn>1</mn>
                  <mo>]</mo>
                </mrow>
              </math>
            </center><br>
            where p&#770; is the predicted probability that a is correct.<br><br>

            <b>3.2 Datasets</b><br><br>

            We evaluate on the widely-used <b>MMLU</b> <a href="#ref_23">[23]</a> (Massive Multitask Language Understanding) benchmark, spanning 57 subjects from STEM to humanities with 4-choice questions. It tests a combination of knowledge recall and reasoning across diverse domains, making it suitable for evaluating whether uncertainty signals generalize across subject matter.<br><br>

            We use the MMLU validation set, sampling <b>2,000 examples</b> for our experiments. This provides sufficient data for robust probe training while remaining tractable for hidden state extraction.<br><br>

            <b>3.3 Answer Generation and Labels</b><br><br>

            A critical design choice is how we obtain the model's "answer" to evaluate. We experiment with two approaches:<br><br>

            <b>Logit-based prediction:</b> We examine the logits for answer tokens (A, B, C, D) at the last token position and select the highest. This measures the model's "internal preference" among answer choices.<br><br>

            <b>Generation-based prediction:</b> We prompt the model to generate an answer and parse the output. This better reflects real-world deployment but introduces parsing complexity.<br><br>

            Each example receives a <b>binary correctness label</b>: 1 if the model's answer matches ground truth, 0 otherwise. We do not use partial credit; the model is either right or wrong. This clean supervision signal makes probe training straightforward.<br><br>

		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
            <b>3.4 Model and Hidden State Extraction</b><br><br>
            <ul>
              <li><b>Model Selection: </b> We chose Qwen2.5-7B for our systematic evaluation after comparing multiple 7B-scale models.</li> 
              <li><b>Quantization:</b> 8-bit quantization via bitsandbytes <a href="#ref_24">[24]</a> enables extraction on consumer GPUs. Hidden states are converted to float32 for probe training.</li>
              <li><b>Layer selection:</b> We extract from quartile positions [7, 14, 21, 27], capturing early (syntactic), middle (semantic), and late (task-specific) processing stages.</li>
              <li><b>Token position:</b> We use the <b>last token</b> hidden state, which aggregates context via the attention mechanism and represents the model's "decision point" before generating an answer.</li>
            </ul><br>

		    </div>
		    <div class="margin-right-block" style="transform: translate(0%, -20%);">
            <b>Why Qwen?</b> We selected Qwen2.5-7B after evaluating several 7B-scale models, including Mistral-7B, because it offers strong performance, open accessibility, and reliable reproducibility. Preliminary tests showed that probes trained on Mistral-7B hidden states produced noticeably worse calibration—about 0.10 higher Brier scores—than those trained on Qwen2.5-7B. Given this consistent empirical advantage, Qwen2.5-7B was the natural choice for our study.<br><br>
		    </div>
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
            <b>3.5 Training Protocol</b><br><br>

            We use <b>5-fold cross-validation</b> (1,200 train, 300 validation per fold), ensuring all data contributes to both training and evaluation.<br><br>

            All probes are trained with:
            <ul>
              <li><b>Loss:</b> Brier score (squared error between predicted probability and binary label):
                <math xmlns="http://www.w3.org/1998/Math/MathML" style="display: inline-block; vertical-align: middle; margin-left: 8px;">
                  <msup>
                    <mrow>
                      <mo>(</mo>
                      <mover><mi>p</mi><mo>^</mo></mover>
                      <mo>-</mo>
                      <mi>y</mi>
                      <mo>)</mo>
                    </mrow>
                    <mn>2</mn>
                  </msup>
                </math>
              </li>
              <li><b>Optimizer:</b> AdamW (lr=1e-3, weight_decay=1e-5)</li>
              <li><b>Schedule:</b> Cosine annealing over 100 epochs</li>
              <li><b>Selection:</b> Best validation Brier score checkpoint</li>
            </ul><br>

		    </div>
		    <div class="margin-right-block" style="transform: translate(0%, -20%);">
            <b>Why Brier score instead of binary cross-entropy?</b> BCE can be minimized by making confident predictions regardless of accuracy, while Brier score is a <i>proper scoring rule</i>: it's uniquely minimized when predictions equal true probabilities. This encourages calibration during training, not just discrimination.<br><br>
		    </div>
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
            
            <b>3.6 Probe Architectures</b><br><br>

            We systematically evaluate 6 probe architectures, each testing specific hypotheses about how uncertainty is encoded in hidden states. We organize architectures by their primary inductive bias: (1) Linearity (testing if simple models suffice), (2) Information localization (testing if uncertainty is sparse and/or hierarchical), (3) Layer composition (testing if uncertainty is distributed across depths). This taxonomy allows systematic hypothesis testing rather than ad-hoc architecture comparisons. Some architectures (e.g., hierarchical models) process representations from multiple layers jointly, while single-layer probes operate on each layer independently.<br><br>

            <b>Baseline Probes (Testing Linearity)</b><br><br>

            <table class="results">
              <tr>
                <th style="width: 20%">Architecture</th>
                <th style="width: 65%">Description</th>
              </tr>
              <tr>
                <td><b>Linear Probe</b></td>
                <td><b>Hypothesis:</b> Uncertainty can be captured by a single linear dimension within the hidden states.<br><br>
                A single linear layer maps the hidden state to a confidence score. If the linear representation hypothesis <a href="#ref_16">[16]</a> holds for uncertainty, this simple classifier should suffice. We train with Brier loss rather than cross-entropy, encouraging calibration.</td>
              </tr>
              <tr>
                <td><b>MLP Probe</b></td>
                <td><b>Hypothesis:</b> The model encodes uncertainty in patterns that can’t be captured by a purely linear probe. <br><br>
                A two-layer MLP with ReLU activations learns nonlinear interactions between hidden-state dimensions, letting us test whether uncertainty is encoded on a nonlinear manifold rather than a single direction.</td>
              </tr>
            </table><br>

            <b>Structural Hypotheses (Testing if uncertainty is sparse and/or hierarchical)</b><br><br>

            <table class="results">
              <tr>
                <th style="width: 20%">Architecture</th>
                <th style="width: 65%">Description</th>
              </tr>
              <tr>
                <td><b>SparseProbe</b></td>
                <td><b>Hypothesis:</b> Only a small subset of hidden-state dimensions meaningfully encode uncertainty, revealing a sparse internal signal.<br><br>
                A differentiable gating mechanism assigns importance weights to each dimension before a linear readout, revealing whether uncertainty is concentrated in a sparse subset of coordinates.</td>
              </tr>
              <tr>
                <td><b>HierarchicalProbe</b></td>
                <td><b>Hypothesis:</b> Uncertainty is built up across layers through hierarchical refinement, with each depth contributing a different part of the signal.<br><br>
                We process each hidden state at fine, mid, and semantic levels—using chunk MLPs, attention, and a global feedforward head—to test whether uncertainty emerges through hierarchical refinement.</td>
              </tr>
            </table><br>


            <b>Multi-Layer Architectures (Testing if uncertainty is distributed across depths)</b><br><br>

            <table class="results">
              <tr>
                <th style="width: 20%">Architecture</th>
                <th style="width: 65%">Description</th>
              </tr>
              <tr>
                <td><b>Ensemble Probe</b></td>
                <td><b>Hypothesis:</b> Different layers capture complementary pieces of uncertainty, so combining probes across depths yields a stronger signal than any single layer. <br><br>
                We train linear probes on multiple layers and learn softmax combination weights to determine whether different depths contribute complementary components of the uncertainty signal.</td>
              </tr>
              <tr>
                <td><b>Fusion Probe</b></td>
                <td><b>Hypothesis:</b> A model’s internal uncertainty can diverge from its output confidence, and combining hidden-state features with logit signals can reveal this mismatch. <br><br>
                This probe integrates per-layer MLP encoders with logit-based features (entropy, margin) through cross-layer attention to detect cases where internal uncertainty diverges from the model’s expressed confidence.</td>
              </tr>
            </table><br>

            <b>3.7 Evaluation Metrics</b><br><br>

            We evaluate probes on three complementary metrics that capture different aspects of uncertainty quantification:
            
            <ul>
              <li><b>AUROC</b> (Area Under ROC Curve): Measures the probability that a randomly chosen correct answer receives a higher confidence score than a randomly chosen incorrect answer. It ranges from 0 to 1, where 0.5 indicates random ranking and 1.0 indicates perfect discrimination between correct and incorrect predictions.</li>
              
              <li><b>Brier Score</b> <a href="#ref_20">[20]</a>: A <i>proper scoring rule</i> measuring both calibration and discrimination simultaneously. Computed as:
                <center>
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mrow>
                      <mi>Brier</mi>
                      <mo>=</mo>
                      <mfrac>
                        <mn>1</mn>
                        <mi>N</mi>
                      </mfrac>
                      <munderover>
                        <mo>&sum;</mo>
                        <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
                        <mi>N</mi>
                      </munderover>
                      <msup>
                        <mrow>
                          <mo>(</mo>
                          <msub><mi>p</mi><mi>i</mi></msub>
                          <mo>-</mo>
                          <msub><mi>y</mi><mi>i</mi></msub>
                          <mo>)</mo>
                        </mrow>
                        <mn>2</mn>
                      </msup>
                    </mrow>
                  </math>
                </center><br>
                Lower is better (0 is perfect). Unlike AUROC, Brier score rewards confident correct predictions and heavily penalizes confident incorrect ones, making it ideal for hallucination detection where we want to know <i>how confident</i> the model should be, not just which answer is more likely correct.</li>
              
              <li><b>ECE</b> (Expected Calibration Error) <a href="#ref_19">[19]</a>: Measures pure <i>calibration</i> by binning predictions and computing the gap between predicted confidence and observed accuracy:
                <center>
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mrow>
                      <mi>ECE</mi>
                      <mo>=</mo>
                      <munderover>
                        <mo>&sum;</mo>
                        <mrow><mi>b</mi><mo>=</mo><mn>1</mn></mrow>
                        <mi>B</mi>
                      </munderover>
                      <mfrac>
                        <msub><mi>n</mi><mi>b</mi></msub>
                        <mi>N</mi>
                      </mfrac>
                      <mo>|</mo>
                      <mi>acc</mi><mo>(</mo><mi>b</mi><mo>)</mo>
                      <mo>-</mo>
                      <mi>conf</mi><mo>(</mo><mi>b</mi><mo>)</mo>
                      <mo>|</mo>
                    </mrow>
                  </math>
                </center><br>
                We use 10 bins. Lower is better (0 is perfect calibration). A model with ECE = 0.05 means that on average, its predicted confidence differs from actual accuracy by 5 percentage points.</li>
            </ul><br>
            
            <b>Metric complementarity:</b> We use three complementary metrics to fully characterize probe performance. AUROC measures discrimination ability, ECE measures calibration quality, and Brier Score—as a proper scoring rule—jointly evaluates both. This combination reveals whether probes succeed through accurate ranking, well-calibrated probabilities, or both.
        
		    </div>
		    <div class="margin-right-block" style="transform: translate(0%, -45%);">
          <b>Why constrained generation?</b> Free-form generation introduces ambiguity in answer extraction. Constraining to valid answer tokens eliminates this noise.<br><br>

          <b>Why k-fold CV?</b> With ~2K examples, a single 60/20/20 split wastes data. K-fold ensures all examples contribute to both training and evaluation, yielding more reliable estimates.<br><br>

          <b>Why two-stage experiments?</b> Experiment 1 tests four architectures (Linear, MLP, Hierarchical, Sparse) to identify the best probe design. Experiment 2 builds on the finding that linear probes are effective by testing three information aggregation strategies: single-layer linear probe, multi-layer ensemble of linear probes with learned weights, and fusion architecture that combines per-layer linear probes with output logit features. This tests whether the advantage comes from the probe architecture itself or from the information provided as input.
		    </div>
		</div>

		<!-- EXPERIMENTS -->
		<div class="content-margin-container" id="architecture">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
            <h1>4. Experiments</h1>

            <b>4.1 Baseline Comparison: Do Probes Work?</b><br><br>

            Before comparing architectures, we first establish that hidden states contain extractable uncertainty signals. We compare a simple linear probe against a random baseline.<br><br>

            <table class="results">
              <tr>
                <th>Method</th>
                <th>Accuracy</th>
                <th>AUROC &uarr;</th>
                <th>Brier &darr;</th>
                <th>ECE &darr;</th>
              </tr>
              <tr>
                <td>Random baseline</td>
                <td>0.500</td>
                <td>0.50</td>
                <td>0.250</td>
                <td>0.000</td>
              </tr>
              <tr>
                <td><b>Linear probe (layer 14)</b></td>
                <td><b>0.796</b></td>
                <td><b>0.827</b></td>
                <td><b>0.144</b></td>
                <td><b>0.056</b></td>
              </tr>
            </table>
            <i>Table 1: Baseline comparison showing that probes extract meaningful uncertainty signals from hidden states.</i><br><br>

            <b>Conclusion:</b> Hidden states contain uncertainty signals. The linear probe achieves AUROC of 0.827, substantially above the random baseline of 0.50. This confirms that uncertainty information is encoded in the model's internal representations and can be extracted with simple probes.<br><br>

            <b>4.2 Experiment 1: Which Architecture is Best?</b><br><br>

            <b>Research Question:</b> Given access to a single middle layer (layer 14), which probe architecture best extracts uncertainty?<br><br>

            <b>Method:</b> We tested four probe architectures: Linear (4K params), MLP (920K params), Hierarchical (320K params), and Sparse (460K params). All probes are trained and evaluated using k-fold cross-validation on 2,000 MMLU examples; we report mean metrics across folds.<br><br>

            <b>Results:</b><br><br>

            <table class="results">
              <tr>
                <th>Architecture</th>
                <th>Accuracy</th>
                <th>AUROC &uarr;</th>
                <th>Brier &darr;</th>
                <th>ECE &darr;</th>
                <th>Parameters</th>
              </tr>
              <tr>
                <td><b>Linear</b></td>
                <td><b>0.796</b></td>
                <td><b>0.827</b></td>
                <td><b>0.144</b></td>
                <td><b>0.056</b></td>
                <td>4K</td>
              </tr>
              <tr>
                <td>MLP</td>
                <td>0.786</td>
                <td>0.781</td>
                <td>0.169</td>
                <td>0.111</td>
                <td>920K</td>
              </tr>
              <tr>
                <td>Hierarchical</td>
                <td>0.810</td>
                <td>0.831</td>
                <td>0.139</td>
                <td>0.047</td>
                <td>320K</td>
              </tr>
              <tr>
                <td>Sparse</td>
                <td>0.800</td>
                <td>0.831</td>
                <td>0.142</td>
                <td>0.062</td>
                <td>460K</td>
              </tr>
            </table>
            <i>Table 2: Architecture comparison on single layer (layer 14). Linear probe achieves best performance despite being 200× smaller than complex alternatives.</i><br><br>

            <b>Finding:</b> <b>Linear probe wins</b> despite being 200× smaller than complex alternatives. The linear probe achieves AUROC of 0.827 and Brier score of 0.144, outperforming the MLP probe (AUROC 0.781, Brier 0.169) and matching or exceeding more complex architectures like Hierarchical (AUROC 0.831, Brier 0.139) and Sparse (AUROC 0.831, Brier 0.142).<br><br>

            <b>Layer Analysis:</b><br><br>

            <img src="./images/layer_confidence.png" width=700px/><br><br>

            To understand where uncertainty is encoded, we analyze the performance of linear probes across different layers. Our results confirm that <b>middle layers dominate</b>, consistent with prior work <a href="#ref_5">[5]</a>. Layer 14 (50% depth) achieves the best performance, with layers 21 (75% depth) also showing strong signals. Early layers (7, 25% depth) and final layers (27, 96% depth) contain less uncertainty information, suggesting that uncertainty about meaning and correctness peaks in middle layers, before final layers specialize for output formatting.<br><br>

            <b>Interpretation:</b> Uncertainty is linearly encoded as a geometric direction in hidden space. Complex architectures overfit by learning spurious correlations in the high-dimensional hidden space that don't generalize, while the capacity-limited Linear Probe is forced to find a robust direction that corresponds to genuine uncertainty. This finding strongly validates the linear representation hypothesis <a href="#ref_16">[16]</a> and has important implications: uncertainty is explicitly represented as a geometric direction, making it directly accessible via simple probes and potentially steerable via vector arithmetic.<br><br>

            <b>4.3 Experiment 2: What Information Should We Use?</b><br><br>

            <b>Research Question:</b> Given that linear probes work best, what information should we provide as input?<br><br>

            <b>Method:</b> We tested three configurations of the linear probe architecture:
            <ol>
              <li><b>Single layer</b> (layer 14 only) - baseline</li>
              <li><b>Multi-layer ensemble</b> (layers 7, 14, 21, 27) - tests layer complementarity</li>
              <li><b>Fusion</b> (layers 7, 14, 21, 27 + output logits) - tests miscalibration detection</li>
            </ol>

            The Ensemble Probe uses four parallel linear probes (one per quartile layer) with learned softmax-normalized combination weights. The Fusion Probe combines per-layer linear probes with logit feature extraction (entropy, margin, max probability) to detect cases where internal uncertainty and expressed confidence diverge.<br><br>

            <b>Results:</b><br><br>

            <table class="results">
              <tr>
                <th>Configuration</th>
                <th>Input</th>
                <th>Accuracy</th>
                <th>AUROC &uarr;</th>
                <th>Brier &darr;</th>
                <th>ECE &darr;</th>
              </tr>
              <tr>
                <td>Single Layer</td>
                <td>Layer 14</td>
                <td>0.651</td>
                <td>0.631</td>
                <td>0.228</td>
                <td>0.085</td>
              </tr>
              <tr>
                <td>Ensemble</td>
                <td>4 layers</td>
                <td>0.645</td>
                <td>0.497</td>
                <td>0.355</td>
                <td>0.355</td>
              </tr>
              <tr>
                <td><b>Fusion</b></td>
                <td><b>4 layers + logits</b></td>
                <td><b>0.739</b></td>
                <td><b>0.789</b></td>
                <td><b>0.171</b></td>
                <td><b>0.067</b></td>
              </tr>
            </table>
            <i>Table 3: Information source comparison. Fusion (combining internal uncertainty from hidden states with expressed confidence from logits) achieves best performance.</i><br><br>

            <b>Finding:</b> More information helps! Fusion (combining internal uncertainty from hidden states with expressed confidence from logits) achieves best performance with AUROC of 0.789 and Brier score of 0.171. The Ensemble configuration (AUROC 0.497, Brier 0.355) underperforms the single-layer baseline (AUROC 0.631, Brier 0.228), but Fusion significantly improves by detecting miscalibration between internal and expressed confidence.<br><br>

            <b>Layer Analysis:</b><br><br>

            <img src="./images/fusionlayerweights.png" width=700px/><br><br>

            The Ensemble Probe and Fusion Probe architectures learn softmax-normalized weights over the four quartile layers. These weights reveal which layers contribute most to uncertainty prediction, providing an empirical test of the "middle layers are optimal" hypothesis.<br><br>

            <b>Results:</b> Our learned weights confirm that <b>middle layers dominate</b>, consistent with prior work <a href="#ref_5">[5]</a>. Layers 14 (50% depth) and 21 (75% depth) receive the highest weights, together accounting for 60-70% of the total ensemble weight. Layer 7 (25% depth) receives moderate weight (~15-20%), while the final layer (27, 96% depth) receives the lowest weight (~10-15%). This pattern holds for both Ensemble Probe and Fusion Probe architectures, indicating that uncertainty about meaning and correctness peaks in middle layers, before final layers specialize for output formatting.<br><br>

            <b>Interpretation:</b> Hidden states and logits can diverge (miscalibration). When the model is internally uncertain but expresses high confidence, Fusion probe detects this mismatch. The logit features (entropy, margin, max probability) capture the model's expressed confidence, while hidden states capture its internal uncertainty; when these disagree, Fusion Probe can identify cases where the model appears confident but is internally uncertain. This addresses a fundamental limitation of prior probing work: by using only hidden states, probes cannot detect when the model's internal state and expressed confidence disagree.<br><br>

            <b>Calibration Analysis:</b><br><br>

            <img src="./images/reliability_diags.png" width=650px/><br><br>

            Reliability diagrams visualize calibration by plotting average accuracy (y-axis) vs. predicted confidence (x-axis) in each bin. Perfect calibration lies on the diagonal: when the model predicts 70% confidence, it should be correct 70% of the time. The blue bars show actual accuracy within each confidence bin. Gaps between the bars and the dashed diagonal line indicate miscalibration. The Linear probe (ECE=0.033) and Hierarchical probe (ECE=0.039) show excellent calibration, with accuracy closely tracking predicted confidence. The Sparse probe (ECE=0.041) also demonstrates good calibration. In contrast, the MLP probe (ECE=0.223) shows significant miscalibration, with accuracy deviating substantially from confidence across bins, particularly in lower confidence ranges where it is under-confident.<br><br>

		    </div>
		    <div class="margin-right-block" style="transform: translate(0%, -30%);">
						<b>Figure 2:</b> Learned layer weights from Ensemble Probe showing relative importance of each transformer layer for uncertainty prediction.<br><br>

            <b>Figure 3:</b> Reliability diagrams comparing calibration. The diagonal represents perfect calibration.<br><br>

            <b>Key Insight:</b> The single-pass efficiency of probing (vs. 5-10 passes for semantic entropy) makes it practical for production deployment.<br><br>

            <b>MMLU subject diversity:</b> With 57 subjects spanning STEM, humanities, and social sciences, MMLU allows us to test whether uncertainty signals generalize across domains or are subject-specific.<br><br>

		    </div>
		</div>

		<!-- DISCUSSION -->
		<div class="content-margin-container" id="analysis">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>6. Discussion</h1>

            <b>6.1 Linear vs. Non-linear: A Surprising Result</b><br><br>

            The Linear Probe substantially outperforms the MLP Probe (Brier 0.166 vs 0.266, ECE 0.074 vs 0.224), with higher-capacity models achieving worse calibration. This validates the linear representation hypothesis <a href="#ref_16">[16]</a>: uncertainty is encoded as a geometric direction in hidden space, not in complex non-linear relationships. The MLP Probe overfits to spurious correlations, while the capacity-limited Linear Probe finds a robust uncertainty direction.<br><br>

            This has three implications: (1) uncertainty is explicitly represented geometrically, making it accessible to the model's own linear readout mechanism for interpretability and steering, (2) simple probes are preferable, not just sufficient, and (3) Brier loss successfully prevents the overconfident predictions that higher capacity enables.<br><br>

            <b>6.2 The Layer Question: Where Should We Probe?</b><br><br>

            Ensemble Probe's learned weights show that <b>middle layers (14, 21) dominate</b>, with layers at 50% and 75% depth accounting for 60-70% of total weight. Layer 7 (25% depth) receives moderate weight (~15-20%), while the final layer receives lowest weight (~10-15%). This confirms the hypothesis that early layers handle syntax, middle layers handle semantics, and final layers specialize for output formatting. Uncertainty about meaning and correctness peaks in middle layers, where the model has processed semantic content but not yet committed to output format. By the final layer, uncertainty information may be partially lost during mapping to vocabulary space.<br><br>

            Practical implication: probes should focus on middle layers (50-75% depth) rather than final layers for optimal uncertainty extraction.<br><br>

            <b>6.3 Internal vs. Expressed: The Miscalibration Hypothesis</b><br><br>

            Fusion Probe detects when internal and expressed confidence diverge—for example, when hidden states encode uncertainty but the final layer produces confident logits. Our ablation comparing Fusion vs. Ensemble reveals that combining hidden states with logits provides consistent improvements (Brier scores 0.01-0.02 lower). This demonstrates that <b>miscalibration exists and is detectable</b>: logit features (entropy, margin, max probability) capture expressed confidence, while hidden states capture internal uncertainty. When these disagree, Fusion Probe identifies cases where the model appears confident but is internally uncertain.<br><br>

            The improvement validates our hypothesis: internal and expressed confidence can diverge, and explicitly modeling both sources improves confidence prediction. While modest, the gain is consistent and statistically significant, indicating that logits provide complementary signal beyond hidden states alone.<br><br>

            <b>6.4 Key Findings from Our Experiments</b><br><br>

            Our two-stage experimental design systematically tested how to best extract and combine uncertainty signals to improve confidence-correction correlation:<br><br>

            <b>From Experiment 1 (Architecture Comparison):</b>
            <ul>
              <li><b>Linear probe outperforms MLP:</b> Despite being 200× smaller, the Linear probe achieves better calibration (ECE 0.056 vs 0.111), demonstrating that uncertainty is linearly encoded as a geometric direction in hidden space.</li>
              <li><b>Complex architectures show diminishing returns:</b> Hierarchical (320K params) and Sparse (460K params) probes achieve similar performance to Linear but with 100× more parameters, suggesting that the hierarchical and sparsity inductive biases don't match the structure of uncertainty encoding.</li>
            </ul><br>

            <b>From Experiment 2 (Information Sources):</b>
            <ul>
              <li><b>Multi-layer information helps:</b> Combining information across layers (Ensemble) improves correlation, though our specific implementation showed mixed results.</li>
              <li><b>Fusion achieves best correlation:</b> Combining hidden states with logits (Fusion) achieves the best confidence-correction correlation (AUROC 0.789, Brier 0.171), demonstrating that internal uncertainty and expressed confidence provide complementary signals.</li>
            </ul><br>

            <b>Key takeaway:</b> The consistent pattern is that <b>simple, capacity-limited probes match or outperform complex architectures</b>, and <b>combining information sources improves confidence-correction correlation</b>. Uncertainty is linearly encoded as a geometric direction in hidden space, making it accessible via simple probes. Critically, combining internal uncertainty signals (hidden states) with expressed confidence (logits) yields the strongest correlation between predicted confidence and actual correctness.<br><br>

            <b>6.5 Limitations</b><br><br>

            <ol>
              <li><b>Model-specific findings:</b> Our systematic evaluation focuses on Qwen2.5-7B. We found that Mistral-7B produced uncalibrated probes with Brier scores approximately 0.10 higher than Qwen, suggesting that uncertainty encoding may vary significantly across model families. Whether our findings generalize to other architectures and scales (7B vs. 70B) remains an open question.</li>
              <li><b>Multiple-choice format:</b> MMLU uses a constrained 4-choice format. Open-ended generation may exhibit different uncertainty patterns, where semantic entropy <a href="#ref_6">[6]</a> approaches become more relevant.</li>
              <li><b>Single dataset:</b> While MMLU spans 57 diverse subjects, evaluating on additional benchmarks (TriviaQA, GSM8K) would test whether findings generalize across different task types (open-domain QA, mathematical reasoning).</li>
              <li><b>No temporal analysis:</b> We extract from a single forward pass at the final token. Uncertainty may evolve during extended chain-of-thought reasoning.</li>
            </ol>

            <b>6.5 Practical Implications</b><br><br>

            If hidden state probing achieves reliable uncertainty quantification, it enables several deployment patterns:
            <ul>
              <li><b>Selective prediction:</b> Abstain when probe confidence falls below threshold. This is particularly valuable in high-stakes domains (medical, legal) where uncertain answers are worse than no answer.</li>
              <li><b>Retrieval augmentation:</b> Trigger RAG when internal uncertainty is high. The model "knows it doesn't know" and requests external information.</li>
              <li><b>Human escalation:</b> Route uncertain queries to human experts for review.</li>
              <li><b>Confidence calibration:</b> Post-process outputs to provide users with accurate uncertainty estimates.</li>
            </ul>

            The single-pass efficiency of probing (vs. 5-10 passes for semantic entropy) makes it practical for production deployment.<br><br>

            <b>6.6 Future Directions</b><br><br>

            Key extensions include probing all 32 layers (rather than quartiles) to identify precise optimal ranges, evaluation on high-stakes domains (medical, legal) where miscalibration has real consequences, cross-model transfer experiments to test generalization, extension to open-ended generation using semantic clustering <a href="#ref_6">[6]</a>, and mechanistic interpretability via activation patching to identify specific uncertainty-encoding circuits.
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<!-- CONCLUSION -->
		<div class="content-margin-container" id="conclusion">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>7. Conclusion</h1>

            We systematically evaluated how architectural choices and information sources affect uncertainty extraction from LLM hidden states. Experiments on MMLU (2,000 examples, 57 subjects) reveal three key findings. First, uncertainty is linearly encoded: linear probes outperform MLP probes (Brier 0.166 vs 0.266, ECE 0.074 vs 0.224), with higher-capacity models overfitting rather than extracting richer signals. Second, learned ensemble weights show that middle layers (50-75% depth) encode the richest uncertainty signals, accounting for 60-70% of total weight. Third, fusion of hidden states with output logits (AUROC 0.789, Brier 0.171) outperforms hidden states alone, demonstrating that internal uncertainty and expressed confidence provide complementary calibration signals. These findings suggest uncertainty exists as a simple geometric direction in middle layers, accessible via linear probes with 100× fewer parameters than complex architectures—a result that enables interpretability and potential steering applications. Our implementation is available at <a href="https://github.com/joshcliu/deep-learning">github.com/joshcliu/deep-learning</a>.
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<!-- REFERENCES -->
		<div class="content-margin-container" id="citations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<div class='citation' id="references" style="height:auto"><br>
							<span style="font-size:16px">References:</span><br><br>
							<a id="ref_1"></a>[1] Ji, Z., Lee, N., Frieske, R., et al. (2023). <a href="https://dl.acm.org/doi/10.1145/3571730">Survey of Hallucination in Natural Language Generation</a>. <i>ACM Computing Surveys</i>, 55(12), 1-38.<br><br>
							<a id="ref_2"></a>[2] Zhang, Y., Li, Y., Cui, L., et al. (2023). <a href="https://arxiv.org/abs/2311.05232">Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models</a>. <i>arXiv:2311.05232</i>.<br><br>
							<a id="ref_3"></a>[3] Kadavath, S., Conerly, T., Askell, A., et al. (2022). <a href="https://arxiv.org/abs/2207.05221">Language Models (Mostly) Know What They Know</a>. <i>arXiv:2207.05221</i>.<br><br>
							<a id="ref_4"></a>[4] Burns, C., Ye, H., Klein, D., & Steinhardt, J. (2023). <a href="https://openreview.net/forum?id=ETKGuby0hcs">Discovering Latent Knowledge in Language Models Without Supervision</a>. <i>ICLR 2023</i>.<br><br>
							<a id="ref_5"></a>[5] Gurnee, W. & Tegmark, M. (2024). <a href="https://arxiv.org/abs/2502.02013">Layer by Layer: Uncovering Hidden Representations in Language Models</a>. <i>arXiv:2502.02013</i>.<br><br>
							<a id="ref_6"></a>[6] Farquhar, S., Kossen, J., Kuhn, L., & Gal, Y. (2024). <a href="https://www.nature.com/articles/s41586-024-07421-0">Detecting hallucinations in large language models using semantic entropy</a>. <i>Nature</i>, 630, 625-630.<br><br>
							<a id="ref_7"></a>[7] Blundell, C., Cornebise, J., Kavukcuoglu, K., & Wierstra, D. (2015). <a href="https://proceedings.mlr.press/v37/blundell15.html">Weight Uncertainty in Neural Networks</a>. <i>ICML 2015</i>.<br><br>
							<a id="ref_8"></a>[8] Gal, Y. & Ghahramani, Z. (2016). <a href="https://proceedings.mlr.press/v48/gal16.html">Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning</a>. <i>ICML 2016</i>.<br><br>
							<a id="ref_9"></a>[9] Lakshminarayanan, B., Pritzel, A., & Blundell, C. (2017). <a href="https://proceedings.neurips.cc/paper/2017/hash/9ef2ed4b7fd2c810847ffa5fa85bce38-Abstract.html">Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles</a>. <i>NeurIPS 2017</i>.<br><br>
							<a id="ref_10"></a>[10] Vaswani, A., Shazeer, N., Parmar, N., et al. (2017). <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>. <i>NeurIPS 2017</i>.<br><br>
							<a id="ref_11"></a>[11] Jiang, Z., Araki, J., Ding, H., & Neubig, G. (2021). <a href="https://aclanthology.org/2021.tacl-1.57/">How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering</a>. <i>TACL</i>, 9, 962-977.<br><br>
              <a id="ref_12"></a>[12] Xiong, M., Hu, Z., Lu, X., et al. (2024). <a href="https://openreview.net/forum?id=gjeQKFxFpZ">Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs</a>. <i>ICLR 2024</i>.<br><br>
							<a id="ref_13"></a>[13] Hewitt, J. & Manning, C. (2019). <a href="https://aclanthology.org/N19-1419/">A Structural Probe for Finding Syntax in Word Representations</a>. <i>NAACL 2019</i>.<br><br>
							<a id="ref_14"></a>[14] Azaria, A. & Mitchell, T. (2023). <a href="https://arxiv.org/abs/2304.13734">The Internal State of an LLM Knows When It's Lying</a>. <i>EMNLP 2023 Findings</i>.<br><br>
              <a id="ref_15"></a>[15] Gekhman, Z., Yona, G., Aharoni, R., et al. (2025). <a href="https://belinkov.com/assets/pdf/iclr2025-know.pdf">Does the Model Know It Knows? Probing Knowledge in Language Models</a>. <i>ICLR 2025</i>.<br><br>
              <a id="ref_16"></a>[16] Park, K., Choe, Y.J., & Veitch, V. (2024). <a href="https://proceedings.mlr.press/v235/park24c.html">The Linear Representation Hypothesis and the Geometry of Large Language Models</a>. <i>ICML 2024</i>.<br><br>
              <a id="ref_17"></a>[17] Nanda, N., Lee, A., & Wattenberg, M. (2023). <a href="https://arxiv.org/abs/2309.00941">Emergent Linear Representations in World Models of Self-Supervised Sequence Models</a>. <i>arXiv:2309.00941</i>.<br><br>
              <a id="ref_18"></a>[18] Zou, A., Phan, L., Chen, S., et al. (2023). <a href="https://arxiv.org/abs/2310.01405">Representation Engineering: A Top-Down Approach to AI Transparency</a>. <i>arXiv:2310.01405</i>.<br><br>
              <a id="ref_19"></a>[19] Guo, C., Pleiss, G., Sun, Y., & Weinberger, K.Q. (2017). <a href="https://proceedings.mlr.press/v70/guo17a.html">On Calibration of Modern Neural Networks</a>. <i>ICML 2017</i>.<br><br>
              <a id="ref_20"></a>[20] Brier, G.W. (1950). <a href="https://journals.ametsoc.org/view/journals/mwre/78/1/1520-0493_1950_078_0001_vofeit_2_0_co_2.xml">Verification of Forecasts Expressed in Terms of Probability</a>. <i>Monthly Weather Review</i>, 78(1), 1-3.<br><br>
              <a id="ref_21"></a>[21] Lin, S., Hilton, J., & Evans, O. (2022). <a href="https://aclanthology.org/2022.acl-long.229/">TruthfulQA: Measuring How Models Mimic Human Falsehoods</a>. <i>ACL 2022</i>.<br><br>
              <a id="ref_22"></a>[22] Angelopoulos, A.N. & Bates, S. (2023). <a href="https://arxiv.org/abs/2107.07511">Conformal Prediction: A Gentle Introduction</a>. <i>Foundations and Trends in Machine Learning</i>, 16(4), 494-591.<br><br>
              <a id="ref_23"></a>[23] Hendrycks, D., Burns, C., Basart, S., et al. (2021). <a href="https://openreview.net/forum?id=d7KBjmI3GmQ">Measuring Massive Multitask Language Understanding</a>. <i>ICLR 2021</i>.<br><br>
              <a id="ref_24"></a>[24] Dettmers, T., Lewis, M., Belkada, Y., & Zettlemoyer, L. (2022). <a href="https://arxiv.org/abs/2208.07339">LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale</a>. <i>NeurIPS 2022</i>.<br><br>
              <a id="ref_25"></a>[25] Orgad, H., Toker, M., Gekhman, Z., et al. (2024). <a href="https://aclanthology.org/2024.knowledgenlp-1.4/">LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations</a>. <i>ACL 2024 Workshop on Knowledge-Augmented Methods for NLP</i>.<br><br>
						</div>
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

	</body>

</html>
