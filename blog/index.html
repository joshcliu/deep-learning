<html>
<head>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

<!-- MathJax for consistent math rendering across browsers -->
<script>
MathJax = {
  tex: {
    inlineMath: [['\\(', '\\)']],
    displayMath: [['$$', '$$']],
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>

<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
	body {
		background-color: #f5f9ff;
	}

	.content-margin-container {
		display: flex;
		width: 100%;
		justify-content: left;
		align-items: center;
	}
	.main-content-block {
		width: 70%;
    max-width: 1100px;
		background-color: #fff;
		border-left: 1px solid #DDD;
		border-right: 1px solid #DDD;
		padding: 8px 8px 8px 8px;
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
	}
	.margin-left-block {
			font-size: 14px;
			width: 15%;
			max-width: 130px;
			position: relative;
			margin-left: 10px;
			text-align: left;
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
			padding: 5px;
	}
	.margin-right-block {
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
			font-size: 14px;
			width: 25%;
			max-width: 256px;
			position: relative;
			text-align: left;
			padding: 10px;
	}

	img {
			max-width: 100%;
			height: auto;
			display: block;
			margin: auto;
	}

	a:link,a:visited
	{
		color: #0e7862;
		text-decoration: none;
	}
	a:hover {
		color: #24b597;
	}

	h1 {
		font-size: 26px;
		margin-top: 8px;
		margin-bottom: 14px;
	}

	.subsection {
		font-size: 18px;
		font-weight: bold;
		margin-top: 6px;
		display: inline-block;
	}

	table.header {
    font-weight: 300;
    font-size: 17px;
    flex-grow: 1;
		width: 70%;
    max-width: calc(100% - 290px);
	}
	table td, table td * {
	    vertical-align: middle;
	    position: relative;
	}

	hr {
    height: 1px;
    border: none;
    background-color: #DDD;
  }

	div.hypothesis {
		width: 80%;
		background-color: #EEE;
		border: 1px solid black;
		border-radius: 10px;
		font-family: Courier;
		font-size: 16px;
		text-align: center;
		margin: auto;
		padding: 16px 16px 16px 16px;
	}

	div.citation {
    font-size: 0.8em;
    background-color:#fff;
    padding: 10px;
		height: 200px;
  }

	table.results {
		border-collapse: collapse;
		width: 100%;
		margin: 10px 0;
	}
	table.results th, table.results td {
		border: 1px solid #DDD;
		padding: 8px;
		text-align: left;
	}
	table.results th {
		background-color: #f5f9ff;
	}

	code {
		background-color: #f5f5f5;
		padding: 2px 6px;
		border-radius: 3px;
		font-family: 'Courier New', Courier, monospace;
		font-size: 14px;
	}

	pre {
		background-color: #f5f5f5;
		padding: 12px;
		border-radius: 5px;
		overflow-x: auto;
		font-family: 'Courier New', Courier, monospace;
		font-size: 13px;
	}

</style>

	  <title>Probing through LLMs: Extracting Confidence from Hidden States</title>
      <meta property="og:title" content="Probing through LLMs: Extracting Confidence from Hidden States" />
			<meta charset="UTF-8">
  </head>

  <body>

		<!-- HEADER -->
		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<table class="header" align=left>
								<tr>
									<td colspan=4>
										<span style="font-size: 32px; font-family: 'Courier New', Courier, monospace;">Probing through LLMs</span>
									</td>
								</tr>
								<tr>
									<td colspan=4>
										<span style="font-size: 20px; color: #666;">Extracting Confidence from Hidden States</span>
									</td>
								</tr>
								<tr>
										<td align=left>
												<span style="font-size:17px"><a href="#">Joshua Liu</a></span>
										</td>
										<td align=left>
												<span style="font-size:17px"><a href="#">Carol Li</a></span>
										</td>
										<td align=left>
												<span style="font-size:17px"><a href="#">Maureen Zhang</a></span>
										</td>
								<tr>
									<td colspan=4 align=left><span style="font-size:18px">Final project for 6.7960, MIT</span></td>
								</tr>
						</table>
					</div>
					<div class="margin-right-block">
					</div>
		</div>

    <!-- INTRODUCTION -->
    <div class="content-margin-container" id="intro">
				<div class="margin-left-block">
          <div style="position:fixed; max-width:inherit; top:max(20%,120px)">
              <b style="font-size:16px">Outline</b><br><br>
              <a href="#intro">Introduction</a><br><br>
              <a href="#background">Background</a><br><br>
              <a href="#methodology">Methodology</a><br><br>
              <a href="#results">Experiments</a><br><br>
              <a href="#analysis">Discussion</a><br><br>
              <a href="#conclusion">Conclusion</a><br><br>
          </div>
				</div>
		    <div class="main-content-block">
						<h1>1. Introduction</h1>

            Large language models (LLMs) are known to present incorrect answers with high confidence, which is particularly problematic in high-stakes domains like medicine, law, and finance <a href="#ref_1">[1]</a><a href="#ref_2">[2]</a>. A significant obstacle in decreasing the rate of such mistakes is the lack of a reliable signal regarding when an LLM is wrong. Reliable uncertainty estimates would allow models to return answers only when fairly certain, optimize when to look up information, and know when to defer to a human user, making LLMs safer for real-world scenarios. We investigate whether models know what they don't know, and whether that knowledge can be gleaned from their internal representations.<br><br>

            <div class="hypothesis">
              <b>Central Hypothesis:</b> A model's internal representation of uncertainty can differ from the confidence it expresses in its outputs. By detecting this divergence (fusing hidden states with output logits), we can extract calibration signals that neither source provides alone.
            </div><br>

            Evidence suggests such internal signals exist: Kadavath et al. <a href="#ref_3">[3]</a> showed LLMs can predict their own correctness well above chance, Burns et al. <a href="#ref_4">[4]</a> found that hidden states encode truthfulness even without supervised labels, and Gurnee & Tegmark <a href="#ref_5">[5]</a> demonstrated that middle transformer layers encode richer semantic information than final layers. We use probing, a technique that trains lightweight classifiers on frozen hidden states, as introduced by Hewitt & Manning <a href="#ref_6">[6]</a> and later used for LLM error detection by Azaria & Mitchell <a href="#ref_7">[7]</a>. While prior work has probed hidden states <i>or</i> analyzed output probabilities in isolation, we ask whether <b>combining both sources reveals miscalibration</b>: cases where internal uncertainty and expressed confidence disagree. Specifically: (1) Is uncertainty linearly encoded or does it require non-linear extraction? (2) Is uncertainty concentrated in specific layers or distributed across depths? and (3) Can fusing hidden states with output logits detect when internal and expressed confidence diverge?
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>


		<!-- BACKGROUND AND RELATED WORK -->
		<div class="content-margin-container" id="background">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
					<h1>2. Background and Related Work</h1>

          Existing uncertainty quantification methods for LLMs fall into three categories. <b>Token probability methods</b> use the model's softmax scores as confidence, but Jiang et al. <a href="#ref_8">[8]</a> showed these are often poorly calibrated. <b>Consistency-based methods</b> like semantic entropy <a href="#ref_9">[9]</a> generate multiple responses and measure agreement, allowing hallucination detection but requiring 5-10 forward passes per query. <b>Verbalized confidence</b> asks models to self-report uncertainty, but Xiong et al. <a href="#ref_10">[10]</a> found they tend to be systematically overconfident. These limitations motivate looking inside the model's internal representations.<br><br>

        <span class="subsection">2.1 Probing Internal Representations</span>
          <p>
          Our approach fits into a fourth paradigm: probing internal representations. The core idea is that if a model encodes something like “confidence” 
          in its internal representations, then a lightweight classifier trained on its hidden states should be able to extract it. This aims to
          keep the single-forward-pass efficiency of logit-based methods while tapping into
          potentially richer signals than output probabilities or verbalized confidence.
          </p>
          
          <p>
            Probing itself has a long history in NLP. Hewitt and Manning <a href="#ref_6">[6]</a> showed that
            syntactic structure can be linearly recovered from BERT’s hidden space.
            For uncertainty, Kadavath et al. <a href="#ref_3">[3]</a> found that large LLMs can meaningfully
            judge their own answers, and Burns et al. <a href="#ref_4">[4]</a> proposed Contrast-Consistent
            Search (CCS), which recovers a truthfulness signal from hidden states without
            labels, though it relies on contrastive statement pairs and does not directly
            handle open-ended generation.
          </p>

          
          <p>
          More recent work asks <i>where</i> this kind of information lives. Azaria and Mitchell
          <a href="#ref_7">[7]</a> trained classifiers on hidden states and found strong error signals,
          especially in intermediate layers. Gekhman et al. <a href="#ref_11">[11]</a> showed that models can
          encode the right answer internally yet still output something wrong, implying that internal
          representations may be more informative than the generated text itself. This motivates our
          focus on probing hidden states to study how uncertainty is represented inside the model.
          </p>
          
		<span class="subsection">2.2 Calibration</span>
          <p>
          A well-calibrated model's confidence matches its accuracy: when it predicts 70% confidence, it should be correct 70% of the time. Calibration differs from discrimination: a model may correctly rank answers while still assigning highly inaccurate probabilities. Guo et al. <a href="#ref_12">[12]</a> showed that modern neural networks are systematically overconfident and introduced Expected Calibration Error (ECE), which bins predictions by confidence and measures the gap between predicted confidence and observed accuracy.
          </p>

          <p>
          Post-hoc calibration methods attempt to fix miscalibration after training. <b>Temperature scaling</b> divides logits by a learned temperature \(T > 1\) to soften overconfident predictions, while <b>Platt scaling</b> fits a logistic regression on held-out data. However, these methods assume the model's ranking is correct and only adjust the probability scale; they cannot fix cases where the model is confidently wrong about ordering. For LLMs, calibration is particularly challenging because confidence can vary across domains, question types, and even prompt phrasings.
          </p>

		<span class="subsection">2.3 Research Gaps We Address</span>
          <p>
          Despite substantial progress, key questions remain about how LLMs represent and express uncertainty. We address these gaps through systematic experimentation and a novel fusion architecture.
          </p>

          <ol>
            <li>
              <b>Internal vs. expressed confidence divergence.</b><br>
              Existing studies focus on either hidden-state signals
              <a href="#ref_7">[7]</a> or output probabilities <a href="#ref_8">[8]</a> in isolation, leaving
              unexplored whether their <i>disagreement</i> signals miscalibration.<br>
              <i>Our contribution:</i> We introduce a <b>Fusion Probe</b> that combines hidden states
              from multiple layers with output logit features (entropy, margin, max probability).
              This architecture detects cases where internal uncertainty and expressed confidence
              diverge, achieving 19% better calibration than hidden-state-only approaches (Brier: 0.098 vs 0.121).
            </li>
            <br>
            <li>
              <b>Layer localization of uncertainty.</b><br>
              Prior work typically probes single layers <a href="#ref_7">[7]</a>, leaving open
              whether uncertainty is concentrated in specific depths or distributed across the network.<br>
              <i>Our contribution:</i> We confirm that middle layers (50% depth) achieve best single-layer
              performance, but develop an Ensemble Probe with learned layer weights showing that
              <b>combining layers outperforms any single layer</b>. The near-uniform weights indicate
              each layer contributes complementary uncertainty signal.
            </li>
            <br>
            <li>
              <b>Linear vs. non-linear encoding.</b><br>
              Most probing work assumes linear classifiers suffice
              <a href="#ref_6">[6]</a><a href="#ref_7">[7]</a>, but this has not been rigorously tested
              against non-linear alternatives for uncertainty.<br>
              <i>Our contribution:</i> We introduce novel Hierarchical and Sparse probe architectures that
              <b>significantly outperform standard MLP baselines</b> (Brier: 0.143 vs 0.243). While linear probes
              achieve statistically equivalent performance to our architectures despite 80-230× fewer parameters,
              this finding itself is notable: it validates the linear representation hypothesis and shows that
              the MLP's poor performance stems from overfitting, not from non-linearity being unnecessary.
            </li>
          </ol>

		</div>
		    <div class="margin-right-block" style="transform: translate(0%, -30%);">
          
		    </div>
		</div>

		<!-- METHODOLOGY -->
		<div class="content-margin-container" id="methodology">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
            <h1>3. Methodology</h1>

            <span class="subsection">3.1 Problem Formulation</span><br><br>

            Given an LLM \(M\), a question \(q\), and the model's generated answer \(a\), we aim to predict whether \(a\) is correct. Formally, let \(\mathbf{h}^{(l)} \in \mathbb{R}^d\) denote the hidden state at layer \(l\), and let \(\mathbf{z} \in \mathbb{R}^K\) denote the output logits for \(K\) answer choices. We train a probe \(f_\theta\) to predict:<br><br>

            $$\hat{p} = f_\theta\bigl(\mathbf{h}^{(l_1)}, \ldots, \mathbf{h}^{(l_k)}, \mathbf{z}\bigr) \in [0, 1]$$

            where \(\hat{p}\) is the predicted probability that \(a\) is correct.<br><br>

            <span class="subsection">3.2 Probe Architectures</span><br><br>

            We systematically evaluate 6 probe architectures, each testing specific hypotheses about how uncertainty is encoded in hidden states. We organize architectures by their primary inductive bias: (1) Linearity (testing if simple models suffice), (2) Information localization (testing if uncertainty is sparse and/or hierarchical), (3) Layer composition (testing if uncertainty is distributed across depths).<br><br>

            <b>Baseline Probes (Testing Linearity)</b><br><br>

            <table class="results">
              <tr>
                <th style="width: 20%">Architecture</th>
                <th style="width: 65%">Description</th>
              </tr>
              <tr>
                <td><b>Linear Probe</b></td>
                <td><b>Hypothesis:</b> Uncertainty can be captured by a single linear dimension within the hidden states.<br><br>
                $$\hat{p} = \sigma(\mathbf{w}^\top \mathbf{h} + b)$$
                A single linear layer maps the hidden state to a confidence score. If the linear representation hypothesis <a href="#ref_14">[14]</a> holds for uncertainty, this simple classifier should suffice. We train with Brier loss rather than cross-entropy, encouraging calibration.</td>
              </tr>
              <tr>
                <td><b>MLP Probe</b></td>
                <td><b>Hypothesis:</b> The model encodes uncertainty in patterns that can't be captured by a purely linear probe.<br><br>
                $$\hat{p} = \sigma\bigl(\mathbf{W}_2 \cdot \text{ReLU}(\mathbf{W}_1 \mathbf{h} + \mathbf{b}_1) + b_2\bigr)$$
                A two-layer MLP with ReLU activations learns nonlinear interactions between hidden-state dimensions, letting us test whether uncertainty is encoded on a nonlinear manifold rather than a single direction.</td>
              </tr>
            </table><br>

            <b>Structural Hypotheses (Testing if uncertainty is reflected in architectural structure)</b><br><br>

            <table class="results">
              <tr>
                <th style="width: 20%">Architecture</th>
                <th style="width: 65%">Description</th>
              </tr>
              <tr>
                <td><b>SparseProbe</b></td>
                <td><b>Hypothesis:</b> Only a small subset of hidden-state dimensions meaningfully encode uncertainty, revealing a sparse internal signal.<br><br>
                $$\hat{p} = \sigma(\mathbf{w}^\top (\mathbf{g} \odot \mathbf{h}) + b), \quad \mathbf{g} = \sigma(\mathbf{V}\mathbf{h})$$
                A differentiable gating mechanism \(\mathbf{g}\) assigns importance weights to each dimension before a linear readout, revealing whether uncertainty is concentrated in a sparse subset of coordinates.</td>
              </tr>
              <tr>
                <td><b>HierarchicalProbe</b></td>
                <td><b>Hypothesis:</b> Uncertainty is built up across layers through hierarchical refinement, with each depth contributing a different part of the signal.<br><br>
                $$\hat{p} = \sigma\bigl(\text{MLP}_{\text{global}}([\mathbf{r}_{\text{fine}}; \mathbf{r}_{\text{mid}}; \mathbf{r}_{\text{sem}}])\bigr)$$
                We process the hidden state at fine, mid, and semantic levels (using chunk MLPs, attention pooling, and a global feedforward head) to test whether uncertainty emerges through hierarchical refinement.</td>
              </tr>
            </table><br>

            <b>Multi-Layer Architectures (Testing if uncertainty is distributed across depths)</b><br><br>

            <table class="results">
              <tr>
                <th style="width: 20%">Architecture</th>
                <th style="width: 65%">Description</th>
              </tr>
              <tr>
                <td><b>Ensemble Probe</b></td>
                <td><b>Hypothesis:</b> Different layers capture complementary pieces of uncertainty, so combining probes across depths yields a stronger signal than any single layer.<br><br>
                $$\hat{p} = \sum_{l} \alpha_l \cdot \hat{p}_l, \quad \boldsymbol{\alpha} = \text{softmax}(\mathbf{a})$$
                We train linear probes on multiple layers and learn softmax combination weights \(\boldsymbol{\alpha}\) to determine whether different depths contribute complementary components of the uncertainty signal.</td>
              </tr>
              <tr>
                <td><b>Fusion Probe</b></td>
                <td><b>Hypothesis:</b> A model's internal uncertainty can diverge from its output confidence, and combining hidden-state features with logit signals can reveal this mismatch.<br><br>
                $$\hat{p} = \sigma\bigl(\text{MLP}([\mathbf{h}_{\text{attn}}; \mathbf{f}_{\text{logit}}])\bigr), \quad \mathbf{f}_{\text{logit}} = [H(\mathbf{z}), \text{margin}(\mathbf{z}), \max(\mathbf{z})]$$
                This probe integrates per-layer MLP encoders with logit-based features (entropy \(H\), margin, max probability) through cross-layer attention to detect cases where internal uncertainty diverges from the model's expressed confidence.</td>
              </tr>
            </table><br>

		    </div>
		    <div class="margin-right-block">
            <b>Parameter counts:</b><br>
            Linear: 4K<br>
            Sparse: 460K<br>
            Hierarchical: 320K<br>
            MLP: 920K<br><br>
            The simplest probe has 230× fewer parameters than the most complex.
		    </div>
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
            <span class="subsection">3.3 Dataset</span><br><br>

            We evaluate on two complementary benchmarks totaling <b>~10,000 examples</b>:<br><br>

            <ul>
              <li><b>MMLU</b> <a href="#ref_15">[15]</a> (Massive Multitask Language Understanding): 57 subjects spanning STEM, humanities, and social sciences with 4-choice questions. We use the full validation set (~1,500 examples) to ensure coverage across all subject categories.</li><br>
              <li><b>MMLU-Pro</b>: An extended version with 10-choice questions and more challenging reasoning problems. We sample ~8,500 examples, providing greater difficulty variance and testing whether uncertainty signals remain extractable when the task is harder and answer distributions are more uniform.</li>
            </ul><br>

            Combining both datasets provides sufficient scale for robust probe training while testing generalization across difficulty levels.<br><br>

		    </div>
		    <div class="margin-right-block" style="transform: translate(0%, -20%);">
            <b>Why two datasets?</b> MMLU provides broad subject coverage, while MMLU-Pro increases difficulty and answer choices. If probes generalize across both, we have stronger evidence that they capture genuine uncertainty rather than dataset-specific artifacts.<br><br>
		    </div>
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
            <span class="subsection">3.4 Training Protocol</span><br><br>

            We use 10-fold cross validation over ~10,000 examples (combining MMLU and MMLU-Pro). In each fold, ~1,000 examples are held out as a test set, while the remaining data are split into ~8,100 training and ~900 validation examples, so that each example is used for training in nine folds and for testing in one fold.<br><br>

            All probes are trained with:
            <ul>
              <li><b>Loss: Brier score</b> \((\hat{p} - y)^2\). Unlike binary cross-entropy, which can be minimized by making extreme predictions, Brier score is a <i>proper scoring rule</i> that is uniquely minimized when predicted probabilities equal true probabilities. This directly optimizes for calibration rather than just discrimination.</li><br>

              <li><b>Optimizer: AdamW</b> (lr = \(10^{-3}\), weight_decay = \(10^{-5}\)). We selected AdamW over SGD for its adaptive learning rates and decoupled weight decay, which provides more stable training on our relatively small probe architectures. Hyperparameters were tuned via grid search over lr \(\in \{10^{-4}, 5 \times 10^{-4}, 10^{-3}, 5 \times 10^{-3}\}\) and weight_decay \(\in \{10^{-6}, 10^{-5}, 10^{-4}\}\).</li><br>

              <li><b>Schedule: Cosine annealing</b> over 100 epochs. Cosine decay provides smooth learning rate reduction without requiring manual milestone selection. We found this outperformed step decay and linear warmup schedules in preliminary experiments, likely because the gradual decay allows fine-tuning of the uncertainty direction in later epochs.</li><br>

              <li><b>Selection: Best validation Brier score</b>. We checkpoint based on validation Brier rather than loss plateau to directly select for calibration quality. This prevents selecting overfit models that achieve low training loss but poor generalization.</li>
            </ul><br>

		    </div>
		    <div class="margin-right-block" style="transform: translate(0%, -20%);">
            <b>Why Brier loss?</b> Unlike cross-entropy, Brier score penalizes overconfidence, directly optimizing for calibration rather than just discrimination.<br><br>
            <b>Why 10-fold CV?</b> With ~10K examples, 10-fold provides 1,000 test samples per fold, enough for reliable metric estimates while maximizing training data.<br><br>
		    </div>
		</div>

		<!-- EXPERIMENTS -->
		<div class="content-margin-container" id="results">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
            <h1>4. Experiments</h1>

            We use a <b>two-stage experimental design</b> to systematically test our hypotheses. <b>Experiment 1</b> asks: given access to a single middle layer, which probe architecture best extracts uncertainty? We compare Linear, MLP, Hierarchical, and Sparse probes. <b>Experiment 2</b> asks: given that linear probes work well, what information should we provide as input? We compare single-layer, multi-layer ensemble, and fusion (hidden states + logits) configurations.<br><br>

            <span class="subsection">4.1 Experimental Setup</span><br><br>

            <b>Model:</b> We use <b>Qwen2.5-7B</b> with 8-bit quantization via bitsandbytes <a href="#ref_16">[16]</a>. We selected Qwen after preliminary tests comparing Qwen2.5-7B, Mistral-7B, and Llama-3.1-8B showed that probes trained on Mistral and Llama produced ~0.10 higher Brier scores, indicating worse calibration.<br><br>

            <b>Hidden State Extraction:</b> We extract hidden states at \(k\) evenly-spaced quantile layer positions. After preliminary testing with \(k \in \{2, 4, 6, 8\}\), we selected \(k = 4\), yielding layers [7, 14, 21, 27] for Qwen2.5-7B's 28-layer architecture. These positions capture early (syntactic), middle (semantic), and late (task-specific) processing stages. We use the <b>last token</b> hidden state, which aggregates context via attention and represents the model's "decision point."<br><br>

            <b>Labels:</b> We use <b>logit-based prediction</b>: examining logits for each answer choice at the last position and selecting the highest. Each example receives a binary correctness label (1 if correct, 0 otherwise).<br><br>

            <span class="subsection">4.2 Evaluation Metrics</span><br><br>

            We evaluate probes on three complementary metrics:

            <ul>
              <li><b>AUROC</b>: Measures discrimination, i.e., the probability that a correct answer receives higher confidence than an incorrect one. Range [0, 1], where 0.5 is random. We use AUROC because it is threshold-independent and directly evaluates the probe's ability to rank answers, which is essential for selective prediction (abstaining on low-confidence outputs).</li>

              <li><b>Brier Score</b> <a href="#ref_13">[13]</a>: A proper scoring rule measuring calibration and discrimination jointly: \(\frac{1}{N} \sum_{i=1}^{N} (p_i - y_i)^2\). Lower is better (0 is perfect). We use Brier score because, unlike cross-entropy, it does not reward extreme predictions, making it ideal for evaluating well-calibrated probability estimates rather than just classification accuracy.</li>

              <li><b>ECE</b> <a href="#ref_12">[12]</a>: Expected Calibration Error measures pure calibration by binning predictions: \(\sum_{b=1}^{B} \frac{n_b}{N} | \text{acc}(b) - \text{conf}(b) |\). We use \(B = 10\) bins. Lower is better. We include ECE because it isolates calibration from discrimination, directly measuring whether predicted probabilities match empirical frequencies. This is critical for deployment where users must trust confidence scores.</li>
            </ul><br>

            <span class="subsection">4.3 Baseline: Do Probes Work?</span><br><br>

            Before comparing architectures, we first establish that hidden states contain extractable uncertainty signals. We compare a simple linear probe against a random baseline.<br><br>

            <table class="results">
              <tr>
                <th>Method</th>
                <th>AUROC &uarr;</th>
                <th>Brier &darr;</th>
                <th>ECE &darr;</th>
              </tr>
              <tr>
                <td>Random baseline</td>
                <td>0.5016</td>
                <td>0.3334</td>
                <td>0.2873</td>
              </tr>
              <tr>
                <td><b>Linear probe (layer 14)</b></td>
                <td><b>0.829</b></td>
                <td><b>0.143</b></td>
                <td><b>0.033</b></td>
              </tr>
            </table>
            <i>Table 1: Baseline comparison showing that probes extract meaningful uncertainty signals from hidden states.</i><br><br>

            <b>Conclusion:</b> Hidden states contain uncertainty signals. The linear probe achieves AUROC of 0.829, well above the random baseline of 0.5016, showing that uncertainty information is encoded in the model's internal representations and can be extracted with simple probes.<br><br>

            <span class="subsection">4.4 Experiment 1: Which Architecture is Best?</span><br><br>

            <b>Research Question:</b> Given access to a single middle layer (layer 14), which probe architecture best extracts uncertainty?<br><br>

            We tested four probe architectures: Linear (4K params), MLP (920K params), Hierarchical (320K params), and Sparse (460K params).<br><br>

            <b>Results:</b><br><br>

            <table class="results">
              <tr>
                <th>Architecture</th>
                <th>AUROC &uarr;</th>
                <th>Brier &darr;</th>
                <th>ECE &darr;</th>
                <th>Parameters</th>
              </tr>
              <tr>
                <td><b>Linear</b></td>
                <td><b>0.829 ± 0.014</b></td>
                <td><b>0.143 ± 0.009</b></td>
                <td><b>0.033 ± 0.007</b></td>
                <td>4K</td>
              </tr>
              <tr>
                <td>MLP</td>
                <td>0.702 ± 0.031</td>
                <td>0.243 ± 0.018</td>
                <td>0.223 ± 0.034</td>
                <td>920K</td>
              </tr>
              <tr>
                <td>Hierarchical</td>
                <td>0.818 ± 0.016</td>
                <td>0.143 ± 0.010</td>
                <td>0.039 ± 0.009</td>
                <td>320K</td>
              </tr>
              <tr>
                <td>Sparse</td>
                <td>0.833 ± 0.015</td>
                <td>0.142 ± 0.008</td>
                <td>0.041 ± 0.010</td>
                <td>460K</td>
              </tr>
            </table>
            <i>Table 2: Architecture comparison on layer 14. Values are mean ± std across 10-fold CV. Linear probe achieves statistically equivalent performance to complex alternatives despite 80-230× fewer parameters.</i><br><br>

            <b>Finding:</b> The <b>linear probe matches complex alternatives</b> despite being 80-230× smaller. The linear probe achieves AUROC of 0.829 and Brier score of 0.143, significantly outperforming the MLP probe (AUROC 0.702, Brier 0.243) while achieving statistically equivalent performance to Hierarchical (AUROC 0.818, Brier 0.143) and Sparse (AUROC 0.833, Brier 0.142).<br><br>

            <b>Why does linear win with a single layer?</b> When restricted to a single layer, the available uncertainty signal is limited. Complex architectures (MLP, Hierarchical, Sparse) have sufficient capacity to memorize training examples or fit spurious correlations in the high-dimensional hidden space, but these patterns fail to generalize. The capacity-constrained linear probe is forced to find the single most robust direction corresponding to genuine uncertainty, avoiding overfitting. The result aligns with the linear representation hypothesis <a href="#ref_14">[14]</a>: when signal is sparse, simplicity wins.<br><br>

            <b>Layer Analysis:</b><br><br>

            <img src="./images/layer_confidence.png" width=700px/><br>
            <i>Figure 1: Linear probe AUROC across transformer layers. Middle layers (50% depth) achieve best single-layer performance.</i><br><br>

            To understand where uncertainty is encoded, we analyze linear probe performance across layers. Our results confirm that <b>middle layers dominate</b>, consistent with prior work <a href="#ref_5">[5]</a>. Layer 14 (50% depth) achieves the best single-layer performance. Early layers (7, 25% depth) encode primarily syntactic information, while final layers (27, 96% depth) specialize for output formatting. Neither captures uncertainty as strongly as middle layers where semantic processing peaks.<br><br>

            <b>Calibration Analysis:</b><br><br>

            <img src="./images/reliability_diags.png" width=650px/><br>
            <i>Figure 2: Reliability diagrams comparing probe calibration. The diagonal represents perfect calibration; bars show actual accuracy per confidence bin.</i><br><br>

            Reliability diagrams visualize calibration by plotting average accuracy (y-axis) vs. predicted confidence (x-axis) in each bin. Perfect calibration lies on the diagonal: when the model predicts 70% confidence, it should be correct 70% of the time. The blue bars show actual accuracy within each confidence bin. Gaps between the bars and the dashed diagonal line indicate miscalibration. The Linear probe (ECE=0.033) and Hierarchical probe (ECE=0.039) show excellent calibration, with accuracy closely tracking predicted confidence. The Sparse probe (ECE=0.041) also demonstrates good calibration with a slight decrease in calibration at higher confidence ranges. In contrast, the MLP probe (ECE=0.223) shows significant miscalibration, with accuracy deviating substantially from confidence across bins, particularly in lower confidence ranges.<br><br>

            <span class="subsection">4.5 Experiment 2: What Information Should We Use?</span><br><br>

            <b>Research Question:</b> Given that linear probes work best, what information should we provide as input?<br><br>

            We tested three configurations of the linear probe architecture:
            <ol>
              <li><b>Single layer</b> (layer 14 only) - baseline</li>
              <li><b>Multi-layer ensemble</b> (layers 7, 14, 21, 27) - testing layer complementarity</li>
              <li><b>Fusion</b> (layers 7, 14, 21, 27 + output logits) - testing miscalibration detection</li>
            </ol>

            The multi-layer ensemble probe uses four parallel linear probes (one per quartile layer) with learned softmax-normalized combination weights. The fusion probe combines per-layer linear probes with logit feature extraction (entropy, margin, max probability) to detect cases where internal uncertainty and expressed confidence diverge.<br><br>

            <b>Results:</b><br><br>

            <table class="results">
              <tr>
                <th>Configuration</th>
                <th>Input</th>
                <th>AUROC &uarr;</th>
                <th>Brier &darr;</th>
                <th>ECE &darr;</th>
              </tr>
              <tr>
                <td>Single Layer</td>
                <td>Layer 14</td>
                <td>0.829 ± 0.014</td>
                <td>0.143 ± 0.009</td>
                <td>0.033 ± 0.007</td>
              </tr>
              <tr>
                <td>Ensemble</td>
                <td>4 layers</td>
                <td>0.862 ± 0.012</td>
                <td>0.121 ± 0.008</td>
                <td>0.028 ± 0.006</td>
              </tr>
              <tr>
                <td><b>Fusion</b></td>
                <td><b>4 layers + logits</b></td>
                <td><b>0.904 ± 0.009</b></td>
                <td><b>0.098 ± 0.007</b></td>
                <td><b>0.021 ± 0.005</b></td>
              </tr>
            </table>
            <i>Table 3: Information source comparison. Values are mean ± std across 10-fold CV.</i><br><br>

            <b>Finding:</b> Adding more information sources consistently improves performance. The single-layer baseline (AUROC 0.829) improves to 0.862 with the multi-layer ensemble, and reaches 0.904 with fusion. Brier score drops from 0.143 → 0.121 → 0.098, indicating progressively better calibration.<br><br>

            Different layers encode complementary aspects of uncertainty. Early layers capture syntactic confidence, middle layers encode semantic uncertainty, and later layers reflect task-specific processing. By combining signals across layers, the ensemble aggregates these complementary views. The fusion probe further improves by incorporating output logits, which capture the model's "expressed" confidence, enabling detection of cases where internal uncertainty diverges from output confidence. After testing different quantile sizes, we found that four layers offered the best performance-compute tradeoff.<br><br>

            <b>Layer Weight Analysis:</b><br><br>

            <img src="./images/fusionlayerweights.png" width=700px/><br>
            <i>Figure 3: Learned layer weights from Fusion Probe showing near-uniform contribution across transformer layers.</i><br><br>

            The fusion probe learns softmax-normalized weights over the four quartile layers, revealing which layers contribute most to uncertainty prediction.<br><br>

            <b>Results:</b> The learned weights are roughly uniform across layers, indicating that each layer contributes comparable uncertainty signal. Layer 14 (50% depth) receives a slightly higher weight, but the difference is modest. Uncertainty information seems distributed throughout the network rather than concentrated in specific layers, which explains why combining multiple layers improves performance over any single layer.<br><br>

            The logit features (entropy, margin, max probability) capture the model's expressed confidence, while hidden states capture internal uncertainty. When these signals diverge (the model appears confident but is internally uncertain), the fusion probe detects this miscalibration. This gets at a key limitation of hidden-state-only approaches: they cannot identify when internal and expressed confidence disagree.<br><br>

		    </div>
		    <div class="margin-right-block" style="transform: translate(0%, -30%);">
            <b>Exp 1 takeaway:</b> Linear probes achieve best calibration (ECE 0.033) despite 230× fewer parameters than MLP.<br><br>
            <b>Exp 2 takeaway:</b> Fusion Probe achieves 19% better Brier (0.098 vs 0.121) than hidden-states-only, statistically significant at p < 0.01.<br><br>
            <b>Efficiency:</b> Single forward pass vs. 5-10× for semantic entropy methods.
		    </div>
		</div>

		<!-- DISCUSSION -->
		<div class="content-margin-container" id="analysis">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>5. Discussion</h1>

            <span class="subsection">5.1 Linear vs. Non-linear</span><br><br>

            The Linear Probe substantially outperforms the MLP Probe (Brier 0.143 vs 0.243, ECE 0.033 vs 0.223), with higher-capacity models achieving worse calibration. The finding supports the linear representation hypothesis <a href="#ref_14">[14]</a>: uncertainty is encoded as a geometric direction in hidden space, not in complex non-linear relationships. The MLP Probe overfits to spurious correlations, while the capacity-limited Linear Probe finds a robust uncertainty direction.<br><br>

            This has three implications: (1) uncertainty is explicitly represented geometrically, making it accessible to the model's own linear readout mechanism for interpretability and steering, (2) simple probes are preferable, not just sufficient, and (3) Brier loss successfully prevents the overconfident predictions that higher capacity enables.<br><br>

            <span class="subsection">5.2 The Layer Question</span><br><br>

            The fusion probe's learned weights are approximately uniform across layers, with middle layers receiving only a slight advantage. Uncertainty signal appears to be <b>distributed throughout the network</b> rather than concentrated in specific layers. Each layer captures a different aspect of uncertainty: early layers encode syntactic confidence, middle layers encode semantic uncertainty, and later layers reflect task-specific processing. The near-uniform weighting suggests these signals are complementary rather than redundant.<br><br>

            Practical implication: multi-layer probes outperform single-layer approaches because they aggregate complementary uncertainty signals from across the network. While middle layers provide the best single-layer performance, the gains from combining layers indicate that no single layer captures the full uncertainty picture.<br><br>

            <span class="subsection">5.3 Internal vs. Expressed</span><br><br>

            Fusion Probe detects when internal and expressed confidence diverge. For example, hidden states may encode uncertainty while the final layer produces confident logits. Our ablation comparing Fusion vs. Ensemble shows that combining hidden states with logits yields substantial improvements (Brier score 0.121 → 0.098, a 19% reduction). The result confirms that <b>miscalibration exists and is detectable</b>: logit features (entropy, margin, max probability) capture expressed confidence, while hidden states capture internal uncertainty. When these disagree, Fusion Probe identifies cases where the model appears confident but is internally uncertain.<br><br>

            These results support our hypothesis: internal and expressed confidence can diverge, and explicitly modeling both sources improves confidence prediction. The consistent gains indicate that logits provide complementary signal beyond hidden states alone, specifically information about the model's "expressed" confidence that hidden states do not fully capture.<br><br>

            <span class="subsection">5.4 Limitations</span><br><br>

            <ol>
              <li><b>Model-specific findings.</b> Our evaluation focuses on Qwen2.5-7B, and we observed that Llama-3.1-8B produced less calibrated probes (Brier scores ~0.10 higher). Uncertainty encoding may vary across model families and scales. <i>Mitigation:</i> Future work could develop model-agnostic probes by training on hidden states from multiple models, or use transfer learning to adapt probes across architectures.</li>
              <li><b>Multiple-choice format.</b> MMLU's constrained 4-choice format may not reflect uncertainty patterns in open-ended generation. <i>Mitigation:</i> Combining probing with semantic entropy <a href="#ref_6">[6]</a> by using probes for efficient screening, then semantic clustering for high-uncertainty cases.</li>
              <li><b>Single task type.</b> While MMLU spans 57 subjects, all questions are factual recall. Mathematical reasoning or open-domain QA may exhibit different uncertainty structures. <i>Mitigation:</i> Multi-task probe training across diverse benchmarks.</li>
              <li><b>Static extraction.</b> We extract from a single forward pass at the final token. Uncertainty may evolve during chain-of-thought reasoning. <i>Mitigation:</i> Temporal probing, tracking hidden state evolution across reasoning steps.</li>
            </ol><br>

            <span class="subsection">5.5 Practical Implications</span><br><br>

            Reliable uncertainty quantification enables several deployment patterns:<br><br>

            <ol>
              <li><b>Selective prediction.</b> Set a confidence threshold below which the model abstains. In high-stakes domains (medical, legal, financial), an uncertain "I don't know" is safer than a confident wrong answer. Our probe's ECE of 0.021 enables precise thresholds.</li>
              <li><b>Uncertainty-triggered retrieval.</b> Integrate probes with RAG: when uncertainty exceeds a threshold, retrieve relevant documents before responding. The system "knows what it doesn't know" and seeks external information only when needed.</li>
              <li><b>Tiered human escalation.</b> Route queries by confidence: high-confidence responses go directly to users, low-confidence queries escalate to human experts. This focuses expert attention where it matters most.</li>
              <li><b>User-facing confidence scores.</b> Display calibrated confidence alongside outputs, enabling users to decide when to trust or verify responses. Well-calibrated scores (ECE < 0.03) provide meaningful probability estimates.</li>
              <li><b>Training signal for RLHF.</b> Use probe-detected uncertainty as a reward signal: penalize high confidence on incorrect answers and reward appropriate uncertainty expression.</li>
            </ol><br>

            The single-pass efficiency of probing (vs. 5-10 passes for semantic entropy) makes these patterns practical for production deployment at scale.<br><br>

            <span class="subsection">5.6 Future Directions</span><br><br>

            Several extensions could build on this work. <b>Fine-grained layer analysis</b> (probing all 28 layers) would reveal how uncertainty representation evolves through the network. <b>Uncertainty steering</b> via activation engineering could enable controlled calibration by adding or subtracting the learned "uncertainty vector." <b>Cross-model transfer</b> experiments would test whether uncertainty encoding is universal or model-specific. <b>Uncertainty-aware decoding</b> could adapt sampling temperature based on probe outputs, exploring alternatives when uncertain and committing when confident. Finally, extending probes to <b>vision-language models</b> and <b>chain-of-thought reasoning</b> would address hallucination detection and temporal uncertainty dynamics in more complex generation settings.
		    </div>
		    <div class="margin-right-block">
            <b>Scope caveat:</b> Results are for Qwen2.5-7B on MMLU multiple-choice. Cross-model transfer and open-ended generation remain open questions for future work.
		    </div>
		</div>

		<!-- CONCLUSION -->
		<div class="content-margin-container" id="conclusion">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>6. Conclusion</h1>

            We set out to test whether a model's internal representation of uncertainty diverges from its expressed confidence, and whether this divergence can be detected to improve calibration. Our experiments on ~10,000 MMLU examples provide three key findings:<br><br>

            <ol>
              <li><b>Internal and expressed confidence can diverge, and detecting this improves calibration.</b> Our Fusion Probe, which combines hidden states with output logits, achieves 19% better Brier score than hidden-state-only approaches (0.098 vs 0.121, \(p < 0.01\)). Models sometimes "know" they are uncertain internally while expressing confidence externally, and fusing both signals reveals this miscalibration.</li><br>
              <li><b>Uncertainty is linearly encoded and distributed across layers.</b> Linear probes achieve statistically equivalent performance to complex architectures (Hierarchical, Sparse) despite 80-230× fewer parameters, while the higher-capacity MLP significantly underperforms. Learned layer weights are near-uniform, indicating uncertainty signal is distributed rather than localized.</li><br>
              <li><b>Lightweight probes achieve strong calibration.</b> Our best probe achieves AUROC 0.904 and ECE 0.021 in a single forward pass, competitive with multi-sample methods like semantic entropy but 5-10× more efficient.</li>
            </ol><br>

            These findings support our central hypothesis: models do "know what they don't know," and this knowledge can be extracted by fusing internal uncertainty with expressed confidence. In practice, this means a compute-performance tradeoff: single-layer linear probes offer efficient baseline uncertainty quantification, while multi-layer fusion probes achieve superior calibration for high-stakes applications. Both approaches enable selective prediction, uncertainty-triggered retrieval, and human escalation, making LLMs safer for real-world deployment. Our implementation is available on <a href="https://github.com/joshcliu/deep-learning">Github</a>.
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<!-- REFERENCES -->
		<div class="content-margin-container" id="citations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<div class='citation' id="references" style="height:auto"><br>
							<span style="font-size:16px">References:</span><br><br>
							<a id="ref_1"></a>[1] Ji, Z., Lee, N., Frieske, R., et al. (2023). <a href="https://dl.acm.org/doi/10.1145/3571730">Survey of Hallucination in Natural Language Generation</a>. <i>ACM Computing Surveys</i>, 55(12), 1-38.<br><br>
							<a id="ref_2"></a>[2] Zhang, Y., Li, Y., Cui, L., et al. (2023). <a href="https://arxiv.org/abs/2311.05232">Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models</a>. <i>arXiv:2311.05232</i>.<br><br>
							<a id="ref_3"></a>[3] Kadavath, S., Conerly, T., Askell, A., et al. (2022). <a href="https://arxiv.org/abs/2207.05221">Language Models (Mostly) Know What They Know</a>. <i>arXiv:2207.05221</i>.<br><br>
							<a id="ref_4"></a>[4] Burns, C., Ye, H., Klein, D., & Steinhardt, J. (2023). <a href="https://openreview.net/forum?id=ETKGuby0hcs">Discovering Latent Knowledge in Language Models Without Supervision</a>. <i>ICLR 2023</i>.<br><br>
							<a id="ref_5"></a>[5] Gurnee, W. & Tegmark, M. (2024). <a href="https://arxiv.org/abs/2502.02013">Layer by Layer: Uncovering Hidden Representations in Language Models</a>. <i>arXiv:2502.02013</i>.<br><br>
							<a id="ref_6"></a>[6] Hewitt, J. & Manning, C. (2019). <a href="https://aclanthology.org/N19-1419/">A Structural Probe for Finding Syntax in Word Representations</a>. <i>NAACL 2019</i>.<br><br>
							<a id="ref_7"></a>[7] Azaria, A. & Mitchell, T. (2023). <a href="https://arxiv.org/abs/2304.13734">The Internal State of an LLM Knows When It's Lying</a>. <i>EMNLP 2023 Findings</i>.<br><br>
							<a id="ref_8"></a>[8] Jiang, Z., Araki, J., Ding, H., & Neubig, G. (2021). <a href="https://aclanthology.org/2021.tacl-1.57/">How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering</a>. <i>TACL</i>, 9, 962-977.<br><br>
							<a id="ref_9"></a>[9] Farquhar, S., Kossen, J., Kuhn, L., & Gal, Y. (2024). <a href="https://www.nature.com/articles/s41586-024-07421-0">Detecting hallucinations in large language models using semantic entropy</a>. <i>Nature</i>, 630, 625-630.<br><br>
							<a id="ref_10"></a>[10] Xiong, M., Hu, Z., Lu, X., et al. (2024). <a href="https://openreview.net/forum?id=gjeQKFxFpZ">Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs</a>. <i>ICLR 2024</i>.<br><br>
							<a id="ref_11"></a>[11] Gekhman, Z., Yona, G., Aharoni, R., et al. (2025). <a href="https://belinkov.com/assets/pdf/iclr2025-know.pdf">Does the Model Know It Knows? Probing Knowledge in Language Models</a>. <i>ICLR 2025</i>.<br><br>
							<a id="ref_12"></a>[12] Guo, C., Pleiss, G., Sun, Y., & Weinberger, K.Q. (2017). <a href="https://proceedings.mlr.press/v70/guo17a.html">On Calibration of Modern Neural Networks</a>. <i>ICML 2017</i>.<br><br>
							<a id="ref_13"></a>[13] Brier, G.W. (1950). <a href="https://journals.ametsoc.org/view/journals/mwre/78/1/1520-0493_1950_078_0001_vofeit_2_0_co_2.xml">Verification of Forecasts Expressed in Terms of Probability</a>. <i>Monthly Weather Review</i>, 78(1), 1-3.<br><br>
							<a id="ref_14"></a>[14] Park, K., Choe, Y.J., & Veitch, V. (2024). <a href="https://proceedings.mlr.press/v235/park24c.html">The Linear Representation Hypothesis and the Geometry of Large Language Models</a>. <i>ICML 2024</i>.<br><br>
							<a id="ref_15"></a>[15] Hendrycks, D., Burns, C., Basart, S., et al. (2021). <a href="https://openreview.net/forum?id=d7KBjmI3GmQ">Measuring Massive Multitask Language Understanding</a>. <i>ICLR 2021</i>.<br><br>
							<a id="ref_16"></a>[16] Dettmers, T., Lewis, M., Belkada, Y., & Zettlemoyer, L. (2022). <a href="https://arxiv.org/abs/2208.07339">LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale</a>. <i>NeurIPS 2022</i>.<br><br>
						</div>
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

	</body>

</html>
