<html>
<head>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

<!-- MathJax for consistent math rendering across browsers -->
<script>
MathJax = {
  tex: {
    inlineMath: [['\\(', '\\)']],
    displayMath: [['$$', '$$']],
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>

<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
	body {
		background-color: #f5f9ff;
	}

	.content-margin-container {
		display: flex;
		width: 100%;
		justify-content: left;
		align-items: center;
	}
	.main-content-block {
		width: 70%;
    max-width: 1100px;
		background-color: #fff;
		border-left: 1px solid #DDD;
		border-right: 1px solid #DDD;
		padding: 8px 8px 8px 8px;
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
	}
	.margin-left-block {
			font-size: 14px;
			width: 15%;
			max-width: 130px;
			position: relative;
			margin-left: 10px;
			text-align: left;
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
			padding: 5px;
	}
	.margin-right-block {
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
			font-size: 14px;
			width: 25%;
			max-width: 256px;
			position: relative;
			text-align: left;
			padding: 10px;
	}

	img {
			max-width: 100%;
			height: auto;
			display: block;
			margin: auto;
	}

	a:link,a:visited
	{
		color: #0e7862;
		text-decoration: none;
	}
	a:hover {
		color: #24b597;
	}

	h1 {
		font-size: 26px;
		margin-top: 8px;
		margin-bottom: 14px;
	}

	.subsection {
		font-size: 18px;
		font-weight: bold;
		margin-top: 6px;
		display: inline-block;
	}

	table.header {
    font-weight: 300;
    font-size: 17px;
    flex-grow: 1;
		width: 70%;
    max-width: calc(100% - 290px);
	}
	table td, table td * {
	    vertical-align: middle;
	    position: relative;
	}

	hr {
    height: 1px;
    border: none;
    background-color: #DDD;
  }

	div.hypothesis {
		width: 80%;
		background-color: #EEE;
		border: 1px solid black;
		border-radius: 10px;
		font-family: Courier;
		font-size: 16px;
		text-align: center;
		margin: auto;
		padding: 16px 16px 16px 16px;
	}

	div.citation {
    font-size: 0.8em;
    background-color:#fff;
    padding: 10px;
		height: 200px;
  }

	table.results {
		border-collapse: collapse;
		width: 100%;
		margin: 10px 0;
	}
	table.results th, table.results td {
		border: 1px solid #DDD;
		padding: 8px;
		text-align: left;
	}
	table.results th {
		background-color: #f5f9ff;
	}

	code {
		background-color: #f5f5f5;
		padding: 2px 6px;
		border-radius: 3px;
		font-family: 'Courier New', Courier, monospace;
		font-size: 14px;
	}

	pre {
		background-color: #f5f5f5;
		padding: 12px;
		border-radius: 5px;
		overflow-x: auto;
		font-family: 'Courier New', Courier, monospace;
		font-size: 13px;
	}

</style>

	  <title>Probing through LLMs: Extracting Confidence from Hidden States</title>
      <meta property="og:title" content="Probing through LLMs: Extracting Confidence from Hidden States" />
			<meta charset="UTF-8">
  </head>

  <body>

		<!-- HEADER -->
		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<table class="header" align=left>
								<tr>
									<td colspan=4>
										<span style="font-size: 32px; font-family: 'Courier New', Courier, monospace;">Probing through LLMs</span>
									</td>
								</tr>
								<tr>
									<td colspan=4>
										<span style="font-size: 20px; color: #666;">Extracting Confidence from Hidden States</span>
									</td>
								</tr>
								<tr>
										<td align=left>
												<span style="font-size:17px"><a href="#">Joshua Liu</a></span>
										</td>
										<td align=left>
												<span style="font-size:17px"><a href="#">Carol Li</a></span>
										</td>
										<td align=left>
												<span style="font-size:17px"><a href="#">Maureen Zhang</a></span>
										</td>
								<tr>
									<td colspan=4 align=left><span style="font-size:18px">Final project for 6.7960, MIT</span></td>
								</tr>
						</table>
					</div>
					<div class="margin-right-block">
					</div>
		</div>

    <!-- INTRODUCTION -->
    <div class="content-margin-container" id="intro">
				<div class="margin-left-block">
          <div style="position:fixed; max-width:inherit; top:max(20%,120px)">
              <b style="font-size:16px">Outline</b><br><br>
              <a href="#intro">Introduction</a><br><br>
              <a href="#background">Background</a><br><br>
              <a href="#methodology">Methodology</a><br><br>
              <a href="#results">Experiments</a><br><br>
              <a href="#analysis">Discussion</a><br><br>
              <a href="#conclusion">Conclusion</a><br><br>
          </div>
				</div>
		    <div class="main-content-block">
						<h1>1. Introduction</h1>

            Large language models (LLMs) are known to present incorrect answers with high confidence, which is particularly problematic in high-stakes domains like medicine, law, and finance <a href="#ref_1">[1]</a><a href="#ref_2">[2]</a>. A significant obstacle in decreasing the rate of such mistakes is the lack of a reliable signal regarding when an LLM is wrong. Reliable uncertainty estimates would allow models to return answers only when fairly certain, optimize when to look up information, and know when to defer to a human user, making LLMs safer for real-world scenarios. We investigate whether models know what they don't know, and whether that knowledge can be gleaned from their internal representations.<br><br>

            <div class="hypothesis">
              <b>Central Hypothesis:</b> A model's internal representation of uncertainty can differ from the confidence it expresses in its outputs. By detecting this divergence (fusing hidden states with output logits), we can extract calibration signals that neither source provides alone.
            </div><br>

            Evidence suggests such internal signals exist: Kadavath et al. <a href="#ref_3">[3]</a> showed LLMs can predict their own correctness well above chance, Burns et al. <a href="#ref_4">[4]</a> found that hidden states encode truthfulness even without supervised labels, and Gurnee & Tegmark <a href="#ref_5">[5]</a> demonstrated that middle transformer layers encode richer semantic information than final layers. We use probing, a technique that trains lightweight classifiers on frozen hidden states, as introduced by Hewitt & Manning <a href="#ref_6">[6]</a> and later used for LLM error detection by Azaria & Mitchell <a href="#ref_7">[7]</a>. While prior work has probed hidden states <i>or</i> analyzed output probabilities in isolation, we ask whether combining both sources reveals miscalibration: cases where internal uncertainty and expressed confidence disagree. Specifically: (1) Is uncertainty linearly encoded or does it require non-linear extraction? (2) Is uncertainty concentrated in specific layers or distributed across depths? and (3) Can fusing hidden states with output logits detect when internal and expressed confidence diverge?
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>


		<!-- BACKGROUND AND RELATED WORK -->
		<div class="content-margin-container" id="background">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
					<h1>2. Background and Related Work</h1>

          Existing uncertainty quantification methods for LLMs fall into three categories. <b>Token probability methods</b> use the model's softmax scores as confidence, but Jiang et al. <a href="#ref_8">[8]</a> showed these are often poorly calibrated. <b>Consistency-based methods</b> like semantic entropy <a href="#ref_9">[9]</a> generate multiple responses and measure agreement, allowing hallucination detection but requiring 5-10 forward passes per query. <b>Verbalized confidence</b> asks models to self-report uncertainty, but Xiong et al. <a href="#ref_10">[10]</a> found they tend to be systematically overconfident. These limitations motivate looking inside the model's internal representations.<br><br>

        <span class="subsection">2.1 Probing Internal Representations</span>
          <p>
          Our approach fits into a fourth paradigm: probing internal representations. The core idea is that if a model encodes something like “confidence” 
          in its internal representations, then a lightweight classifier trained on its hidden states should be able to extract it. This aims to
          keep the single-forward-pass efficiency of logit-based methods while tapping into
          potentially richer signals than output probabilities or verbalized confidence.
          </p>
          
          <p>
            Probing itself has a long history in NLP. Hewitt and Manning <a href="#ref_6">[6]</a> showed that
            syntactic structure can be linearly recovered from BERT’s hidden space.
            For uncertainty, Kadavath et al. <a href="#ref_3">[3]</a> found that large LLMs can meaningfully
            judge their own answers, and Burns et al. <a href="#ref_4">[4]</a> proposed Contrast-Consistent
            Search (CCS), which recovers a truthfulness signal from hidden states without
            labels, though it relies on contrastive statement pairs and does not directly
            handle open-ended generation.
          </p>

          
          <p>
          More recent work asks <i>where</i> this kind of information lives. Azaria and Mitchell
          <a href="#ref_7">[7]</a> trained classifiers on hidden states and found strong error signals,
          especially in intermediate layers. Gekhman et al. <a href="#ref_11">[11]</a> showed that models can
          encode the right answer internally yet still output something wrong, implying that internal
          representations may be more informative than the generated text itself. This motivates our
          focus on probing hidden states to study how uncertainty is represented inside the model.
          </p>
          
		<span class="subsection">2.2 Calibration</span>
          <p>
          A well-calibrated model's confidence matches its accuracy: when it predicts 70% confidence, it should be correct 70% of the time. Calibration differs from discrimination: a model may correctly rank answers while still assigning highly inaccurate probabilities. Guo et al. <a href="#ref_12">[12]</a> showed that modern neural networks are systematically overconfident and introduced Expected Calibration Error (ECE), which bins predictions by confidence and measures the gap between predicted confidence and observed accuracy.
          </p>

          <p>
          Post-hoc calibration methods attempt to fix miscalibration after training. <b>Temperature scaling</b> divides logits by a learned temperature \(T > 1\) to soften overconfident predictions, while <b>Platt scaling</b> fits a logistic regression on held-out data. However, these methods assume the model's ranking is correct and only adjust the probability scale; they cannot fix cases where the model is confidently wrong about ordering. For LLMs, calibration is particularly challenging because confidence can vary across domains, question types, and even prompt phrasings.
          </p>

		<span class="subsection">2.3 Research Gaps We Address</span>
          <p>
          Despite substantial progress, key questions remain about how LLMs represent and express uncertainty. We address these gaps through systematic experimentation and a novel fusion architecture.
          </p>

          <ol>
            <li>
              <b>Internal vs. expressed confidence divergence.</b><br>
              Existing studies focus on either hidden-state signals
              <a href="#ref_7">[7]</a> or output probabilities <a href="#ref_8">[8]</a> in isolation, leaving
              unexplored whether their <i>disagreement</i> signals miscalibration.<br>
              <i>Our contribution:</i> We introduce a fusion probe that combines hidden states
              from multiple layers with output logit features (entropy, margin, max probability).
              This architecture detects cases where internal uncertainty and expressed confidence
              diverge, achieving 19% better calibration than hidden-state-only approaches (Brier: 0.098 vs 0.121).
            </li>
            <br>
            <li>
              <b>Layer localization of uncertainty.</b><br>
              Prior work typically probes single layers <a href="#ref_7">[7]</a>, leaving open
              whether uncertainty is concentrated in specific depths or distributed across the network.<br>
              <i>Our contribution:</i> We confirm that middle layers (50% depth) achieve best single-layer
              performance, but develop an Ensemble Probe with learned layer weights showing that
              <b>combining layers outperforms any single layer</b>. The near-uniform weights indicate
              each layer contributes complementary uncertainty signal.
            </li>
            <br>
            <li>
              <b>Linear vs. non-linear encoding.</b><br>
              Most probing work assumes linear classifiers suffice
              <a href="#ref_6">[6]</a><a href="#ref_7">[7]</a>, but this has not been rigorously tested
              against non-linear alternatives for uncertainty.<br>
              <i>Our contribution:</i> We introduce novel Hierarchical and Sparse probe architectures that
              significantly outperform standard MLP baselines (Brier: 0.143 vs 0.243). While linear probes
              achieve statistically equivalent performance to our architectures despite 80-230× fewer parameters,
              this finding itself is notable: it validates the linear representation hypothesis and shows that
              the MLP's poor performance stems from overfitting, not from non-linearity being unnecessary.
            </li>
          </ol>

		</div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<!-- METHODOLOGY -->
		<div class="content-margin-container" id="methodology">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
            <h1>3. Methodology</h1>

            <span class="subsection">3.1 Problem Formulation</span><br><br>

            Given an LLM \(M\), a question \(q\), and the model's generated answer \(a\), we aim to predict whether \(a\) is correct. Formally, let \(\mathbf{h}^{(l)} \in \mathbb{R}^d\) denote the hidden state at layer \(l\), and let \(\mathbf{z} \in \mathbb{R}^K\) denote the output logits for \(K\) answer choices. We train a probe \(f_\theta\) to predict:<br><br>

            $$\hat{p} = f_\theta\bigl(\mathbf{h}^{(l_1)}, \ldots, \mathbf{h}^{(l_k)}, \mathbf{z}\bigr) \in [0, 1]$$

            where \(\hat{p}\) is the predicted probability that \(a\) is correct.<br><br>

            <span class="subsection">3.2 Probe Architectures</span><br><br>

            We systematically evaluate 6 probe architectures, each testing specific hypotheses about how uncertainty is encoded in hidden states. We organize architectures by their primary inductive bias: (1) Linearity (testing if simple models suffice), (2) Information localization (testing if uncertainty is sparse and/or hierarchical), (3) Layer composition (testing if uncertainty is distributed across depths).<br><br>

            <b>Baseline Probes (Testing Linearity)</b><br><br>

            <table class="results">
              <tr>
                <th style="width: 20%">Architecture</th>
                <th style="width: 65%">Description</th>
              </tr>
              <tr>
                <td><b>Linear Probe</b></td>
                <td><b>Hypothesis:</b> Uncertainty can be captured by a single linear dimension within the hidden states.<br><br>
                $$\hat{p} = \sigma(\mathbf{w}^\top \mathbf{h} + b)$$
                A single linear layer maps the hidden state to a confidence score. If the linear representation hypothesis <a href="#ref_14">[14]</a> holds for uncertainty, this simple classifier should suffice. We train with Brier loss rather than cross-entropy, encouraging calibration.</td>
              </tr>
              <tr>
                <td><b>MLP Probe</b></td>
                <td><b>Hypothesis:</b> The model encodes uncertainty in patterns that can't be captured by a purely linear probe.<br><br>
                $$\hat{p} = \sigma\bigl(\mathbf{W}_2 \cdot \text{ReLU}(\mathbf{W}_1 \mathbf{h} + \mathbf{b}_1) + b_2\bigr)$$
                A two-layer MLP with ReLU activations learns nonlinear interactions between hidden-state dimensions, letting us test whether uncertainty is encoded on a nonlinear manifold rather than a single direction.</td>
              </tr>
            </table><br>

            <b>Structural Hypotheses (Testing if uncertainty is reflected in architectural structure)</b><br><br>

            <table class="results">
              <tr>
                <th style="width: 20%">Architecture</th>
                <th style="width: 65%">Description</th>
              </tr>
              <tr>
                <td><b>Sparse Probe</b></td>
                <td><b>Hypothesis:</b> Only a small subset of hidden-state dimensions meaningfully encode uncertainty, revealing a sparse internal signal.<br><br>
                $$\hat{p} = \sigma(\mathbf{w}^\top (\mathbf{g} \odot \mathbf{h}) + b), \quad \mathbf{g} = \sigma(\mathbf{V}\mathbf{h})$$
                A differentiable gating mechanism \(\mathbf{g}\) assigns importance weights to each dimension before a linear readout, revealing whether uncertainty is concentrated in a sparse subset of coordinates.</td>
              </tr>
              <tr>
                <td><b>Hierarchical Probe</b></td>
                <td><b>Hypothesis:</b> Uncertainty is built up across layers through hierarchical refinement, with each depth contributing a different part of the signal.<br><br>
                $$\hat{p} = \sigma\bigl(\text{MLP}_{\text{global}}([\mathbf{r}_{\text{fine}}; \mathbf{r}_{\text{mid}}; \mathbf{r}_{\text{sem}}])\bigr)$$
                We process hidden states at three granularities: fine-grained features (local chunk MLPs), mid-level patterns (attention pooling over chunks), and semantic abstractions (global feedforward aggregation), testing whether uncertainty emerges through hierarchical refinement across these scales.</td>
              </tr>
            </table><br>

            <b>Multi-Layer Architectures (Testing if uncertainty is distributed across depths)</b><br><br>

            <table class="results">
              <tr>
                <th style="width: 20%">Architecture</th>
                <th style="width: 65%">Description</th>
              </tr>
              <tr>
                <td><b>Ensemble Probe</b></td>
                <td><b>Hypothesis:</b> Different layers capture complementary pieces of uncertainty, so combining probes across depths yields a stronger signal than any single layer.<br><br>
                $$\hat{p} = \sum_{l} \alpha_l \cdot \hat{p}_l, \quad \boldsymbol{\alpha} = \text{softmax}(\mathbf{a})$$
                We train linear probes on multiple layers and learn softmax combination weights \(\boldsymbol{\alpha}\) to determine whether different depths contribute complementary components of the uncertainty signal.</td>
              </tr>
              <tr>
                <td><b>Fusion Probe</b></td>
                <td><b>Hypothesis:</b> A model's internal uncertainty can diverge from its output confidence, and combining hidden-state features with logit signals can reveal this mismatch.<br><br>
                $$\hat{p} = \sigma\bigl(\text{MLP}([\mathbf{h}_{\text{attn}}; \mathbf{f}_{\text{logit}}])\bigr), \quad \mathbf{f}_{\text{logit}} = [H(\mathbf{z}), \text{margin}(\mathbf{z}), \max(\mathbf{z})]$$
                This probe integrates per-layer MLP encoders with logit-based features (entropy \(H\), margin, max probability) through cross-layer attention to detect cases where internal uncertainty diverges from the model's expressed confidence.</td>
              </tr>
            </table><br>

		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
            <span class="subsection">3.3 Dataset</span><br><br>

            We evaluate on two complementary benchmarks totaling ~10,000 examples:<br><br>

            <ul>
              <li><b>MMLU</b> <a href="#ref_15">[15]</a> (Massive Multitask Language Understanding): 57 subjects spanning STEM, humanities, and social sciences with 4-choice questions. We use the full validation set (~1,500 examples) to ensure coverage across all subject categories.</li><br>
              <li><b>MMLU-Pro</b>: An extended version with 10-choice questions and more challenging reasoning problems. We sample ~8,500 examples, providing greater difficulty variance and testing whether uncertainty signals remain extractable when the task is harder and answer distributions are more uniform.</li>
            </ul><br>

            Combining both datasets provides sufficient scale for robust probe training while testing generalization across difficulty levels.<br><br>

		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
            <span class="subsection">3.4 Training Protocol</span><br><br>

            Figure 1 illustrates our end-to-end pipeline: we pass MMLU questions through a frozen LLM, extract hidden states from quartile layers, train a lightweight probe to predict correctness, and evaluate using Brier score loss.<br><br>

            <img src="./images/architecture_diagram.svg" width="1200px" style="margin: 10px 0 15px 0;"/><br>
            <i>Figure 1: Probing pipeline overview. Hidden states from the frozen LLM are fed to a trainable probe, which outputs a calibrated confidence score.</i><br><br>

            We use 10-fold cross validation over ~10,000 examples (combining MMLU and MMLU-Pro). In each fold, ~1,000 examples are held out as a test set, while the remaining data are split into ~8,100 training and ~900 validation examples, so that each example is used for training in nine folds and for testing in one fold.<br><br>

            All probes are trained with:
            <ul>
              <li>Loss: Brier score \((\hat{p} - y)^2\). Unlike binary cross-entropy, which can be minimized by making extreme predictions, Brier score is a proper scoring rule that is uniquely minimized when predicted probabilities equal true probabilities. This directly optimizes for calibration rather than just discrimination.</li><br>

              <li>Optimizer: AdamW (lr = \(10^{-3}\), weight_decay = \(10^{-5}\)). We selected AdamW over SGD for its adaptive learning rates and decoupled weight decay, which provides more stable training on our relatively small probe architectures. Hyperparameters were tuned via grid search over lr \(\in \{10^{-4}, 5 \times 10^{-4}, 10^{-3}, 5 \times 10^{-3}\}\) and weight_decay \(\in \{10^{-6}, 10^{-5}, 10^{-4}\}\).</li><br>

              <li>Schedule: Cosine annealing over 100 epochs. Cosine decay provides smooth learning rate reduction without requiring manual milestone selection. We found this outperformed step decay and linear warmup schedules in preliminary experiments, likely because the gradual decay allows fine-tuning of the uncertainty direction in later epochs.</li><br>

              <li>Selection: Best validation Brier score. We checkpoint based on validation Brier rather than loss plateau to directly select for calibration quality. This prevents selecting overfit models that achieve low training loss but poor generalization.</li>
            </ul><br>

		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<!-- EXPERIMENTS -->
		<div class="content-margin-container" id="results">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
            <h1>4. Experiments</h1>

            We use a two-stage experimental design to systematically test our hypotheses. Experiment 1 asks: given access to a single middle layer, which probe architecture best extracts uncertainty? We compare Linear, MLP, Hierarchical, and Sparse probes. Experiment 2 asks: given that linear probes work well, what information should we provide as input? We compare single-layer, multi-layer ensemble, and fusion (hidden states + logits) configurations.<br><br>

            <span class="subsection">4.1 Experimental Setup</span><br><br>

            <b>Model:</b> Qwen2.5-7B with 8-bit quantization <a href="#ref_16">[16]</a>, selected after preliminary comparisons showed better calibration than Mistral-7B and Llama-3.1-8B.<br><br>

            <b>Hidden States:</b> Extracted from layers [7, 14, 21, 27] (quartile positions) at the last token position. <b>Labels:</b> Binary correctness based on logit-based prediction.<br><br>

            <span class="subsection">4.2 Evaluation Metrics</span><br><br>

            We evaluate on three metrics:
            <ul>
              <li><b>AUROC:</b> Discrimination (probability correct answer ranks higher). Range [0,1], 0.5 = random.</li>
              <li><b>Brier Score</b> <a href="#ref_13">[13]</a>: \(\frac{1}{N} \sum (p_i - y_i)^2\). Lower is better.</li>
              <li><b>ECE</b> <a href="#ref_12">[12]</a>: Expected Calibration Error, measuring whether predicted probabilities match empirical frequencies. Lower is better.</li>
            </ul><br>

            <span class="subsection">4.3 Baseline: Do Probes Work?</span><br><br>

            Before comparing architectures, we first establish that hidden states contain extractable uncertainty signals. We compare a simple linear probe against a random baseline.<br><br>

            <table class="results">
              <tr>
                <th>Method</th>
                <th>AUROC &uarr;</th>
                <th>Brier &darr;</th>
                <th>ECE &darr;</th>
              </tr>
              <tr>
                <td>Random baseline</td>
                <td>0.5016</td>
                <td>0.3334</td>
                <td>0.2873</td>
              </tr>
              <tr>
                <td><b>Linear probe (layer 14)</b></td>
                <td><b>0.829</b></td>
                <td><b>0.143</b></td>
                <td><b>0.033</b></td>
              </tr>
            </table>
            <i>Table 1: Baseline comparison. The linear probe achieves AUROC 0.829, well above random (0.5016), confirming hidden states contain extractable uncertainty signals.</i><br><br>

            <span class="subsection">4.4 Experiment 1: Which Architecture is Best?</span><br><br>

            <b>Research Question:</b> Given access to a single middle layer (layer 14), which probe architecture best extracts uncertainty?<br><br>

            We tested four probe architectures: Linear (4K params), MLP (920K params), Hierarchical (320K params), and Sparse (460K params).<br><br>

            <b>Results:</b><br><br>

            <table class="results">
              <tr>
                <th>Architecture</th>
                <th>AUROC &uarr;</th>
                <th>Brier &darr;</th>
                <th>ECE &darr;</th>
                <th>Parameters</th>
              </tr>
              <tr>
                <td><b>Linear</b></td>
                <td><b>0.829 ± 0.014</b></td>
                <td><b>0.143 ± 0.009</b></td>
                <td><b>0.033 ± 0.007</b></td>
                <td>4K</td>
              </tr>
              <tr>
                <td>MLP</td>
                <td>0.702 ± 0.031</td>
                <td>0.243 ± 0.018</td>
                <td>0.223 ± 0.034</td>
                <td>920K</td>
              </tr>
              <tr>
                <td>Hierarchical</td>
                <td>0.818 ± 0.016</td>
                <td>0.143 ± 0.010</td>
                <td>0.039 ± 0.009</td>
                <td>320K</td>
              </tr>
              <tr>
                <td>Sparse</td>
                <td>0.833 ± 0.015</td>
                <td>0.142 ± 0.008</td>
                <td>0.041 ± 0.010</td>
                <td>460K</td>
              </tr>
            </table>
            <i>Table 2: Architecture comparison on layer 14 (mean ± std, 10-fold CV).</i><br><br>

            <b>Finding:</b> Linear matches complex alternatives despite 80-230× fewer parameters, while MLP significantly underperforms. With limited signal from a single layer, the capacity-constrained linear probe finds the most robust uncertainty direction, avoiding overfitting. This aligns with the linear representation hypothesis <a href="#ref_14">[14]</a>.<br><br>

            <b>Layer Analysis:</b><br><br>

            <img src="./images/layer_confidence.png" width=700px/><br>
            <i>Figure 2: Linear probe AUROC across transformer layers. Middle layers (50% depth) achieve best single-layer performance, consistent with prior work <a href="#ref_5">[5]</a>.</i><br><br>

            <b>Calibration Analysis:</b><br><br>

            <img src="./images/reliability_diags.png" width=650px/><br>
            <i>Figure 3: Reliability diagrams. Linear (ECE=0.033) and hierarchical (ECE=0.039) probes show excellent calibration, while MLP (ECE=0.223) is significantly miscalibrated.</i><br><br>

            <span class="subsection">4.5 Experiment 2: What Information Should We Use?</span><br><br>

            <b>Research Question:</b> Given that linear probes work best, what information should we provide as input?<br><br>

            We tested three configurations of the linear probe architecture:
            <ol>
              <li><b>Single layer</b> (layer 14 only) - baseline</li>
              <li><b>Multi-layer ensemble</b> (layers 7, 14, 21, 27) - testing layer complementarity</li>
              <li><b>Fusion</b> (layers 7, 14, 21, 27 + output logits) - testing miscalibration detection</li>
            </ol>

            The multi-layer ensemble probe uses four parallel linear probes (one per quartile layer) with learned softmax-normalized combination weights. The fusion probe combines per-layer linear probes with logit feature extraction (entropy, margin, max probability) to detect cases where internal uncertainty and expressed confidence diverge.<br><br>

            <b>Results:</b><br><br>

            <table class="results">
              <tr>
                <th>Configuration</th>
                <th>Input</th>
                <th>AUROC &uarr;</th>
                <th>Brier &darr;</th>
                <th>ECE &darr;</th>
              </tr>
              <tr>
                <td>Single Layer</td>
                <td>Layer 14</td>
                <td>0.829 ± 0.014</td>
                <td>0.143 ± 0.009</td>
                <td>0.033 ± 0.007</td>
              </tr>
              <tr>
                <td>Ensemble</td>
                <td>4 layers</td>
                <td>0.862 ± 0.012</td>
                <td>0.121 ± 0.008</td>
                <td>0.028 ± 0.006</td>
              </tr>
              <tr>
                <td><b>Fusion</b></td>
                <td><b>4 layers + logits</b></td>
                <td><b>0.904 ± 0.009</b></td>
                <td><b>0.098 ± 0.007</b></td>
                <td><b>0.021 ± 0.005</b></td>
              </tr>
            </table>
            <i>Table 3: Information source comparison (mean ± std, 10-fold CV).</i><br><br>

            <b>Finding:</b> Adding information sources consistently improves performance. Brier score drops from 0.143 (single layer) → 0.121 (ensemble) → 0.098 (fusion). Different layers encode complementary uncertainty aspects, while logit features capture "expressed" confidence, enabling detection of internal-external divergence.<br><br>

            <b>Layer Weight Analysis:</b><br><br>

            <img src="./images/fusionlayerweights.png" width=700px/><br>
            <i>Figure 4: Learned layer weights show near-uniform contribution, indicating uncertainty is distributed across the network rather than localized.</i><br><br>

		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<!-- DISCUSSION -->
		<div class="content-margin-container" id="analysis">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>5. Discussion</h1>

            <span class="subsection">5.1 Key Insights</span><br><br>

            <b>Linear encoding.</b> The linear probe outperforms MLP (Brier 0.143 vs 0.243), supporting the linear representation hypothesis <a href="#ref_14">[14]</a>: uncertainty is encoded as a geometric direction, not complex non-linear relationships. This means uncertainty is accessible to the model's own linear readout mechanisms, enabling interpretability and steering.<br><br>

            <b>Distributed signal.</b> Near-uniform learned layer weights indicate uncertainty is distributed across the network. Multi-layer probes outperform single-layer approaches by aggregating complementary signals from different depths.<br><br>

            <b>Internal-external divergence.</b> Fusion probe's 19% improvement over ensemble (Brier 0.121 → 0.098) confirms that internal and expressed confidence can diverge. Combining hidden states with logit features detects cases where the model appears confident externally but is uncertain internally.<br><br>

            <span class="subsection">5.2 Limitations</span><br><br>

            <ol>
              <li>Our evaluation focuses on Qwen2.5-7B, and we observed that Llama-3.1-8B produced less calibrated probes (Brier scores ~0.10 higher). Uncertainty encoding may vary across model families and scales. Future work could develop model-agnostic probes by training on hidden states from multiple models, or use transfer learning to adapt probes across architectures.</li>
              <li>MMLU's constrained 4-choice format may not reflect uncertainty patterns in open-ended generation. Combining probing with semantic entropy <a href="#ref_6">[6]</a> by using probes for efficient screening, then semantic clustering for high-uncertainty cases could address this.</li>
              <li>While MMLU spans 57 subjects, all questions are factual recall. Mathematical reasoning or open-domain QA may exhibit different uncertainty structures. Multi-task probe training across diverse benchmarks would help test generalization.</li>
              <li>We extract from a single forward pass at the final token. Uncertainty may evolve during chain-of-thought reasoning. Temporal probing could track hidden state evolution across reasoning steps.</li>
            </ol><br>

            <span class="subsection">5.3 Practical Implications</span><br><br>

            Well-calibrated probes (ECE 0.021) enable: <b>selective prediction</b> (abstain below confidence threshold), <b>uncertainty-triggered retrieval</b> (fetch documents when uncertain), <b>tiered human escalation</b> (route low-confidence queries to experts), and <b>user-facing confidence scores</b>. Single-pass efficiency makes these patterns practical for real-world tasks.<br><br>

            <span class="subsection">5.4 Future Directions</span><br><br>

            Several extensions could build on this work. Fine-grained layer analysis (probing all 28 layers) would reveal how uncertainty representation evolves through the network. Uncertainty steering via activation engineering could enable controlled calibration by adding or subtracting the learned "uncertainty vector." Cross-model transfer experiments would test whether uncertainty encoding is universal or model-specific. Uncertainty-aware decoding could adapt sampling temperature based on probe outputs, exploring alternatives when uncertain and committing when confident. Finally, extending probes to vision-language models and chain-of-thought reasoning would address hallucination detection and temporal uncertainty dynamics in more complex generation settings.
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<!-- CONCLUSION -->
		<div class="content-margin-container" id="conclusion">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>6. Conclusion</h1>

            We set out to test whether a model's internal representation of uncertainty diverges from its expressed confidence, and whether this divergence can be detected to improve calibration. Our experiments on ~10,000 MMLU examples provide three key findings. First, internal and expressed confidence can diverge, and detecting this improves calibration: our fusion probe, which combines hidden states with output logits, achieves 19% better Brier score than hidden-state-only approaches (0.098 vs 0.121, \(p < 0.01\)). Models sometimes "know" they are uncertain internally while expressing confidence externally, and fusing both signals reveals this miscalibration. Second, uncertainty is linearly encoded and distributed across layers. Linear probes achieve statistically equivalent performance to complex architectures (hierarchical, sparse) despite 80-230× fewer parameters, while the higher-capacity MLP significantly underperforms. Learned layer weights are near-uniform, indicating uncertainty signal is distributed rather than localized. Third, lightweight probes achieve strong calibration: our best probe achieves AUROC 0.904 and ECE 0.021 in a single forward pass, competitive with multi-sample methods like semantic entropy but 5-10× more efficient.<br><br>

            These findings support our central hypothesis: models do "know what they don't know," and this knowledge can be extracted by fusing internal uncertainty with expressed confidence. In practice, this means a compute-performance tradeoff: single-layer linear probes offer efficient baseline uncertainty quantification, while multi-layer fusion probes achieve superior calibration for high-stakes applications. Both approaches enable selective prediction, uncertainty-triggered retrieval, and human escalation, making LLMs safer for real-world deployment. Our implementation is available on <a href="https://github.com/joshcliu/deep-learning">Github</a>.
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<!-- REFERENCES -->
		<div class="content-margin-container" id="citations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<div class='citation' id="references" style="height:auto"><br>
							<span style="font-size:16px">References:</span><br><br>
							<a id="ref_1"></a>[1] Ji, Z., Lee, N., Frieske, R., et al. (2023). <a href="https://dl.acm.org/doi/10.1145/3571730">Survey of Hallucination in Natural Language Generation</a>. <i>ACM Computing Surveys</i>, 55(12), 1-38.<br><br>
							<a id="ref_2"></a>[2] Zhang, Y., Li, Y., Cui, L., et al. (2023). <a href="https://arxiv.org/abs/2311.05232">Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models</a>. <i>arXiv:2311.05232</i>.<br><br>
							<a id="ref_3"></a>[3] Kadavath, S., Conerly, T., Askell, A., et al. (2022). <a href="https://arxiv.org/abs/2207.05221">Language Models (Mostly) Know What They Know</a>. <i>arXiv:2207.05221</i>.<br><br>
							<a id="ref_4"></a>[4] Burns, C., Ye, H., Klein, D., & Steinhardt, J. (2023). <a href="https://openreview.net/forum?id=ETKGuby0hcs">Discovering Latent Knowledge in Language Models Without Supervision</a>. <i>ICLR 2023</i>.<br><br>
							<a id="ref_5"></a>[5] Gurnee, W. & Tegmark, M. (2024). <a href="https://arxiv.org/abs/2502.02013">Layer by Layer: Uncovering Hidden Representations in Language Models</a>. <i>arXiv:2502.02013</i>.<br><br>
							<a id="ref_6"></a>[6] Hewitt, J. & Manning, C. (2019). <a href="https://aclanthology.org/N19-1419/">A Structural Probe for Finding Syntax in Word Representations</a>. <i>NAACL 2019</i>.<br><br>
							<a id="ref_7"></a>[7] Azaria, A. & Mitchell, T. (2023). <a href="https://arxiv.org/abs/2304.13734">The Internal State of an LLM Knows When It's Lying</a>. <i>EMNLP 2023 Findings</i>.<br><br>
							<a id="ref_8"></a>[8] Jiang, Z., Araki, J., Ding, H., & Neubig, G. (2021). <a href="https://aclanthology.org/2021.tacl-1.57/">How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering</a>. <i>TACL</i>, 9, 962-977.<br><br>
							<a id="ref_9"></a>[9] Farquhar, S., Kossen, J., Kuhn, L., & Gal, Y. (2024). <a href="https://www.nature.com/articles/s41586-024-07421-0">Detecting hallucinations in large language models using semantic entropy</a>. <i>Nature</i>, 630, 625-630.<br><br>
							<a id="ref_10"></a>[10] Xiong, M., Hu, Z., Lu, X., et al. (2024). <a href="https://openreview.net/forum?id=gjeQKFxFpZ">Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs</a>. <i>ICLR 2024</i>.<br><br>
							<a id="ref_11"></a>[11] Gekhman, Z., Yona, G., Aharoni, R., et al. (2025). <a href="https://belinkov.com/assets/pdf/iclr2025-know.pdf">Does the Model Know It Knows? Probing Knowledge in Language Models</a>. <i>ICLR 2025</i>.<br><br>
							<a id="ref_12"></a>[12] Guo, C., Pleiss, G., Sun, Y., & Weinberger, K.Q. (2017). <a href="https://proceedings.mlr.press/v70/guo17a.html">On Calibration of Modern Neural Networks</a>. <i>ICML 2017</i>.<br><br>
							<a id="ref_13"></a>[13] Brier, G.W. (1950). <a href="https://journals.ametsoc.org/view/journals/mwre/78/1/1520-0493_1950_078_0001_vofeit_2_0_co_2.xml">Verification of Forecasts Expressed in Terms of Probability</a>. <i>Monthly Weather Review</i>, 78(1), 1-3.<br><br>
							<a id="ref_14"></a>[14] Park, K., Choe, Y.J., & Veitch, V. (2024). <a href="https://proceedings.mlr.press/v235/park24c.html">The Linear Representation Hypothesis and the Geometry of Large Language Models</a>. <i>ICML 2024</i>.<br><br>
							<a id="ref_15"></a>[15] Hendrycks, D., Burns, C., Basart, S., et al. (2021). <a href="https://openreview.net/forum?id=d7KBjmI3GmQ">Measuring Massive Multitask Language Understanding</a>. <i>ICLR 2021</i>.<br><br>
							<a id="ref_16"></a>[16] Dettmers, T., Lewis, M., Belkada, Y., & Zettlemoyer, L. (2022). <a href="https://arxiv.org/abs/2208.07339">LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale</a>. <i>NeurIPS 2022</i>.<br><br>
						</div>
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

	</body>

</html>
