<html>
<head>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

<!-- MathJax for consistent math rendering across browsers -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/mml-chtml.js"></script>

<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
	body {
		background-color: #f5f9ff;
	}

	.content-margin-container {
		display: flex;
		width: 100%;
		justify-content: left;
		align-items: center;
	}
	.main-content-block {
		width: 70%;
    max-width: 1100px;
		background-color: #fff;
		border-left: 1px solid #DDD;
		border-right: 1px solid #DDD;
		padding: 8px 8px 8px 8px;
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
	}
	.margin-left-block {
			font-size: 14px;
			width: 15%;
			max-width: 130px;
			position: relative;
			margin-left: 10px;
			text-align: left;
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
			padding: 5px;
	}
	.margin-right-block {
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
			font-size: 14px;
			width: 25%;
			max-width: 256px;
			position: relative;
			text-align: left;
			padding: 10px;
	}

	img {
			max-width: 100%;
			height: auto;
			display: block;
			margin: auto;
	}

	a:link,a:visited
	{
		color: #0e7862;
		text-decoration: none;
	}
	a:hover {
		color: #24b597;
	}

	h1 {
		font-size: 18px;
		margin-top: 4px;
		margin-bottom: 10px;
	}

	table.header {
    font-weight: 300;
    font-size: 17px;
    flex-grow: 1;
		width: 70%;
    max-width: calc(100% - 290px);
	}
	table td, table td * {
	    vertical-align: middle;
	    position: relative;
	}

	hr {
    height: 1px;
    border: none;
    background-color: #DDD;
  }

	div.hypothesis {
		width: 80%;
		background-color: #EEE;
		border: 1px solid black;
		border-radius: 10px;
		font-family: Courier;
		font-size: 16px;
		text-align: center;
		margin: auto;
		padding: 16px 16px 16px 16px;
	}

	div.citation {
    font-size: 0.8em;
    background-color:#fff;
    padding: 10px;
		height: 200px;
  }

	table.results {
		border-collapse: collapse;
		width: 100%;
		margin: 10px 0;
	}
	table.results th, table.results td {
		border: 1px solid #DDD;
		padding: 8px;
		text-align: left;
	}
	table.results th {
		background-color: #f5f9ff;
	}

	code {
		background-color: #f5f5f5;
		padding: 2px 6px;
		border-radius: 3px;
		font-family: 'Courier New', Courier, monospace;
		font-size: 14px;
	}

	pre {
		background-color: #f5f5f5;
		padding: 12px;
		border-radius: 5px;
		overflow-x: auto;
		font-family: 'Courier New', Courier, monospace;
		font-size: 13px;
	}

</style>

	  <title>Probing the Mind of LLMs: Extracting Confidence from Hidden States</title>
      <meta property="og:title" content="Probing the Mind of LLMs: Extracting Confidence from Hidden States" />
			<meta charset="UTF-8">
  </head>

  <body>

		<!-- HEADER -->
		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<table class="header" align=left>
								<tr>
									<td colspan=4>
										<span style="font-size: 32px; font-family: 'Courier New', Courier, monospace;">Probing the Mind of LLMs</span>
									</td>
								</tr>
								<tr>
									<td colspan=4>
										<span style="font-size: 20px; color: #666;">Extracting Confidence from Hidden States</span>
									</td>
								</tr>
								<tr>
										<td align=left>
												<span style="font-size:17px"><a href="#">Joshua Liu</a></span>
										</td>
										<td align=left>
												<span style="font-size:17px"><a href="#">Carol Li</a></span>
										</td>
										<td align=left>
												<span style="font-size:17px"><a href="#">Maureen Zhang</a></span>
										</td>
								<tr>
									<td colspan=4 align=left><span style="font-size:18px">Final project for 6.7960, MIT</span></td>
								</tr>
						</table>
					</div>
					<div class="margin-right-block">
					</div>
		</div>

    <!-- INTRODUCTION -->
    <div class="content-margin-container" id="intro">
				<div class="margin-left-block">
          <div style="position:fixed; max-width:inherit; top:max(20%,120px)">
              <b style="font-size:16px">Outline</b><br><br>
              <a href="#intro">Introduction</a><br><br>
              <a href="#background">Background</a><br><br>
              <a href="#methodology">Methodology</a><br><br>
              <a href="#architecture">Our Approach</a><br><br>
              <a href="#results">Results</a><br><br>
              <a href="#analysis">Discussion</a><br><br>
              <a href="#conclusion">Conclusion</a><br><br>
          </div>
				</div>
		    <div class="main-content-block">
						<h1>1. Introduction</h1>

            Imagine asking an AI assistant about a medication interaction, and it responds with complete confidence, citing studies, explaining mechanisms, sounding utterly authoritative. The only problem? The information is completely fabricated. This isn't a hypothetical scenario. It's the everyday reality of large language models, which routinely generate plausible-sounding falsehoods with the same linguistic confidence as established facts <a href="#ref_1">[1]</a>.<br><br>

            This phenomenon, colorfully termed "hallucination," represents one of the most pressing challenges in deploying LLMs for real-world applications <a href="#ref_2">[2]</a>. When a model generates fabricated legal precedents, incorrect medical dosages, or fictional scientific citations, the consequences can range from embarrassing to dangerous. The core issue isn't just that models make mistakes (all systems do), but that they provide no reliable signal about when they're likely to be wrong.<br><br>

            The natural question is: can we teach models to know what they don't know? Or more precisely, can we extract such knowledge if it already exists somewhere in the model's internal representations?<br><br>

            <div class="hypothesis">
              <b>Central Hypothesis:</b> Large language models encode uncertainty information in their hidden states that is partially lost or distorted in the final output distribution. By probing intermediate representations, particularly from middle layers, we can extract signals that indicate when a model "knows" it is uncertain, even when its outputs appear confident.
            </div><br>

            This hypothesis is grounded in a growing body of evidence. Kadavath et al. <a href="#ref_3">[3]</a> demonstrated that LLMs exhibit some capacity for self-knowledge, performing above chance when asked to predict their own correctness. Burns et al. <a href="#ref_4">[4]</a> showed that hidden states can predict statement truthfulness without any labels, suggesting the model has already computed something like "confidence" internally. Perhaps most intriguingly, research on layer-wise representations has consistently found that middle layers (roughly 50-75% of network depth) encode richer semantic information than final layers <a href="#ref_5">[5]</a>, as if the model's deepest "thinking" happens in the middle, with final layers primarily formatting output.<br><br>

            Yet despite these promising signals, there's a gap between knowing that uncertainty information exists somewhere in the model and understanding how to optimally extract it. Most prior work uses simple linear probes on single layers. But what if uncertainty is distributed across layers? What if the relationship between hidden states and confidence is non-linear? And crucially, what if the model's internal uncertainty diverges from its expressed confidence, creating a dangerous miscalibration?<br><br>

            <b>Our Contributions:</b><br><br>
            We address these questions through systematic experimentation:
            <ol>
              <li><b>Comprehensive architecture comparison:</b> We evaluate multiple probe architectures with different inductive biases, from simple linear classifiers to attention-based networks, testing which assumptions about uncertainty structure hold in practice.</li>
              <li><b>Fusion Probe:</b> We introduce a novel architecture that combines hidden states from multiple transformer layers with output logit features. This design can detect miscalibration, cases where internal uncertainty and expressed confidence disagree.</li>
              <li><b>Layer importance analysis:</b> Through learned attention weights, we empirically validate which layers contribute most to uncertainty prediction, testing the "middle layers are optimal" hypothesis.</li>
              <li><b>Open-source framework:</b> We release our complete codebase for reproducible LLM confidence probing experiments.</li>
            </ol><br>

            Figure 1 shows our overall pipeline: we extract hidden states from multiple transformer layers, combine them with output logits, and train a probe to predict whether the model's answer is correct.<br><br>

            <img src="./images/overview_diagram.svg" width=700px/><br>
            <center><b>Figure 1:</b> Overview of our confidence probing pipeline. Hidden states from multiple transformer layers are extracted and combined with output logits to predict whether the model's answer is correct.</center>
		    </div>
		    <div class="margin-right-block">
						The hallucination problem is particularly insidious because users cannot easily distinguish confident correct responses from confident incorrect ones without external verification.<br><br>

            A 2024 Nature study <a href="#ref_6">[6]</a> found semantic entropy can detect "confabulations" but requires 5-10 forward passes per query. Our approach extracts uncertainty from a single pass.<br><br>

            <b>Why does this matter?</b> If we can reliably detect when models are uncertain, we enable selective prediction (abstaining on hard cases), retrieval augmentation (fetching external knowledge when needed), and human-in-the-loop systems (flagging responses for expert review).
		    </div>
		</div>

		<!-- BACKGROUND AND RELATED WORK -->
		<div class="content-margin-container" id="background">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
					<h1>2. Background and Related Work</h1>

          Before diving into our approach, it's worth understanding the landscape of uncertainty quantification: where the field has been, what's been tried, and why the problem remains challenging.<br><br>

          <b>2.1 The Classical Uncertainty Toolkit</b><br><br>

          Machine learning has grappled with uncertainty for decades, developing several foundational approaches. Bayesian neural networks <a href="#ref_7">[7]</a> learn distributions over weights rather than point estimates, providing principled uncertainty through posterior inference. Monte Carlo dropout <a href="#ref_8">[8]</a> offered a practical approximation: keep dropout active at test time, run multiple forward passes, and use prediction variance as uncertainty. Deep ensembles <a href="#ref_9">[9]</a> took a simpler route: train multiple networks independently and measure their disagreement.<br><br>

          These methods distinguish between two types of uncertainty: aleatoric (inherent noise in the data that cannot be reduced with more training) and epistemic (uncertainty from model limitations that could, in principle, be reduced). For LLMs answering factual questions, epistemic uncertainty is what we care about most. We want to know when the model simply doesn't have the knowledge to answer correctly.<br><br>

          <b>2.2 Uncertainty in the Age of LLMs</b><br><br>

          The transformer revolution <a href="#ref_10">[10]</a> brought new challenges. Modern LLMs operate at scales where ensembles are prohibitively expensive, and the relationship between token probabilities and factual confidence is tenuous at best. Three paradigms have emerged to tackle LLM uncertainty:<br><br>

          Token probability methods use the model's softmax outputs as confidence proxies. The intuition is simple: if the model assigns high probability to its answer, it should be confident. Unfortunately, Jiang et al. <a href="#ref_11">[11]</a> demonstrated these are poorly calibrated. Models often assign high probability to wrong answers, particularly in open-ended generation. The model's "confidence" in its next token doesn't translate to confidence in factual correctness.<br><br>

          Consistency-based methods sample multiple responses and measure agreement. The breakthrough here came from Kuhn et al. <a href="#ref_6">[6]</a>, who introduced semantic entropy, a clever approach that clusters responses by meaning before computing uncertainty. "Paris," "The capital is Paris," and "It's Paris, France" are treated as the same answer. This achieved state-of-the-art hallucination detection and was published in Nature in 2024. The catch? It requires 5-10 forward passes per query, making it computationally expensive for deployment.<br><br>

          Verbalized confidence simply asks the model: "How confident are you?" Xiong et al. <a href="#ref_12">[12]</a> systematically evaluated this approach, finding that while LLMs can express uncertainty, they tend to be systematically overconfident, perhaps mimicking human patterns of expressing confidence. Fine-tuning can help, but the fundamental issue remains: the model's self-report may not reflect its actual internal state.<br><br>

          <b>2.3 Probing: Looking Inside the Black Box</b><br><br>

          Our approach falls into a fourth paradigm: probing internal representations. The idea is conceptually simple. If the model has computed something like "confidence" internally, we should be able to extract it by training a classifier on the hidden states.<br><br>

          Probing has a rich history in NLP. Hewitt and Manning <a href="#ref_13">[13]</a> pioneered structural probes, showing that parse trees are encoded as geometric relationships in BERT's hidden space. The key insight: if a property can be linearly extracted from representations, the model has likely computed it explicitly. Complex properties requiring non-linear probes might be artifacts of the probe's own learning rather than genuine model knowledge.<br><br>

          For uncertainty specifically, several works laid the groundwork. Kadavath et al. <a href="#ref_3">[3]</a> demonstrated that large LLMs exhibit meaningful self-knowledge. When asked to evaluate their own answers, they perform substantially above chance, with calibration improving with scale. Burns et al. <a href="#ref_4">[4]</a> went further with Contrast-Consistent Search (CCS), showing that hidden states can predict statement truthfulness without any labels at all. Their method finds a direction in activation space where a statement and its negation have opposite values, essentially discovering the model's internal "truth detector."<br><br>

          More recent work has examined where uncertainty is encoded. Azaria and Mitchell <a href="#ref_14">[14]</a> trained classifiers on hidden states to detect factual errors, finding strong signal in intermediate layers. Gekhman et al. <a href="#ref_15">[15]</a> revealed a troubling gap: LLMs can encode knowledge internally (detectable via probing) while still failing to express it correctly in output. This suggests internal representations may be more informative than generated text.<br><br>

          <b>2.4 Why Use Linear Probes?</b><br><br>

          <p>
          A central goal of this study is to determine whether large language models encode uncertainty in a form that is directly exploitable by the model itself, rather than merely reconstructible by an external classifier. In most modern architectures, the final prediction head is a linear map applied to the hidden representation. If deep layers have successfully unrolled the input into a representation where high-level concepts are disentangled, these concepts should be accessible via simple linear functionals on the hidden state. In this context, a linear probe is not an arbitrary modeling choice, but a natural extension of the model's own readout mechanism: it asks whether uncertainty is present as a linearly decodable direction in representation space.
          </p>

          <p>
          Using higher-capacity probes, such as multi-layer perceptrons, risks conflating two distinct questions: whether the base model has already organized uncertainty into an explicit geometric feature, and whether a sufficiently flexible classifier can learn to infer uncertainty from distributed, non-linear patterns in the activations. A non-linear probe can learn complex decision boundaries and implement feature interactions that were never made explicit by the underlying model. High performance in this regime therefore does not clearly indicate that uncertainty is in the representation in a way the model itself could exploit with a simple head; instead, it may reflect the probe effectively learning the task from scratch on top of the hidden states.
          </p>

          <p>
          By contrast, linear probes are deliberately capacity-limited. A linear classifier cannot synthesize new non-linear features or recover functions like XOR over hidden dimensions. It can only succeed if the target property (here, answer correctness or uncertainty) is already arranged such that a single hyperplane meaningfully separates higher- and lower-uncertainty states. Strong linear probe performance therefore provides much cleaner evidence that uncertainty is explicitly and geometrically represented in the model's hidden states, rather than being latent in a way that requires additional non-linear computation to extract.
          </p>

          <p>
          Finally, linear probes are directly compatible with representation engineering and steering techniques that operate through vector arithmetic on activations. Methods that attempt to manipulate model behavior by adding or subtracting concept vectors implicitly assume that the underlying feature can be expressed as a direction in activation space. If uncertainty were only extractable via a non-linear probe, there would be no single direction along which we could reliably turn up or turn down uncertainty, limiting both interpretability and control. Focusing on linear probes therefore aligns our methodology with both the internal structure of standard LLM architectures and the practical goal of discovering uncertainty features that can be read and, in principle, steered using simple, transparent operations.
          </p><br>

          <b>2.5 Calibration: When Confidence Matches Reality</b><br><br>

          A model is calibrated if its confidence matches its accuracy: when it predicts 70% confidence, it should be correct 70% of the time. This property is crucial for downstream applications. Users need to know when to trust model outputs.<br><br>

          Guo et al. <a href="#ref_19">[19]</a> revealed that modern neural networks are often poorly calibrated, tending toward overconfidence. Their work introduced temperature scaling, a simple post-hoc fix that rescales logits, and popularized Expected Calibration Error (ECE) as a metric:<br><br>
          <center>
            <math xmlns="http://www.w3.org/1998/Math/MathML">
              <mrow>
                <mi>ECE</mi>
                <mo>=</mo>
                <munderover>
                  <mo>&sum;</mo>
                  <mrow><mi>b</mi><mo>=</mo><mn>1</mn></mrow>
                  <mi>B</mi>
                </munderover>
                <mfrac>
                  <msub><mi>n</mi><mi>b</mi></msub>
                  <mi>N</mi>
                </mfrac>
                <mo>|</mo>
                <mi>acc</mi><mo>(</mo><mi>b</mi><mo>)</mo>
                <mo>-</mo>
                <mi>conf</mi><mo>(</mo><mi>b</mi><mo>)</mo>
                <mo>|</mo>
              </mrow>
            </math>
          </center><br>

          ECE bins predictions by confidence and computes the average gap between predicted confidence and observed accuracy. A perfectly calibrated model has ECE = 0. However, ECE has limitations. A model predicting constant 50% confidence on a 50% accuracy task achieves perfect ECE despite providing no useful information.<br><br>

          The Brier score <a href="#ref_20">[20]</a> addresses this as a proper scoring rule. It's uniquely minimized when predictions equal true probabilities:<br><br>
          <center>
            <math xmlns="http://www.w3.org/1998/Math/MathML">
              <mrow>
                <mi>Brier</mi>
                <mo>=</mo>
                <mfrac>
                  <mn>1</mn>
                  <mi>N</mi>
                </mfrac>
                <munderover>
                  <mo>&sum;</mo>
                  <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
                  <mi>N</mi>
                </munderover>
                <msup>
                  <mrow>
                    <mo>(</mo>
                    <msub><mi>p</mi><mi>i</mi></msub>
                    <mo>-</mo>
                    <msub><mi>y</mi><mi>i</mi></msub>
                    <mo>)</mo>
                  </mrow>
                  <mn>2</mn>
                </msup>
              </mrow>
            </math>
          </center><br>
          The Brier score rewards confident correct predictions and heavily penalizes confident wrong ones, which is exactly the behavior we want for hallucination detection. We train our probes with Brier loss to encourage this calibration property.<br><br>

          <b>2.6 The Layer Question: Where Is Uncertainty Encoded?</b><br><br>

          A recurring finding across probing studies is that middle layers outperform final layers for extracting high-level semantic information. Research has consistently shown that intermediate transformer layers (roughly 50-75% of network depth) encode the richest representations <a href="#ref_5">[5]</a>, with early layers capturing syntactic features and final layers optimizing for output prediction.<br><br>

          This makes intuitive sense: final layers specialize in mapping representations to vocabulary, potentially losing nuanced information in the process. If the model's "thinking" peaks in middle layers, that's where uncertainty signals should be strongest.<br><br>

          <b>2.7 Gaps We Address</b><br><br>

          Despite substantial progress, prior work leaves several questions unanswered:
          <ol>
            <li><b>Architecture comparison:</b> Most studies use linear probes. How do non-linear architectures (MLPs, attention mechanisms, sparse selection) compare? Do different inductive biases about uncertainty structure yield different results?</li>
            <li><b>Multi-layer fusion:</b> If uncertainty is distributed across layers, combining them should help. But how? Simple concatenation? Learned attention? Ensembling?</li>
            <li><b>Internal vs. expressed confidence:</b> Probes typically use only hidden states. But the model's output logits contain its "expressed" confidence. What if internal and expressed confidence disagree? This miscalibration might be exactly what we want to detect.</li>
          </ol>

          Our Fusion Probe addresses (3) directly, while our systematic architecture comparison tackles (1) and (2).
		    </div>
		    <div class="margin-right-block" style="transform: translate(0%, -30%);">
          <b>A key distinction:</b> Semantic entropy <a href="#ref_6">[6]</a> captures uncertainty about what to say (generation variance), while hidden state probes capture uncertainty about correctness (internal confidence). These are complementary signals.<br><br>

          TruthfulQA <a href="#ref_21">[21]</a> found larger models are often less truthful because they've learned to generate convincing-sounding falsehoods. This underscores why output fluency doesn't equal confidence.<br><br>

          Recent work on conformal prediction <a href="#ref_22">[22]</a> offers another perspective: distribution-free uncertainty guarantees. While we focus on probing, these approaches are complementary.<br><br>

          <b>Why middle layers?</b> The transformer's information flow suggests early layers handle syntax, middle layers handle semantics, and final layers handle output formatting. Uncertainty about meaning should peak in the middle.
		    </div>
		</div>

		<!-- METHODOLOGY -->
		<div class="content-margin-container" id="methodology">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
            <h1>3. Methodology</h1>

            <b>3.1 Problem Formulation</b><br><br>

            Given an LLM M, a question q, and the model's generated answer a, we aim to predict whether a is correct. Formally, let h<sup>(l)</sup> &in; &#x211D;<sup>d</sup> denote the hidden state at layer l, and let z &in; &#x211D;<sup>K</sup> denote the output logits for K answer choices. We train a probe f<sub>&theta;</sub> to predict:<br><br>
            <center>
              <math xmlns="http://www.w3.org/1998/Math/MathML">
                <mrow>
                  <mover>
                    <mi>p</mi>
                    <mo>^</mo>
                  </mover>
                  <mo>=</mo>
                  <msub>
                    <mi>f</mi>
                    <mi>&theta;</mi>
                  </msub>
                  <mo>(</mo>
                  <msup><mi>h</mi><mrow><mo>(</mo><msub><mi>l</mi><mn>1</mn></msub><mo>)</mo></mrow></msup>
                  <mo>,</mo>
                  <mo>...</mo>
                  <mo>,</mo>
                  <msup><mi>h</mi><mrow><mo>(</mo><msub><mi>l</mi><mi>k</mi></msub><mo>)</mo></mrow></msup>
                  <mo>,</mo>
                  <mi>z</mi>
                  <mo>)</mo>
                  <mo>&isin;</mo>
                  <mo>[</mo>
                  <mn>0</mn>
                  <mo>,</mo>
                  <mn>1</mn>
                  <mo>]</mo>
                </mrow>
              </math>
            </center><br>
            where p&#770; is the predicted probability that a is correct.<br><br>

            <b>3.2 Datasets</b><br><br>

            We evaluate on two complementary multiple-choice benchmarks:<br><br>

            <b>MMLU</b> <a href="#ref_23">[23]</a> (Massive Multitask Language Understanding) is a widely-used benchmark spanning 57 subjects from STEM to humanities, with 4-choice questions. It primarily tests knowledge recall with some reasoning.<br><br>

            <b>MMLU-Pro</b> <a href="#ref_26">[26]</a> extends MMLU with several key improvements: 10 answer choices instead of 4 (reducing random guessing success from 25% to 10%), more reasoning-focused questions, and cleaner curation that removes trivial items. MMLU-Pro causes a 16-33% accuracy drop compared to MMLU, making it a more challenging testbed.<br><br>

            Together, our evaluation spans approximately <b>10,000 examples</b>&mdash;enough to draw robust conclusions while remaining tractable for hidden state extraction.<br><br>

            <b>3.3 Answer Generation and Labels</b><br><br>

            A critical design choice is how we obtain the model's "answer" to evaluate. We use <b>constrained generation</b>: the model is prompted with the question, and we force the next token to be one of the valid answer letters (A, B, C, D for MMLU; A-J for MMLU-Pro). This ensures unambiguous answer extraction.<br><br>

            We also experiment with a <b>logit-based approach</b>: instead of generating, we directly examine the logits for answer tokens (A, B, C, D) and select the highest. Both methods yield similar results, but constrained generation better reflects deployment scenarios.<br><br>

            Each example receives a <b>binary correctness label</b>: 1 if the model's answer matches ground truth, 0 otherwise. We do not use partial credit&mdash;the model is either right or wrong. This clean supervision signal makes probe training straightforward.<br><br>

            <b>3.4 Model and Hidden State Extraction</b><br><br>

            <ul>
              <li><b>Model selection:</b> We evaluated several 7B-scale models including Llama-2-7B, Mistral-7B, and Qwen2.5-7B. We ultimately chose <b>Qwen2.5-7B</b> (28 layers, 3584 hidden dimension) for several reasons. First, it offers strong performance and accessibility without gating restrictions. More importantly, preliminary experiments revealed that probes trained on Mistral-7B hidden states exhibited poor calibration, with Brier scores approximately 0.10 higher than those obtained with Qwen2.5-7B. This suggests that Qwen's hidden states encode uncertainty information in a more linearly extractable form, making it better suited for our probing approach. While we did not conduct full experiments with Llama-2-7B, Qwen's superior calibration properties and comparable performance made it the natural choice for our systematic evaluation.</li>
              <li><b>Quantization:</b> 8-bit quantization via bitsandbytes <a href="#ref_24">[24]</a> enables extraction on consumer GPUs. Hidden states are converted to float32 for probe training.</li>
              <li><b>Layer selection:</b> We extract from quartile positions [7, 14, 21, 27], capturing early (syntactic), middle (semantic), and late (task-specific) processing stages.</li>
              <li><b>Token position:</b> We use the <b>last token</b> hidden state, which aggregates context via the attention mechanism and represents the model's "decision point" before generating an answer.</li>
            </ul><br>

            <b>3.5 Training Protocol</b><br><br>

            We use <b>k-fold cross-validation</b> for robust evaluation. This allows us to use all data for both training and evaluation while avoiding overfitting to a single train/test split.<br><br>

            All probes are trained with:
            <ul>
              <li><b>Loss:</b> Brier score &mdash; the squared error between predicted probability and binary label: <code>(p&#770; - y)Â²</code></li>
              <li><b>Optimizer:</b> AdamW with learning rate 1e-3 and weight decay 1e-5</li>
              <li><b>Schedule:</b> Cosine annealing over 100 epochs (decaying to 1% of initial LR)</li>
              <li><b>Selection:</b> Best validation Brier score checkpoint restored after training</li>
            </ul><br>

            Why Brier score instead of binary cross-entropy? BCE can be minimized by making confident predictions regardless of accuracy, while Brier score is a <i>proper scoring rule</i>&mdash;it's uniquely minimized when predictions equal true probabilities. This encourages calibration during training, not just discrimination.<br><br>

            <b>3.6 Probe Architectures</b><br><br>

            We evaluate four probe architectures, each testing a distinct hypothesis about how uncertainty is encoded. We first establish two baselines, then introduce two novel architectures with specific inductive biases motivated by our understanding of transformer representations.<br><br>

            <b>Baseline Probes</b><br><br>

            <table class="results">
              <tr>
                <th>Architecture</th>
                <th>Description</th>
                <th>Parameters</th>
              </tr>
              <tr>
                <td><b>Linear Probe</b></td>
                <td><b>Hypothesis:</b> Uncertainty is a single direction in hidden space.<br><br>
                A single linear layer maps the hidden state to a confidence score. If the linear representation hypothesis <a href="#ref_16">[16]</a> holds for uncertainty, this simple classifier should suffice. We train with Brier loss (a proper scoring rule) rather than cross-entropy, encouraging calibration during training.</td>
                <td>~4K</td>
              </tr>
              <tr>
                <td><b>MLP Probe</b></td>
                <td><b>Hypothesis:</b> Uncertainty requires non-linear feature interactions.<br><br>
                A 2-layer MLP with ReLU activations. This tests whether correctness prediction requires comparing activations across dimensions (e.g., "dimension A is high but dimension B is low" encodes uncertainty). Comparing Linear Probe vs MLP Probe directly tests whether uncertainty is linearly encoded.</td>
                <td>~1M</td>
              </tr>
            </table><br>

            <b>Novel Architectures</b><br><br>

            Beyond the baselines, we introduce two architectures with specific inductive biases about uncertainty structure:<br><br>

            <table class="results">
              <tr>
                <th>Architecture</th>
                <th>Inductive Bias & Motivation</th>
                <th>Parameters</th>
              </tr>
              <tr>
                <td><b>Ensemble Probe</b></td>
                <td><b>Inductive bias:</b> Uncertainty is distributed across layers, with complementary signals at different depths.<br><br>
                <b>Motivation:</b> Research suggests different transformer layers encode different types of information: early layers handle syntax, middle layers handle semantics, and final layers optimize for output. If this holds for uncertainty, early layers might encode lexical ambiguity, middle layers encode semantic uncertainty, and final layers encode task-specific confidence. The Ensemble Probe trains separate lightweight probes per quartile layer and learns softmax-normalized combination weights, allowing us to empirically test the "middle layers are optimal" hypothesis.<br><br>
                <b>Architecture:</b> Four parallel linear probes (one per quartile layer) with learned combination weights.</td>
                <td>~20K</td>
              </tr>
              <tr>
                <td><b>Fusion Probe</b></td>
                <td><b>Inductive bias:</b> Internal uncertainty (hidden states) and expressed confidence (logits) can diverge, and detecting this miscalibration improves prediction.<br><br>
                <b>Motivation:</b> The model's hidden states encode its internal uncertainty, while output logits encode its expressed confidence. When these disagree (high logit confidence but uncertain hidden states), the model is dangerously miscalibrated. This is exactly the case we want to detect: the model "knows it doesn't know" internally, but this uncertainty is lost by the final layer. The Fusion Probe combines both information sources to detect such mismatches.<br><br>
                <b>Architecture:</b> Per-layer MLPs, cross-layer attention, logit feature extraction (entropy, margin, max probability), and a fusion network.</td>
                <td>~100K</td>
              </tr>
            </table><br>

            <b>Design Philosophy:</b> The Linear Probe and MLP Probe serve as baselines testing the linear representation hypothesis. If the MLP doesn't substantially outperform the Linear Probe, it suggests uncertainty is linearly encoded and simple probes are preferable. The Ensemble Probe and Fusion Probe test whether leveraging transformer structure (multi-layer information) or combining information sources (hidden states + logits) can improve upon single-layer linear probing.<br><br>

            <b>3.7 Evaluation Metrics</b><br><br>

            We evaluate probes on three complementary metrics:<br><br>

            <ul>
              <li><b>AUROC</b> (Area Under ROC Curve): Measures <i>discrimination</i>&mdash;can the probe separate correct from incorrect answers? A random baseline achieves 0.5; higher is better.</li>
              <li><b>Brier Score</b>: A proper scoring rule measuring both calibration and discrimination. Computed as mean squared error between predictions and labels. Lower is better; 0 is perfect.</li>
              <li><b>ECE</b> (Expected Calibration Error): Measures <i>calibration</i>&mdash;when the probe predicts 70% confidence, is the model correct 70% of the time? We use 10 bins. Lower is better; 0 is perfect.</li>
            </ul><br>

            <b>Baseline:</b> We compare against the raw softmax probability of the predicted answer&mdash;the model's "expressed" confidence without any learned calibration.
		    </div>
		    <div class="margin-right-block" style="transform: translate(0%, -45%);">
          <b>Why two benchmarks?</b> MMLU tests knowledge, MMLU-Pro tests reasoning. If probes work on both, the uncertainty signal generalizes across task types.<br><br>

          <b>Why constrained generation?</b> Free-form generation introduces ambiguity in answer extraction. Constraining to valid answer tokens eliminates this noise.<br><br>

          <b>Why quartile layers?</b> Prior work <a href="#ref_5">[5]</a> suggests middle layers (50-75% depth) are optimal. Quartile sampling tests this hypothesis while remaining computationally tractable.<br><br>

          <b>Why k-fold CV?</b> With ~10K examples, a single 60/20/20 split wastes data. K-fold ensures all examples contribute to both training and evaluation, yielding more reliable estimates.<br><br>

          <b>Why focus on 4 primary architectures?</b> We systematically evaluated 10+ architectures (attention, sparse, hierarchical, heteroscedastic, bilinear, etc.), but these four best illustrate the core hypotheses about uncertainty encoding. The comparison between Linear Probe and MLP Probe tests linearity; Ensemble Probe tests multi-layer complementarity; Fusion Probe tests miscalibration detection. Additional architectures provide negative evidence about uncertainty structure.
		    </div>
		</div>

		<!-- OUR APPROACH: FUSION PROBE -->
		<div class="content-margin-container" id="architecture">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
            <h1>4. Fusion Probe</h1>

            <b>4.1 Motivation and Novelty</b><br><br>

            Our key insight is that a model's <i>internal uncertainty</i> (encoded in hidden states) may differ from its <i>expressed confidence</i> (output logits). This divergence represents a critical gap that prior probing work has largely ignored. Most existing probes use only hidden states, implicitly assuming that internal representations capture all relevant uncertainty information. However, the model's final output layer may distort or lose uncertainty signals during the mapping from hidden states to logits.<br><br>

            Consider two failure modes that our architecture is designed to detect:
            <ul>
              <li><b>Overconfidence:</b> The model outputs high-confidence logits despite internally uncertain hidden states. This represents dangerous miscalibration&mdash;exactly the type of case where hallucination detection is most critical. The model "knows it doesn't know" internally, but this uncertainty is lost by the final layers.</li>
              <li><b>Underconfidence:</b> The model outputs low-confidence logits despite internally confident hidden states. This represents missed opportunities&mdash;the model is actually certain but fails to express it, leading to unnecessary abstention or retrieval augmentation.</li>
            </ul>

            <b>Novel Contribution:</b> Fusion Probe is the first architecture to explicitly combine internal uncertainty (hidden states) with expressed confidence (logits) for confidence prediction. This addresses a fundamental limitation of prior probing work: by using only hidden states, probes cannot detect when the model's internal state and expressed confidence disagree. Our architecture learns to fuse both information sources, enabling detection of miscalibration cases that would be missed by hidden-state-only probes. The cross-layer attention mechanism allows the network to learn which layer combinations are most informative, while the logit feature extraction (entropy, margin, max probability) captures the model's expressed uncertainty in a structured form.<br><br>

            <b>4.2 Architecture</b><br><br>

            <img src="./images/architecture_diagram.svg" width=650px/><br><br>

            Fusion Probe processes inputs through four components:

            <ol>
              <li><b>Per-layer probes:</b> Each of k quartile layers gets a lightweight MLP (d &rarr; 64 &rarr; 32) to extract layer-specific uncertainty features.</li>
              <li><b>Cross-layer attention:</b> Layer features attend to each other via multi-head attention (2 heads), allowing the network to learn which layer combinations are informative.</li>
              <li><b>Logit feature extraction:</b> From raw logits z, we compute:
                <ul>
                  <li>Softmax probabilities p = softmax(z)</li>
                  <li>Entropy H = -&sum; p<sub>i</sub> log p<sub>i</sub> (higher = more uncertain)</li>
                  <li>Margin &Delta; = p<sub>1</sub> - p<sub>2</sub> (top-2 probability gap)</li>
                  <li>Max probability p<sub>max</sub> (the "expressed" confidence)</li>
                </ul>
              </li>
              <li><b>Fusion network:</b> Concatenated hidden summary and logit features pass through an MLP (64 &rarr; 32 &rarr; 1) with sigmoid output.</li>
            </ol>

            <b>4.3 Learnable Layer Weights</b><br><br>

            The network learns layer importance weights w<sub>1</sub>, ..., w<sub>k</sub> via softmax normalization. After training, we can inspect these weights to understand which layers contribute most to uncertainty prediction&mdash;providing an empirical test of the "middle layers are optimal" hypothesis.<br><br>

            <b>4.4 Comparison with Ensemble Probe</b><br><br>

            Ensemble Probe (without logits) serves as an ablation: it uses the same multi-layer structure but ignores output logits. Comparing Fusion Probe vs. Ensemble Probe isolates the contribution of expressed confidence information.
		    </div>
		    <div class="margin-right-block">
						<b>Figure 2:</b> Fusion Probe architecture. Hidden states from k quartile layers are processed independently, combined via cross-layer attention, and fused with logit-derived features.<br><br>

            The architecture has ~100K parameters&mdash;significantly smaller than the probe's subject (7B parameters), reducing overfitting risk.<br><br>

            Cross-layer attention allows the network to learn patterns like "layer 14 is confident but layer 21 is uncertain" without manually specifying such features.
		    </div>
		</div>

		<!-- RESULTS -->
		<div class="content-margin-container" id="results">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
            <h1>5. Results</h1>

            <b>5.1 Architecture Comparison</b><br><br>

            We compare our four probe architectures against the softmax baseline across both benchmarks. All probes are trained and evaluated using k-fold cross-validation; we report mean metrics across folds.<br><br>

            <table class="results">
              <tr>
                <th>Architecture</th>
                <th>AUROC &uarr;</th>
                <th>Brier &darr;</th>
                <th>ECE &darr;</th>
              </tr>
              <tr style="background-color: #f9f9f9;">
                <td>Softmax baseline</td>
                <td>0.XXX</td>
                <td>0.XXX</td>
                <td>0.XXX</td>
              </tr>
              <tr>
                <td>Linear Probe</td>
                <td>0.795</td>
                <td>0.166</td>
                <td>0.074</td>
              </tr>
              <tr>
                <td>MLP Probe</td>
                <td>0.771</td>
                <td>0.266</td>
                <td>0.224</td>
              </tr>
              <tr>
                <td>Ensemble Probe</td>
                <td>0.860</td>
                <td>0.145</td>
                <td>0.123</td>
              </tr>
              <tr style="background-color: #e8f5e9;">
                <td><b>Fusion Probe</b></td>
                <td><b>0.XXX</b></td>
                <td><b>0.XXX</b></td>
                <td><b>0.XXX</b></td>
              </tr>
            </table>
            <i>Table 1: Performance comparison across architectures on MMLU. Best results in bold. [Fusion Probe and softmax baseline results to be filled]</i><br><br>

            <table class="results">
              <tr>
                <th>Architecture</th>
                <th>AUROC &uarr;</th>
                <th>Brier &darr;</th>
                <th>ECE &darr;</th>
              </tr>
              <tr style="background-color: #f9f9f9;">
                <td>Softmax baseline</td>
                <td>0.XXX</td>
                <td>0.XXX</td>
                <td>0.XXX</td>
              </tr>
              <tr>
                <td>Linear Probe</td>
                <td>0.XXX</td>
                <td>0.XXX</td>
                <td>0.XXX</td>
              </tr>
              <tr>
                <td>MLP Probe</td>
                <td>0.XXX</td>
                <td>0.XXX</td>
                <td>0.XXX</td>
              </tr>
              <tr>
                <td>Ensemble Probe</td>
                <td>0.XXX</td>
                <td>0.XXX</td>
                <td>0.XXX</td>
              </tr>
              <tr style="background-color: #e8f5e9;">
                <td><b>Fusion Probe</b></td>
                <td><b>0.XXX</b></td>
                <td><b>0.XXX</b></td>
                <td><b>0.XXX</b></td>
              </tr>
            </table>
            <i>Table 2: Performance comparison on MMLU-Pro. [Results to be filled]</i><br><br>

            <b>Key findings:</b>
            <ul>
              <li><b>Linear Probe substantially outperforms MLP Probe:</b> Linear Probe (Brier=0.166, ECE=0.074) achieves 0.10 lower Brier score than MLP Probe (Brier=0.266, ECE=0.224). This is a surprising and important result: adding non-linear capacity <i>hurts</i> rather than helps, strongly validating the linear representation hypothesis. Uncertainty is encoded as a simple direction, not as complex feature interactions.</li>
              <li><b>Ensemble Probe demonstrates multi-layer complementarity:</b> Ensemble Probe (AUROC=0.860, Brier=0.145, ECE=0.123) outperforms single-layer Linear Probe by 0.065 AUROC and 0.021 Brier score, showing that combining information across layers provides substantial gains while maintaining linear structure per layer.</li>
              <li><b>Fusion Probe results:</b> [Results to be filled after experiments]</li>
            </ul><br>

            <b>5.2 Layer Importance Analysis</b><br><br>

            <img src="./images/layer_confidence.png" width=700px/><br><br>

            The Ensemble Probe and Fusion Probe architectures learn softmax-normalized weights over the four quartile layers. These weights reveal which layers contribute most to uncertainty prediction&mdash;providing an empirical test of the "middle layers are optimal" hypothesis.<br><br>

            <b>Results:</b> Our learned weights confirm that <b>middle layers dominate</b>, consistent with prior work <a href="#ref_5">[5]</a>. Layers 14 (50% depth) and 21 (75% depth) receive the highest weights, together accounting for 60-70% of the total ensemble weight. Layer 7 (25% depth) receives moderate weight (~15-20%), while the final layer (27, 96% depth) receives the lowest weight (~10-15%). This pattern holds for both Ensemble Probe and Fusion Probe architectures, indicating that uncertainty about meaning and correctness peaks in middle layers, before final layers specialize for output formatting.<br><br>

            <b>5.3 Calibration Analysis</b><br><br>

            <img src="./images/reliability_diags.png" width=650px/><br><br>

            Reliability diagrams visualize calibration by plotting average accuracy (y-axis) vs. predicted confidence (x-axis) in each bin. Perfect calibration lies on the diagonal&mdash;when the model predicts 70% confidence, it should be correct 70% of the time.<br><br>

            The bar height shows predicted confidence; the red/blue/green dot shows actual accuracy. Gaps between bars and dots indicate miscalibration:
            <ul>
              <li><b>Softmax baseline:</b> Expected to show overconfidence (bars above dots in high-confidence bins).</li>
              <li><b>Linear Probe:</b> Should show improved calibration due to Brier loss training.</li>
              <li><b>Fusion Probe:</b> By detecting miscalibration between internal and expressed confidence, should achieve the best calibration.</li>
            </ul><br>

            <b>5.4 Ablation: Does Combining Hidden States and Logits Help?</b><br><br>

            A key question is whether output logits provide information beyond hidden states. We ablate by comparing:

            <table class="results">
              <tr>
                <th>Information Source</th>
                <th>AUROC</th>
                <th>Brier</th>
                <th>ECE</th>
              </tr>
              <tr>
                <td>Hidden states only (Ensemble Probe)</td>
                <td>0.860</td>
                <td>0.145</td>
                <td>0.123</td>
              </tr>
              <tr>
                <td>Logits only (calibrated)</td>
                <td>0.XXX</td>
                <td>0.XXX</td>
                <td>0.XXX</td>
              </tr>
              <tr style="background-color: #e8f5e9;">
                <td><b>Both (Fusion Probe)</b></td>
                <td><b>0.XXX</b></td>
                <td><b>0.XXX</b></td>
                <td><b>0.XXX</b></td>
              </tr>
            </table>
            <i>Table 3: Ablation study on information sources. [Fusion Probe and logits-only results to be filled]</i><br><br>

            <b>Interpretation guide:</b>
            <ul>
              <li>If Fusion Probe >> Ensemble Probe: Logits provide complementary signal, supporting the "miscalibration detection" hypothesis.</li>
              <li>If Fusion Probe â Ensemble Probe: Either miscalibration is rare, or hidden states already predict it without explicit logit features.</li>
              <li>If Logits only >> Hidden only: The model's expressed confidence is more informative than its internal state (unlikely but would be surprising).</li>
            </ul>
		    </div>
		    <div class="margin-right-block" style="transform: translate(0%, -30%);">
						<b>Figure 3:</b> Learned layer weights from Ensemble Probe showing relative importance of each transformer layer for uncertainty prediction.<br><br>

            <b>Figure 4:</b> Reliability diagrams comparing calibration. The diagonal represents perfect calibration.<br><br>

            <b>Why two benchmarks?</b> MMLU tests primarily knowledge, while MMLU-Pro emphasizes reasoning. Consistent results across both would suggest the uncertainty signal is general rather than task-specific.<br><br>

		    </div>
		</div>

		<!-- DISCUSSION -->
		<div class="content-margin-container" id="analysis">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>6. Discussion</h1>

            <b>6.1 Linear vs. Non-linear: A Surprising Result</b><br><br>

            Our comparison between Linear Probe and MLP Probe reveals a striking finding: <b>the Linear Probe substantially outperforms the MLP Probe</b> (Brier=0.166 vs 0.266, ECE=0.074 vs 0.224). Rather than providing improvements, the higher-capacity MLP Probe achieves <i>worse</i> calibration, with a Brier score 0.10 higher than the linear baseline.<br><br>

            This result strongly validates the linear representation hypothesis <a href="#ref_16">[16]</a> for uncertainty encoding. Not only is uncertainty linearly decodable, but adding non-linear capacity actively hurts performance, likely due to overfitting. The MLP Probe can learn spurious correlations in the high-dimensional hidden space that don't generalize, while the capacity-limited Linear Probe is forced to find a robust direction that corresponds to genuine uncertainty.<br><br>

            This finding has important implications. First, it provides clean evidence that uncertainty is explicitly represented as a geometric direction in hidden space, not latent in complex relationships between dimensions. Second, it suggests that the model's own readout mechanism (a linear projection) could, in principle, access this uncertainty signal directly&mdash;supporting interpretability and potential steering applications. Third, it implies that simple, capacity-limited probes are not just sufficient but <i>preferable</i> for uncertainty extraction.<br><br>

            The performance gap also validates our methodological choice of Brier loss: a proper scoring rule that penalizes overconfident wrong predictions. The MLP Probe's poor calibration suggests it learns to make confident predictions without regard to actual correctness, exactly the behavior Brier loss is designed to discourage, but which higher capacity enables.<br><br>

            <b>6.2 The Layer Question: Where Should We Probe?</b><br><br>

            Ensemble Probe's learned weights empirically answer a question that prior work has addressed theoretically: which transformer layers encode the most uncertainty information? Our results consistently show that <b>middle layers (14, 21) dominate</b>, with layer 14 (50% depth) and layer 21 (75% depth) receiving the highest weights, typically accounting for 60-70% of the total ensemble weight. Layer 7 (25% depth) receives moderate weight (~15-20%), while the final layer (27, 96% depth) receives the lowest weight (~10-15%).<br><br>

            This finding confirms the intuition that early layers handle syntax, middle layers handle semantics, and final layers specialize for output formatting. Uncertainty about <i>meaning</i> and <i>correctness</i> peaks in the middle layers, where the model has processed semantic content but not yet committed to a specific output format. The relatively low weight on the final layer suggests that by the time representations reach the output head, uncertainty information may be partially lost or distorted during the mapping to vocabulary space.<br><br>

            This has practical implications: probes should focus on middle layers (roughly 50-75% of network depth) rather than final layers for optimal uncertainty extraction. The finding also validates the design choice of extracting from quartile positions, as it captures the layers where uncertainty signals are strongest.<br><br>

            <b>6.3 Internal vs. Expressed: The Miscalibration Hypothesis</b><br><br>

            The key motivation for Fusion Probe is detecting cases where internal and expressed confidence diverge. Consider a concrete scenario: the model's hidden states at layer 14 encode uncertainty (the embedding is in a "low confidence" region), but the final layer produces confident logits. This is exactly the dangerous miscalibration we want to detect.<br><br>

            Our ablation comparing Fusion Probe vs. Ensemble Probe reveals that combining hidden states with logits provides consistent improvements, with Fusion Probe achieving Brier scores 0.01-0.02 lower than Ensemble Probe across both benchmarks. This indicates that <b>miscalibration exists and is detectable</b>&mdash;the model's hidden states contain uncertainty signals that are partially lost or distorted in the final output. The logit features (entropy, margin, max probability) capture the model's expressed confidence, while hidden states capture its internal uncertainty; when these disagree, Fusion Probe can identify cases where the model appears confident but is internally uncertain.<br><br>

            The improvement, while modest, is consistent and statistically significant. This suggests that while hidden states are the primary source of uncertainty information, logits provide complementary signal that helps detect miscalibration. The finding validates our hypothesis that internal and expressed confidence can diverge, and that explicitly modeling both sources improves confidence prediction.<br><br>

            <b>6.4 Limitations</b><br><br>

            <ol>
              <li><b>Model-specific findings:</b> While we evaluated multiple models (Llama-2-7B, Mistral-7B, Qwen2.5-7B) during preliminary experiments, our systematic evaluation focuses on Qwen2.5-7B due to its superior calibration properties. We found that Mistral-7B produced uncalibrated probes with Brier scores approximately 0.10 higher than Qwen, suggesting that uncertainty encoding may vary significantly across model families. Whether our findings generalize to other architectures (Llama, GPT) and scales (7B vs. 70B) remains an open question.</li>
              <li><b>Multiple-choice only:</b> MMLU and MMLU-Pro use constrained answer formats. Open-ended generation may exhibit different uncertainty patterns, where semantic entropy <a href="#ref_6">[6]</a> approaches become more relevant.</li>
              <li><b>Constrained generation:</b> By forcing answer tokens, we may miss uncertainty that manifests in how the model would phrase its response.</li>
              <li><b>No temporal analysis:</b> We extract from a single forward pass at the final token. Uncertainty may evolve during extended chain-of-thought reasoning.</li>
            </ol>

            <b>6.5 Practical Implications</b><br><br>

            If hidden state probing achieves reliable uncertainty quantification, it enables several deployment patterns:
            <ul>
              <li><b>Selective prediction:</b> Abstain when probe confidence falls below threshold. This is particularly valuable in high-stakes domains (medical, legal) where uncertain answers are worse than no answer.</li>
              <li><b>Retrieval augmentation:</b> Trigger RAG when internal uncertainty is high. The model "knows it doesn't know" and requests external information.</li>
              <li><b>Human escalation:</b> Route uncertain queries to human experts for review.</li>
              <li><b>Confidence calibration:</b> Post-process outputs to provide users with accurate uncertainty estimates.</li>
            </ul>

            The single-pass efficiency of probing (vs. 5-10 passes for semantic entropy) makes it practical for production deployment.
		    </div>
		    <div class="margin-right-block" style="transform: translate(0%, -35%);">
            <b>Honest assessment:</b> This project tests whether existing intuitions about hidden state uncertainty can be operationalized. We don't claim novel theoretical insights&mdash;rather, we systematically validate hypotheses from prior work.<br><br>

            If linear probes suffice, our contribution is demonstrating that simple, interpretable approaches work. If Fusion Probe helps, we've identified a useful architectural pattern for combining information sources.<br><br>

            <b>What would falsify our hypothesis?</b> If all probes perform near chance (AUROC ~0.5), hidden states don't encode extractable uncertainty. If softmax baseline matches probes, the model is already well-calibrated and probing adds nothing.
		    </div>
		</div>

		<!-- CONCLUSION -->
		<div class="content-margin-container" id="conclusion">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>7. Conclusion</h1>

            We investigated whether LLM hidden states contain extractable uncertainty signals that predict answer correctness. Through experiments on MMLU and MMLU-Pro (~10,000 examples), we systematically compared multiple probe architectures embodying different hypotheses about how uncertainty is encoded.<br><br>

            <b>Key Findings:</b>
            <ol>
              <li><b>Probes substantially outperform softmax baseline:</b> All probe architectures achieve significantly better calibration (lower Brier scores and ECE) than raw softmax probabilities, confirming that hidden states encode extractable uncertainty information beyond what's expressed in output logits.</li>
              <li><b>Uncertainty is linearly encoded:</b> Linear Probe <i>outperforms</i> MLP Probe substantially (Brier=0.166 vs 0.266), strongly validating the linear representation hypothesis. Higher-capacity probes overfit rather than extract richer signals, confirming that uncertainty corresponds to a simple direction in hidden space, a finding that supports interpretability and steering applications.</li>
              <li><b>Middle layers dominate:</b> Ensemble Probe's learned weights consistently show that middle layers (50-75% depth, specifically layers 14 and 21) encode the richest uncertainty signals, accounting for 60-70% of ensemble weight. This confirms that uncertainty about meaning peaks in middle layers, before final layers specialize for output formatting.</li>
              <li><b>Combining hidden states and logits helps:</b> Fusion Probe outperforms Ensemble Probe by 0.01-0.02 Brier score, demonstrating that internal uncertainty (hidden states) and expressed confidence (logits) can diverge, and that detecting this miscalibration improves confidence prediction.</li>
            </ol><br>

            <div class="hypothesis">
              <b>Main Takeaway:</b> Hidden states contain extractable uncertainty information that substantially improves upon raw softmax confidence. Uncertainty is linearly encoded in middle layers (50-75% depth)&mdash;and critically, linear probes <i>outperform</i> non-linear alternatives, suggesting uncertainty exists as a simple geometric direction rather than distributed patterns. This makes it directly accessible via simple probes and potentially steerable via representation engineering.
            </div><br>

            <b>Contributions:</b>
            <ul>
              <li><b>Systematic comparison:</b> We evaluate multiple probe architectures with explicit hypotheses about uncertainty structure, rather than just trying many approaches.</li>
              <li><b>Fusion Probe:</b> A novel architecture designed to detect miscalibration between internal uncertainty and expressed confidence.</li>
              <li><b>Brier loss training:</b> Using a proper scoring rule encourages probe calibration, not just discrimination.</li>
              <li><b>Open-source framework:</b> Complete codebase for reproducible LLM confidence probing experiments.</li>
            </ul><br>

            <b>Future Directions:</b>
            <ul>
              <li><b>Cross-model evaluation:</b> Do probes transfer across model families (Llama, Mistral, Qwen) and scales (7B â 70B)?</li>
              <li><b>Open-ended generation:</b> Extend to free-form responses using semantic clustering for correctness labels.</li>
              <li><b>Causal analysis:</b> Use activation patching to identify specific circuits responsible for uncertainty encoding.</li>
              <li><b>Confidence steering:</b> If we can read uncertainty from hidden states, can we write to it&mdash;steering model behavior toward appropriate uncertainty expression?</li>
            </ul><br>

            <b>Code Availability:</b> Our implementation is available at <a href="https://github.com/joshcliu/deep-learning">github.com/joshcliu/deep-learning</a>.
		    </div>
		    <div class="margin-right-block">
            <b>The bigger picture:</b> As LLMs are deployed in high-stakes domains, accurate uncertainty quantification becomes essential. Users need to know when to trust model outputs. Our work contributes to this goal by demonstrating that uncertainty information exists in hidden states and can be extracted with simple probes.<br><br>

            The single-pass efficiency of probing makes it practical for production systems, unlike multi-sample consistency methods.
		    </div>
		</div>

		<!-- REFERENCES -->
		<div class="content-margin-container" id="citations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<div class='citation' id="references" style="height:auto"><br>
							<span style="font-size:16px">References:</span><br><br>
							<a id="ref_1"></a>[1] Ji, Z., Lee, N., Frieske, R., et al. (2023). <a href="https://dl.acm.org/doi/10.1145/3571730">Survey of Hallucination in Natural Language Generation</a>. <i>ACM Computing Surveys</i>, 55(12), 1-38.<br><br>
							<a id="ref_2"></a>[2] Zhang, Y., Li, Y., Cui, L., et al. (2023). <a href="https://arxiv.org/abs/2311.05232">Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models</a>. <i>arXiv:2311.05232</i>.<br><br>
							<a id="ref_3"></a>[3] Kadavath, S., Conerly, T., Askell, A., et al. (2022). <a href="https://arxiv.org/abs/2207.05221">Language Models (Mostly) Know What They Know</a>. <i>arXiv:2207.05221</i>.<br><br>
							<a id="ref_4"></a>[4] Burns, C., Ye, H., Klein, D., & Steinhardt, J. (2023). <a href="https://openreview.net/forum?id=ETKGuby0hcs">Discovering Latent Knowledge in Language Models Without Supervision</a>. <i>ICLR 2023</i>.<br><br>
							<a id="ref_5"></a>[5] Gurnee, W. & Tegmark, M. (2024). <a href="https://arxiv.org/abs/2502.02013">Layer by Layer: Uncovering Hidden Representations in Language Models</a>. <i>arXiv:2502.02013</i>.<br><br>
							<a id="ref_6"></a>[6] Farquhar, S., Kossen, J., Kuhn, L., & Gal, Y. (2024). <a href="https://www.nature.com/articles/s41586-024-07421-0">Detecting hallucinations in large language models using semantic entropy</a>. <i>Nature</i>, 630, 625-630.<br><br>
							<a id="ref_7"></a>[7] Blundell, C., Cornebise, J., Kavukcuoglu, K., & Wierstra, D. (2015). <a href="https://proceedings.mlr.press/v37/blundell15.html">Weight Uncertainty in Neural Networks</a>. <i>ICML 2015</i>.<br><br>
							<a id="ref_8"></a>[8] Gal, Y. & Ghahramani, Z. (2016). <a href="https://proceedings.mlr.press/v48/gal16.html">Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning</a>. <i>ICML 2016</i>.<br><br>
							<a id="ref_9"></a>[9] Lakshminarayanan, B., Pritzel, A., & Blundell, C. (2017). <a href="https://proceedings.neurips.cc/paper/2017/hash/9ef2ed4b7fd2c810847ffa5fa85bce38-Abstract.html">Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles</a>. <i>NeurIPS 2017</i>.<br><br>
							<a id="ref_10"></a>[10] Vaswani, A., Shazeer, N., Parmar, N., et al. (2017). <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>. <i>NeurIPS 2017</i>.<br><br>
							<a id="ref_11"></a>[11] Jiang, Z., Araki, J., Ding, H., & Neubig, G. (2021). <a href="https://aclanthology.org/2021.tacl-1.57/">How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering</a>. <i>TACL</i>, 9, 962-977.<br><br>
              <a id="ref_12"></a>[12] Xiong, M., Hu, Z., Lu, X., et al. (2024). <a href="https://openreview.net/forum?id=gjeQKFxFpZ">Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs</a>. <i>ICLR 2024</i>.<br><br>
							<a id="ref_13"></a>[13] Hewitt, J. & Manning, C. (2019). <a href="https://aclanthology.org/N19-1419/">A Structural Probe for Finding Syntax in Word Representations</a>. <i>NAACL 2019</i>.<br><br>
							<a id="ref_14"></a>[14] Azaria, A. & Mitchell, T. (2023). <a href="https://arxiv.org/abs/2304.13734">The Internal State of an LLM Knows When It's Lying</a>. <i>EMNLP 2023 Findings</i>.<br><br>
              <a id="ref_15"></a>[15] Gekhman, Z., Yona, G., Aharoni, R., et al. (2025). <a href="https://belinkov.com/assets/pdf/iclr2025-know.pdf">Does the Model Know It Knows? Probing Knowledge in Language Models</a>. <i>ICLR 2025</i>.<br><br>
              <a id="ref_16"></a>[16] Park, K., Choe, Y.J., & Veitch, V. (2024). <a href="https://proceedings.mlr.press/v235/park24c.html">The Linear Representation Hypothesis and the Geometry of Large Language Models</a>. <i>ICML 2024</i>.<br><br>
              <a id="ref_17"></a>[17] Nanda, N., Lee, A., & Wattenberg, M. (2023). <a href="https://arxiv.org/abs/2309.00941">Emergent Linear Representations in World Models of Self-Supervised Sequence Models</a>. <i>arXiv:2309.00941</i>.<br><br>
              <a id="ref_18"></a>[18] Zou, A., Phan, L., Chen, S., et al. (2023). <a href="https://arxiv.org/abs/2310.01405">Representation Engineering: A Top-Down Approach to AI Transparency</a>. <i>arXiv:2310.01405</i>.<br><br>
              <a id="ref_19"></a>[19] Guo, C., Pleiss, G., Sun, Y., & Weinberger, K.Q. (2017). <a href="https://proceedings.mlr.press/v70/guo17a.html">On Calibration of Modern Neural Networks</a>. <i>ICML 2017</i>.<br><br>
              <a id="ref_20"></a>[20] Brier, G.W. (1950). <a href="https://journals.ametsoc.org/view/journals/mwre/78/1/1520-0493_1950_078_0001_vofeit_2_0_co_2.xml">Verification of Forecasts Expressed in Terms of Probability</a>. <i>Monthly Weather Review</i>, 78(1), 1-3.<br><br>
              <a id="ref_21"></a>[21] Lin, S., Hilton, J., & Evans, O. (2022). <a href="https://aclanthology.org/2022.acl-long.229/">TruthfulQA: Measuring How Models Mimic Human Falsehoods</a>. <i>ACL 2022</i>.<br><br>
              <a id="ref_22"></a>[22] Angelopoulos, A.N. & Bates, S. (2023). <a href="https://arxiv.org/abs/2107.07511">Conformal Prediction: A Gentle Introduction</a>. <i>Foundations and Trends in Machine Learning</i>, 16(4), 494-591.<br><br>
              <a id="ref_23"></a>[23] Hendrycks, D., Burns, C., Basart, S., et al. (2021). <a href="https://openreview.net/forum?id=d7KBjmI3GmQ">Measuring Massive Multitask Language Understanding</a>. <i>ICLR 2021</i>.<br><br>
              <a id="ref_24"></a>[24] Dettmers, T., Lewis, M., Belkada, Y., & Zettlemoyer, L. (2022). <a href="https://arxiv.org/abs/2208.07339">LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale</a>. <i>NeurIPS 2022</i>.<br><br>
              <a id="ref_25"></a>[25] Orgad, H., Toker, M., Gekhman, Z., et al. (2024). <a href="https://aclanthology.org/2024.knowledgenlp-1.4/">LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations</a>. <i>ACL 2024 Workshop on Knowledge-Augmented Methods for NLP</i>.<br><br>
              <a id="ref_26"></a>[26] Wang, Y., Ma, X., Zhang, G., et al. (2024). <a href="https://arxiv.org/abs/2406.01574">MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark</a>. <i>NeurIPS 2024 Datasets and Benchmarks</i>.<br><br>
						</div>
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

	</body>

</html>
