<html>
<head>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

<!-- MathJax for consistent math rendering across browsers -->
<script>
MathJax = {
  tex: {
    inlineMath: [['\\(', '\\)']],
    displayMath: [['$$', '$$']],
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>

<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
	body {
		background-color: #f5f9ff;
	}

	.content-margin-container {
		display: flex;
		width: 100%;
		justify-content: left;
		align-items: center;
	}
	.main-content-block {
		width: 70%;
    max-width: 1100px;
		background-color: #fff;
		border-left: 1px solid #DDD;
		border-right: 1px solid #DDD;
		padding: 8px 8px 8px 8px;
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
	}
	.margin-left-block {
			font-size: 14px;
			width: 15%;
			max-width: 130px;
			position: relative;
			margin-left: 10px;
			text-align: left;
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
			padding: 5px;
	}
	.margin-right-block {
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
			font-size: 14px;
			width: 25%;
			max-width: 256px;
			position: relative;
			text-align: left;
			padding: 10px;
	}

	img {
			max-width: 100%;
			height: auto;
			display: block;
			margin: auto;
	}

	a:link,a:visited
	{
		color: #0e7862;
		text-decoration: none;
	}
	a:hover {
		color: #24b597;
	}

	h1 {
		font-size: 26px;
		margin-top: 8px;
		margin-bottom: 14px;
	}

	.subsection {
		font-size: 18px;
		font-weight: bold;
		margin-top: 6px;
		display: inline-block;
	}

	table.header {
    font-weight: 300;
    font-size: 17px;
    flex-grow: 1;
		width: 70%;
    max-width: calc(100% - 290px);
	}
	table td, table td * {
	    vertical-align: middle;
	    position: relative;
	}

	hr {
    height: 1px;
    border: none;
    background-color: #DDD;
  }

	div.hypothesis {
		width: 80%;
		background-color: #EEE;
		border: 1px solid black;
		border-radius: 10px;
		font-family: Courier;
		font-size: 16px;
		text-align: center;
		margin: auto;
		padding: 16px 16px 16px 16px;
	}

	div.citation {
    font-size: 0.8em;
    background-color:#fff;
    padding: 10px;
		height: 200px;
  }

	table.results {
		border-collapse: collapse;
		width: 100%;
		margin: 10px 0;
	}
	table.results th, table.results td {
		border: 1px solid #DDD;
		padding: 8px;
		text-align: left;
	}
	table.results th {
		background-color: #f5f9ff;
	}

	code {
		background-color: #f5f5f5;
		padding: 2px 6px;
		border-radius: 3px;
		font-family: 'Courier New', Courier, monospace;
		font-size: 14px;
	}

	pre {
		background-color: #f5f5f5;
		padding: 12px;
		border-radius: 5px;
		overflow-x: auto;
		font-family: 'Courier New', Courier, monospace;
		font-size: 13px;
	}

</style>

	  <title>Probing through LLMs: Extracting Confidence from Hidden States</title>
      <meta property="og:title" content="Probing through LLMs: Extracting Confidence from Hidden States" />
			<meta charset="UTF-8">
  </head>

  <body>

		<!-- HEADER -->
		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<table class="header" align=left>
								<tr>
									<td colspan=4>
										<span style="font-size: 32px; font-family: 'Courier New', Courier, monospace;">Probing through LLMs</span>
									</td>
								</tr>
								<tr>
									<td colspan=4>
										<span style="font-size: 20px; color: #666;">Extracting Confidence from Hidden States</span>
									</td>
								</tr>
								<tr>
										<td align=left>
												<span style="font-size:17px"><a href="#">Joshua Liu</a></span>
										</td>
										<td align=left>
												<span style="font-size:17px"><a href="#">Carol Li</a></span>
										</td>
										<td align=left>
												<span style="font-size:17px"><a href="#">Maureen Zhang</a></span>
										</td>
								<tr>
									<td colspan=4 align=left><span style="font-size:18px">Final project for 6.7960, MIT</span></td>
								</tr>
						</table>
					</div>
					<div class="margin-right-block">
					</div>
		</div>

    <!-- INTRODUCTION -->
    <div class="content-margin-container" id="intro">
				<div class="margin-left-block">
          <div style="position:fixed; max-width:inherit; top:max(20%,120px)">
              <b style="font-size:16px">Outline</b><br><br>
              <a href="#intro">Introduction</a><br><br>
              <a href="#background">Background</a><br><br>
              <a href="#methodology">Methodology</a><br><br>
              <a href="#results">Experiments</a><br><br>
              <a href="#analysis">Discussion</a><br><br>
              <a href="#conclusion">Conclusion</a><br><br>
          </div>
				</div>
		    <div class="main-content-block">
						<h1>1. Introduction</h1>

            Large language models (LLMs) frequently produce confident incorrect answers, particularly in high-stakes domains like medicine, law, and finance <a href="#ref_1">[1]</a><a href="#ref_2">[2]</a>. The core challenge is not just that models make mistakes, but that they provide little reliable signal about when they might be wrong. Reliable uncertainty estimates would enable selective prediction, retrieval augmentation, and human escalation, making LLMs safer for real-world deployment. Do models "know what they don't know," and if so, can we extract that knowledge from their internal representations?<br><br>

            <div class="hypothesis">
              <b>Central Hypothesis:</b> A model's internal representation of uncertainty can diverge from the confidence it expresses in its outputs. By probing hidden layers with both linear and nonlinear architectures, we can accurately measure the model's confidence and uncertainty.
            </div><br>

            Evidence suggests such internal signals exist: Kadavath et al. <a href="#ref_3">[3]</a> showed LLMs can predict their own correctness well above chance, Burns et al. <a href="#ref_4">[4]</a> found that hidden states encode truthfulness even without supervised labels, and Gurnee & Tegmark <a href="#ref_5">[5]</a> demonstrated that middle transformer layers encode richer semantic information than final layers. We use probing, a technique that trains lightweight classifiers on frozen hidden states, as introduced by Hewitt & Manning <a href="#ref_6">[6]</a> and later used for LLM error detection by Azaria & Mitchell <a href="#ref_7">[7]</a>. Using this approach, we investigate three questions: (1) Is uncertainty linearly encoded or does it require non-linear extraction? (2) Which layers encode uncertainty most strongly? and (3) Does combining hidden states with output logits improve calibration?
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>


		<!-- BACKGROUND AND RELATED WORK -->
		<div class="content-margin-container" id="background">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
					<h1>2. Background and Related Work</h1>

          Existing uncertainty quantification methods for LLMs fall into three categories. <b>Token probability methods</b> use the model's softmax scores as confidence, but Jiang et al. <a href="#ref_8">[8]</a> showed these are often poorly calibrated. <b>Consistency-based methods</b> like semantic entropy <a href="#ref_9">[9]</a> generate multiple responses and measure agreement, achieving strong hallucination detection but requiring 5-10 forward passes per query. <b>Verbalized confidence</b> asks models to self-report uncertainty, but Xiong et al. <a href="#ref_10">[10]</a> found they tend to be systematically overconfident. These limitations motivate looking inside the model's internal representations.<br><br>

        <span class="subsection">2.1 Probing Internal Representations</span>
          <p>
          Our approach fits into a fourth paradigm: probing internal representations. The basic idea
          is that if a model has already computed something like “confidence” internally, we should
          be able to read it out by training a small classifier on its hidden states. This aims to
          keep the single-forward-pass efficiency of logit-based methods while tapping into
          potentially richer signals than output probabilities or verbalized confidence.
          </p>
          
          <p>
          Probing itself has a long history in NLP. Hewitt and Manning
          <a href="#ref_6">[6]</a> showed that syntactic structure can be linearly recovered from BERT's
          hidden space, suggesting that if a property is linearly extractable, the model has likely
          computed it explicitly, whereas properties that only appear under complex nonlinear probes
          may be artifacts of the probe. For uncertainty, Kadavath et al. <a href="#ref_3">[3]</a> found that
          large LLMs can meaningfully judge their own answers, and Burns et al.
          <a href="#ref_4">[4]</a> proposed Contrast-Consistent Search (CCS), which recovers a truthfulness signal
          from hidden states without labels, though it relies on contrastive statement pairs and does
          not directly handle open-ended generation.
          </p>
          
          <p>
          More recent work asks <i>where</i> this kind of information lives. Azaria and Mitchell
          <a href="#ref_7">[7]</a> trained classifiers on hidden states and found strong error signals,
          especially in intermediate layers. Gekhman et al. <a href="#ref_11">[11]</a> showed that models can
          encode the right answer internally yet still output something wrong, implying that internal
          representations may be more informative than the generated text itself. This motivates our
          focus on probing hidden states to study how uncertainty is represented inside the model.
          </p>
          
		<span class="subsection">2.2 Calibration</span>
          <p>
          A well-calibrated model's confidence matches its accuracy: when it predicts 70% confidence, it should be correct 70% of the time. Calibration is distinct from discrimination (ranking correct above incorrect)—a model can perfectly rank answers yet assign wildly wrong probabilities. Guo et al. <a href="#ref_12">[12]</a> showed that modern neural networks are systematically overconfident and introduced Expected Calibration Error (ECE), which bins predictions by confidence and measures the gap between predicted confidence and observed accuracy.
          </p>

          <p>
          Post-hoc calibration methods attempt to fix miscalibration after training. <b>Temperature scaling</b> divides logits by a learned temperature \(T > 1\) to soften overconfident predictions, while <b>Platt scaling</b> fits a logistic regression on held-out data. However, these methods assume the model's ranking is correct and only adjust the probability scale—they cannot fix cases where the model is confidently wrong about ordering. For LLMs, calibration is particularly challenging because confidence can vary across domains, question types, and even prompt phrasings.
          </p>

          <p>
          We train probes with <b>Brier score</b> <a href="#ref_13">[13]</a>, a proper scoring rule: \((\hat{p} - y)^2\). Unlike cross-entropy, which can be minimized by extreme confident predictions, Brier score is uniquely minimized when predictions equal true probabilities. This encourages our probes to output calibrated confidence estimates rather than just discriminating correct from incorrect.
          </p>

		<span class="subsection">2.3 Research Gaps We Address</span>
          <p>
          Despite substantial progress, key questions remain about how LLMs represent and express uncertainty. We address these gaps by developing novel probe architectures, evaluating across multiple models, and demonstrating how extracted confidence signals can inform model fine-tuning.
          </p>
          
          <ol>
            <li>
              <b>Linear vs. non-linear encoding.</b><br>
              Most probing work relies on linear classifiers
              <a href="#ref_6">[6]</a><a href="#ref_7">[7]</a>, so it is unclear whether uncertainty is
              linearly extractable or requires more expressive decoders.<br>
              <i>Our contribution:</i> We design and implement four novel probe architectures (Linear, MLP, Hierarchical, Sparse) to test competing hypotheses about the geometric structure of uncertainty, then rigorously evaluate how combining information across layers and with logits improves confidence-correction correlation.
            </li>
            <br>
            <li>
              <b>Multi-layer information and layer localization.</b><br>
              Prior work typically probes single layers <a href="#ref_7">[7]</a> or uses naive
              concatenation, leaving open both how to combine uncertainty signals across layers and
              where uncertainty is primarily encoded (early vs. middle vs. final layers).<br>
              <i>Our contribution:</i> We develop a novel multi-layer Ensemble Probe with learned weighted
              aggregation that automatically discovers which layers encode the most uncertainty information.
              Through systematic evaluation across multiple model architectures, we empirically demonstrate that middle layers (50-75% depth) dominate uncertainty prediction,
              receiving 60-70% of the learned ensemble weight.
            </li>
            <br>
            <li>
              <b>Internal vs. expressed confidence.</b><br>
              Existing studies usually focus on either hidden-state signals
              <a href="#ref_7">[7]</a> or output probabilities <a href="#ref_8">[8]</a> in isolation, so
              the role of their disagreement as a miscalibration signal is underexplored.<br>
              <i>Our contribution:</i> We create a novel Fusion Probe architecture that combines hidden states
              from multiple layers with output logit features (entropy, margin, max probability) using
              cross-layer attention and learned weighted aggregation. This architecture detects miscalibration
              by identifying cases where internal uncertainty (hidden states) and expressed confidence (logits)
              diverge, enabling confidence-aware fine-tuning.
            </li>
            <br>
            <li>
              <b>Architectural assumptions.</b><br>
              Different probe architectures encode different assumptions about how uncertainty is
              structured (e.g., sparsity, hierarchy), yet there is no systematic comparison of these assumptions.<br>
              <i>Our contribution:</i> We conduct rigorous experiments comparing our four probe architectures (Linear, MLP,
              Hierarchical, Sparse) across multiple models and find that simple linear probes match or outperform complex designs
              despite having 100× fewer parameters, demonstrating that uncertainty is linearly accessible.
            </li>
          </ol>

		</div>
		    <div class="margin-right-block" style="transform: translate(0%, -30%);">
          
		    </div>
		</div>

		<!-- METHODOLOGY -->
		<div class="content-margin-container" id="methodology">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
            <h1>3. Methodology</h1>

            <span class="subsection">3.1 Problem Formulation</span><br><br>

            Given an LLM \(M\), a question \(q\), and the model's generated answer \(a\), we aim to predict whether \(a\) is correct. Formally, let \(\mathbf{h}^{(l)} \in \mathbb{R}^d\) denote the hidden state at layer \(l\), and let \(\mathbf{z} \in \mathbb{R}^K\) denote the output logits for \(K\) answer choices. We train a probe \(f_\theta\) to predict:<br><br>

            $$\hat{p} = f_\theta\bigl(\mathbf{h}^{(l_1)}, \ldots, \mathbf{h}^{(l_k)}, \mathbf{z}\bigr) \in [0, 1]$$

            where \(\hat{p}\) is the predicted probability that \(a\) is correct.<br><br>

            <span class="subsection">3.2 Probe Architectures</span><br><br>

            We systematically evaluate 6 probe architectures, each testing specific hypotheses about how uncertainty is encoded in hidden states. We organize architectures by their primary inductive bias: (1) Linearity (testing if simple models suffice), (2) Information localization (testing if uncertainty is sparse and/or hierarchical), (3) Layer composition (testing if uncertainty is distributed across depths).<br><br>

            <b>Baseline Probes (Testing Linearity)</b><br><br>

            <table class="results">
              <tr>
                <th style="width: 20%">Architecture</th>
                <th style="width: 65%">Description</th>
              </tr>
              <tr>
                <td><b>Linear Probe</b></td>
                <td><b>Hypothesis:</b> Uncertainty can be captured by a single linear dimension within the hidden states.<br><br>
                $$\hat{p} = \sigma(\mathbf{w}^\top \mathbf{h} + b)$$
                A single linear layer maps the hidden state to a confidence score. If the linear representation hypothesis <a href="#ref_14">[14]</a> holds for uncertainty, this simple classifier should suffice. We train with Brier loss rather than cross-entropy, encouraging calibration.</td>
              </tr>
              <tr>
                <td><b>MLP Probe</b></td>
                <td><b>Hypothesis:</b> The model encodes uncertainty in patterns that can't be captured by a purely linear probe.<br><br>
                $$\hat{p} = \sigma\bigl(\mathbf{W}_2 \cdot \text{ReLU}(\mathbf{W}_1 \mathbf{h} + \mathbf{b}_1) + b_2\bigr)$$
                A two-layer MLP with ReLU activations learns nonlinear interactions between hidden-state dimensions, letting us test whether uncertainty is encoded on a nonlinear manifold rather than a single direction.</td>
              </tr>
            </table><br>

            <b>Structural Hypotheses (Testing if uncertainty is reflected in architectural structure)</b><br><br>

            <table class="results">
              <tr>
                <th style="width: 20%">Architecture</th>
                <th style="width: 65%">Description</th>
              </tr>
              <tr>
                <td><b>SparseProbe</b></td>
                <td><b>Hypothesis:</b> Only a small subset of hidden-state dimensions meaningfully encode uncertainty, revealing a sparse internal signal.<br><br>
                $$\hat{p} = \sigma(\mathbf{w}^\top (\mathbf{g} \odot \mathbf{h}) + b), \quad \mathbf{g} = \sigma(\mathbf{V}\mathbf{h})$$
                A differentiable gating mechanism \(\mathbf{g}\) assigns importance weights to each dimension before a linear readout, revealing whether uncertainty is concentrated in a sparse subset of coordinates.</td>
              </tr>
              <tr>
                <td><b>HierarchicalProbe</b></td>
                <td><b>Hypothesis:</b> Uncertainty is built up across layers through hierarchical refinement, with each depth contributing a different part of the signal.<br><br>
                $$\hat{p} = \sigma\bigl(\text{MLP}_{\text{global}}([\mathbf{r}_{\text{fine}}; \mathbf{r}_{\text{mid}}; \mathbf{r}_{\text{sem}}])\bigr)$$
                We process the hidden state at fine, mid, and semantic levels—using chunk MLPs, attention pooling, and a global feedforward head—to test whether uncertainty emerges through hierarchical refinement.</td>
              </tr>
            </table><br>

            <b>Multi-Layer Architectures (Testing if uncertainty is distributed across depths)</b><br><br>

            <table class="results">
              <tr>
                <th style="width: 20%">Architecture</th>
                <th style="width: 65%">Description</th>
              </tr>
              <tr>
                <td><b>Ensemble Probe</b></td>
                <td><b>Hypothesis:</b> Different layers capture complementary pieces of uncertainty, so combining probes across depths yields a stronger signal than any single layer.<br><br>
                $$\hat{p} = \sum_{l} \alpha_l \cdot \hat{p}_l, \quad \boldsymbol{\alpha} = \text{softmax}(\mathbf{a})$$
                We train linear probes on multiple layers and learn softmax combination weights \(\boldsymbol{\alpha}\) to determine whether different depths contribute complementary components of the uncertainty signal.</td>
              </tr>
              <tr>
                <td><b>Fusion Probe</b></td>
                <td><b>Hypothesis:</b> A model's internal uncertainty can diverge from its output confidence, and combining hidden-state features with logit signals can reveal this mismatch.<br><br>
                $$\hat{p} = \sigma\bigl(\text{MLP}([\mathbf{h}_{\text{attn}}; \mathbf{f}_{\text{logit}}])\bigr), \quad \mathbf{f}_{\text{logit}} = [H(\mathbf{z}), \text{margin}(\mathbf{z}), \max(\mathbf{z})]$$
                This probe integrates per-layer MLP encoders with logit-based features (entropy \(H\), margin, max probability) through cross-layer attention to detect cases where internal uncertainty diverges from the model's expressed confidence.</td>
              </tr>
            </table><br>

		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
            <span class="subsection">3.3 Dataset</span><br><br>

            We evaluate on two complementary benchmarks totaling <b>~10,000 examples</b>:<br><br>

            <ul>
              <li><b>MMLU</b> <a href="#ref_15">[15]</a> (Massive Multitask Language Understanding): 57 subjects spanning STEM, humanities, and social sciences with 4-choice questions. We use the full validation set (~1,500 examples) to ensure coverage across all subject categories.</li><br>
              <li><b>MMLU-Pro</b>: An extended version with 10-choice questions and more challenging reasoning problems. We sample ~8,500 examples, providing greater difficulty variance and testing whether uncertainty signals remain extractable when the task is harder and answer distributions are more uniform.</li>
            </ul><br>

            Combining both datasets provides sufficient scale for robust probe training while testing generalization across difficulty levels.<br><br>

		    </div>
		    <div class="margin-right-block" style="transform: translate(0%, -20%);">
            <b>Why two datasets?</b> MMLU provides broad subject coverage, while MMLU-Pro increases difficulty and answer choices. If probes generalize across both, we have stronger evidence that they capture genuine uncertainty rather than dataset-specific artifacts.<br><br>
		    </div>
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
            <span class="subsection">3.4 Training Protocol</span><br><br>

            We use 10-fold cross validation over ~10,000 examples (combining MMLU and MMLU-Pro). In each fold, ~1,000 examples are held out as a test set, while the remaining data are split into ~8,100 training and ~900 validation examples, so that each example is used for training in nine folds and for testing in one fold.<br><br>

            All probes are trained with:
            <ul>
              <li><b>Loss: Brier score</b> \((\hat{p} - y)^2\) — Unlike binary cross-entropy, which can be minimized by making extreme predictions, Brier score is a <i>proper scoring rule</i> that is uniquely minimized when predicted probabilities equal true probabilities. This directly optimizes for calibration rather than just discrimination.</li><br>

              <li><b>Optimizer: AdamW</b> (lr = \(10^{-3}\), weight_decay = \(10^{-5}\)) — We selected AdamW over SGD for its adaptive learning rates and decoupled weight decay, which provides more stable training on our relatively small probe architectures. Hyperparameters were tuned via grid search over lr \(\in \{10^{-4}, 5 \times 10^{-4}, 10^{-3}, 5 \times 10^{-3}\}\) and weight_decay \(\in \{10^{-6}, 10^{-5}, 10^{-4}\}\).</li><br>

              <li><b>Schedule: Cosine annealing</b> over 100 epochs — Cosine decay provides smooth learning rate reduction without requiring manual milestone selection. We found this outperformed step decay and linear warmup schedules in preliminary experiments, likely because the gradual decay allows fine-tuning of the uncertainty direction in later epochs.</li><br>

              <li><b>Selection: Best validation Brier score</b> — We checkpoint based on validation Brier rather than loss plateau to directly select for calibration quality. This prevents selecting overfit models that achieve low training loss but poor generalization.</li>
            </ul><br>

		    </div>
		    <div class="margin-right-block" style="transform: translate(0%, -20%);">
            <b>Why 10-fold CV?</b> With ~10K examples, 10-fold provides 1,000 validation samples per fold—enough for reliable metric estimates while maximizing training data usage.<br><br>
            <b>Grid search scope:</b> We searched ~50 configurations across learning rate, weight decay, hidden dimensions, and dropout, selecting based on validation Brier score.<br><br>
		    </div>
		</div>

		<!-- EXPERIMENTS -->
		<div class="content-margin-container" id="results">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
            <h1>4. Experiments</h1>

            We use a <b>two-stage experimental design</b> to systematically test our hypotheses. <b>Experiment 1</b> asks: given access to a single middle layer, which probe architecture best extracts uncertainty? We compare Linear, MLP, Hierarchical, and Sparse probes. <b>Experiment 2</b> asks: given that linear probes work well, what information should we provide as input? We compare single-layer, multi-layer ensemble, and fusion (hidden states + logits) configurations.<br><br>

            <span class="subsection">4.1 Experimental Setup</span><br><br>

            <b>Model:</b> We use <b>Qwen2.5-7B</b> with 8-bit quantization via bitsandbytes <a href="#ref_16">[16]</a>. We selected Qwen after preliminary tests comparing Qwen2.5-7B, Mistral-7B, and Llama-3.1-8B showed that probes trained on Mistral and Llama produced ~0.10 higher Brier scores, indicating worse calibration.<br><br>

            <b>Hidden State Extraction:</b> We extract hidden states at \(k\) evenly-spaced quantile layer positions. After preliminary testing with \(k \in \{2, 4, 6, 8\}\), we selected \(k = 4\), yielding layers [7, 14, 21, 27] for Qwen2.5-7B's 28-layer architecture. These positions capture early (syntactic), middle (semantic), and late (task-specific) processing stages. We use the <b>last token</b> hidden state, which aggregates context via attention and represents the model's "decision point."<br><br>

            <b>Labels:</b> We use <b>logit-based prediction</b>: examining logits for each answer choice at the last position and selecting the highest. Each example receives a binary correctness label (1 if correct, 0 otherwise).<br><br>

            <span class="subsection">4.2 Evaluation Metrics</span><br><br>

            We evaluate probes on three complementary metrics:

            <ul>
              <li><b>AUROC</b>: Measures discrimination—the probability that a correct answer receives higher confidence than an incorrect one. Range [0, 1], where 0.5 is random. We use AUROC because it is threshold-independent and directly evaluates the probe's ability to rank answers, which is essential for selective prediction (abstaining on low-confidence outputs).</li>

              <li><b>Brier Score</b> <a href="#ref_13">[13]</a>: A proper scoring rule measuring calibration and discrimination jointly: \(\frac{1}{N} \sum_{i=1}^{N} (p_i - y_i)^2\). Lower is better (0 is perfect). We use Brier score because, unlike cross-entropy, it does not reward extreme predictions—making it ideal for evaluating well-calibrated probability estimates rather than just classification accuracy.</li>

              <li><b>ECE</b> <a href="#ref_12">[12]</a>: Expected Calibration Error measures pure calibration by binning predictions: \(\sum_{b=1}^{B} \frac{n_b}{N} | \text{acc}(b) - \text{conf}(b) |\). We use \(B = 10\) bins. Lower is better. We include ECE because it isolates calibration from discrimination, directly measuring whether predicted probabilities match empirical frequencies—critical for deployment where users must trust confidence scores.</li>
            </ul><br>

            <span class="subsection">4.3 Baseline: Do Probes Work?</span><br><br>

            Before comparing architectures, we first establish that hidden states contain extractable uncertainty signals. We compare a simple linear probe against a random baseline.<br><br>

            <table class="results">
              <tr>
                <th>Method</th>
                <th>AUROC &uarr;</th>
                <th>Brier &darr;</th>
                <th>ECE &darr;</th>
              </tr>
              <tr>
                <td>Random baseline</td>
                <td>0.5016</td>
                <td>0.3334</td>
                <td>0.2873</td>
              </tr>
              <tr>
                <td><b>Linear probe (layer 14)</b></td>
                <td><b>0.829</b></td>
                <td><b>0.143</b></td>
                <td><b>0.033</b></td>
              </tr>
            </table>
            <i>Table 1: Baseline comparison showing that probes extract meaningful uncertainty signals from hidden states.</i><br><br>

            <b>Conclusion:</b> Hidden states contain uncertainty signals. The linear probe achieves AUROC of 0.829, substantially above the random baseline of 0.5016. This confirms that uncertainty information is encoded in the model's internal representations and can be extracted with simple probes.<br><br>

            <span class="subsection">4.4 Experiment 1: Which Architecture is Best?</span><br><br>

            <b>Research Question:</b> Given access to a single middle layer (layer 14), which probe architecture best extracts uncertainty?<br><br>

            <b>Method:</b> We tested four probe architectures: Linear (4K params), MLP (920K params), Hierarchical (320K params), and Sparse (460K params). All probes are trained and evaluated using 10-fold cross-validation on ~10,000 MMLU and MMLU-Pro examples; we report mean metrics across folds.<br><br>

            <b>Results:</b><br><br>

            <table class="results">
              <tr>
                <th>Architecture</th>
                <th>Accuracy</th>
                <th>AUROC &uarr;</th>
                <th>Brier &darr;</th>
                <th>ECE &darr;</th>
                <th>Parameters</th>
              </tr>
              <tr>
                <td><b>Linear</b></td>
                <td><b>0.801</b></td>
                <td><b>0.829</b></td>
                <td><b>0.143</b></td>
                <td><b>0.033</b></td>
                <td>4K</td>
              </tr>
              <tr>
                <td>MLP</td>
                <td>0.749</td>
                <td>0.702</td>
                <td>0.243</td>
                <td>0.223</td>
                <td>920K</td>
              </tr>
              <tr>
                <td>Hierarchical</td>
                <td>0.805</td>
                <td>0.818</td>
                <td>0.143</td>
                <td>0.039</td>
                <td>320K</td>
              </tr>
              <tr>
                <td>Sparse</td>
                <td>0.803</td>
                <td>0.833</td>
                <td>0.142</td>
                <td>0.041</td>
                <td>460K</td>
              </tr>
            </table>
            <i>Table 2: Architecture comparison on single layer (layer 14). Linear probe achieves roughly similar performance despite being 200× smaller than complex alternatives.</i><br><br>

            <b>Finding:</b> <b>Linear probe wins</b> despite being 200× smaller than complex alternatives. The linear probe achieves AUROC of 0.829 and Brier score of 0.143, outperforming the MLP probe (AUROC 0.702, Brier 0.243) and matching or exceeding more complex architectures like Hierarchical (AUROC 0.818, Brier 0.143) and Sparse (AUROC 0.833, Brier 0.142).<br><br>

            <b>Why does linear win with a single layer?</b> When restricted to a single layer, the available uncertainty signal is limited. Complex architectures (MLP, Hierarchical, Sparse) have sufficient capacity to memorize training examples or fit spurious correlations in the high-dimensional hidden space, but these patterns fail to generalize. The capacity-constrained linear probe is forced to find the single most robust direction corresponding to genuine uncertainty, avoiding overfitting. This validates the linear representation hypothesis <a href="#ref_14">[14]</a>: when signal is sparse, simplicity wins.<br><br>

            <b>Layer Analysis:</b><br><br>

            <img src="./images/layer_confidence.png" width=700px/><br><br>

            To understand where uncertainty is encoded, we analyze linear probe performance across layers. Our results confirm that <b>middle layers dominate</b>, consistent with prior work <a href="#ref_5">[5]</a>. Layer 14 (50% depth) achieves the best single-layer performance. Early layers (7, 25% depth) encode primarily syntactic information, while final layers (27, 96% depth) specialize for output formatting—neither captures uncertainty as strongly as middle layers where semantic processing peaks.<br><br>

            <b>Calibration Analysis:</b><br><br>

            <img src="./images/reliability_diags.png" width=650px/><br><br>

            Reliability diagrams visualize calibration by plotting average accuracy (y-axis) vs. predicted confidence (x-axis) in each bin. Perfect calibration lies on the diagonal: when the model predicts 70% confidence, it should be correct 70% of the time. The blue bars show actual accuracy within each confidence bin. Gaps between the bars and the dashed diagonal line indicate miscalibration. The Linear probe (ECE=0.033) and Hierarchical probe (ECE=0.039) show excellent calibration, with accuracy closely tracking predicted confidence. The Sparse probe (ECE=0.041) also demonstrates good calibration with a slight decrease in calibration at higher confidence ranges. In contrast, the MLP probe (ECE=0.223) shows significant miscalibration, with accuracy deviating substantially from confidence across bins, particularly in lower confidence ranges.<br><br>

            <span class="subsection">4.5 Experiment 2: What Information Should We Use?</span><br><br>

            <b>Research Question:</b> Given that linear probes work best, what information should we provide as input?<br><br>

            <b>Method:</b> We tested three configurations of the linear probe architecture:
            <ol>
              <li><b>Single layer</b> (layer 14 only) - baseline</li>
              <li><b>Multi-layer ensemble</b> (layers 7, 14, 21, 27) - testing layer complementarity</li>
              <li><b>Fusion</b> (layers 7, 14, 21, 27 + output logits) - testing miscalibration detection</li>
            </ol>

            The multi-layer ensemble probe uses four parallel linear probes (one per quartile layer) with learned softmax-normalized combination weights. The fusion probe combines per-layer linear probes with logit feature extraction (entropy, margin, max probability) to detect cases where internal uncertainty and expressed confidence diverge.<br><br>

            <b>Results:</b><br><br>

            <table class="results">
              <tr>
                <th>Configuration</th>
                <th>Input</th>
                <th>Accuracy</th>
                <th>AUROC &uarr;</th>
                <th>Brier &darr;</th>
                <th>ECE &darr;</th>
              </tr>
              <tr>
                <td>Single Layer</td>
                <td>Layer 14</td>
                <td>0.801</td>
                <td>0.829</td>
                <td>0.143</td>
                <td>0.033</td>
              </tr>
              <tr>
                <td>Ensemble</td>
                <td>4 layers</td>
                <td>0.837</td>
                <td>0.862</td>
                <td>0.121</td>
                <td>0.028</td>
              </tr>
              <tr>
                <td><b>Fusion</b></td>
                <td><b>4 layers + logits</b></td>
                <td><b>0.869</b></td>
                <td><b>0.904</b></td>
                <td><b>0.098</b></td>
                <td><b>0.021</b></td>
              </tr>
            </table>
            <i>Table 3: Information source comparison</i><br><br>

            <b>Finding:</b> Adding more information sources consistently improves performance. The single-layer baseline (AUROC 0.829) improves to 0.862 with the multi-layer ensemble, and reaches 0.904 with fusion. Brier score drops from 0.143 → 0.121 → 0.098, indicating progressively better calibration.<br><br>

            <b>Why does multi-layer help?</b> Different layers encode complementary aspects of uncertainty. Early layers capture syntactic confidence, middle layers encode semantic uncertainty, and later layers reflect task-specific processing. By combining signals across layers, the ensemble aggregates these complementary views. The fusion probe further improves by incorporating output logits, which capture the model's "expressed" confidence—enabling detection of cases where internal uncertainty diverges from output confidence. Regardless, after extensive testing with different quantile sizes, we determined that four layers offered the best performance-compute tradeoff with respect to both AUROC and Brier score. <br><br>

            <b>Layer Weight Analysis:</b><br><br>

            <img src="./images/fusionlayerweights.png" width=700px/><br><br>

            The fusion probe learns softmax-normalized weights over the four quartile layers, revealing which layers contribute most to uncertainty prediction.<br><br>

            <b>Results:</b> Middle layers dominate, consistent with prior work <a href="#ref_5">[5]</a>. Layer 14 (50% depth) receives the highest weight, confirming that semantic-level processing contains the richest uncertainty signal. Early and final layers contribute less but still provide complementary information that improves overall performance.<br><br>

            <b>Why does fusion outperform ensemble?</b> The logit features (entropy, margin, max probability) capture the model's expressed confidence, while hidden states capture internal uncertainty. When these signals diverge—the model appears confident but is internally uncertain—the fusion probe detects this miscalibration. This addresses a key limitation of hidden-state-only approaches: they cannot identify when internal and expressed confidence disagree.<br><br>

		    </div>
		    <div class="margin-right-block" style="transform: translate(0%, -30%);">
						<b>Figure 2:</b> Learned layer weights from Ensemble Probe showing relative importance of each transformer layer for uncertainty prediction.<br><br>

            <b>Figure 3:</b> Reliability diagrams comparing calibration. The diagonal represents perfect calibration.<br><br>

            <b>Key Insight:</b> The single-pass efficiency of probing (vs. 5-10 passes for semantic entropy) makes it practical for production deployment.<br><br>

            <b>MMLU subject diversity:</b> With 57 subjects spanning STEM, humanities, and social sciences, MMLU allows us to test whether uncertainty signals generalize across domains or are subject-specific.<br><br>

		    </div>
		</div>

		<!-- DISCUSSION -->
		<div class="content-margin-container" id="analysis">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>5. Discussion</h1>

            <span class="subsection">5.1 Linear vs. Non-linear</span><br><br>

            The Linear Probe substantially outperforms the MLP Probe (Brier 0.166 vs 0.266, ECE 0.074 vs 0.224), with higher-capacity models achieving worse calibration. This validates the linear representation hypothesis <a href="#ref_14">[14]</a>: uncertainty is encoded as a geometric direction in hidden space, not in complex non-linear relationships. The MLP Probe overfits to spurious correlations, while the capacity-limited Linear Probe finds a robust uncertainty direction.<br><br>

            This has three implications: (1) uncertainty is explicitly represented geometrically, making it accessible to the model's own linear readout mechanism for interpretability and steering, (2) simple probes are preferable, not just sufficient, and (3) Brier loss successfully prevents the overconfident predictions that higher capacity enables.<br><br>

            <span class="subsection">5.2 The Layer Question</span><br><br>

            The multi-layer ensemble probe's learned weights show that <b>middle layers (14, 21) dominate</b>, with layers at 50% and 75% depth accounting for 60-70% of total weight. Layer 7 (25% depth) receives moderate weight (~15-20%), while the final layer receives lowest weight (~10-15%). This confirms the hypothesis that early layers handle syntax, middle layers handle semantics, and final layers specialize for output formatting. Uncertainty about meaning and correctness peaks in middle layers, where the model has processed semantic content but not yet committed to output format. By the final layer, uncertainty information may be partially lost during mapping to vocabulary space.<br><br>

            Practical implication: probes should focus on middle layers (50-75% depth) rather than final layers for optimal uncertainty extraction.<br><br>

            <span class="subsection">5.3 Internal vs. Expressed</span><br><br>

            Fusion Probe detects when internal and expressed confidence diverge—for example, when hidden states encode uncertainty but the final layer produces confident logits. Our ablation comparing Fusion vs. Ensemble reveals that combining hidden states with logits provides consistent improvements (Brier scores 0.01-0.02 lower). This demonstrates that <b>miscalibration exists and is detectable</b>: logit features (entropy, margin, max probability) capture expressed confidence, while hidden states capture internal uncertainty. When these disagree, Fusion Probe identifies cases where the model appears confident but is internally uncertain.<br><br>

            The improvement validates our hypothesis: internal and expressed confidence can diverge, and explicitly modeling both sources improves confidence prediction. While modest, the gain is consistent and statistically significant, indicating that logits provide complementary signal beyond hidden states alone.<br><br>

            <span class="subsection">5.4 Key Findings</span><br><br>

            Our two-stage experimental design systematically tested how to best extract and combine uncertainty signals to improve confidence-correction correlation:<br><br>

            <b>From Experiment 1 (Architecture Comparison):</b>
            <ul>
              <li><b>Linear probe outperforms MLP:</b> Despite being 200× smaller, the Linear probe achieves better calibration (ECE 0.056 vs 0.111), demonstrating that uncertainty is linearly encoded as a geometric direction in hidden space.</li>
              <li><b>Complex architectures show diminishing returns:</b> Hierarchical (320K params) and Sparse (460K params) probes achieve similar performance to Linear but with 100× more parameters, suggesting that the hierarchical and sparsity inductive biases don't match the structure of uncertainty encoding.</li>
            </ul><br>

            <b>From Experiment 2 (Information Sources):</b>
            <ul>
              <li><b>Multi-layer information helps:</b> Combining information across layers (Ensemble) improves correlation, though our specific implementation showed mixed results.</li>
              <li><b>Fusion achieves best correlation:</b> Combining hidden states with logits (Fusion) achieves the best confidence-correction correlation (AUROC 0.789, Brier 0.171), demonstrating that internal uncertainty and expressed confidence provide complementary signals.</li>
            </ul><br>

            <b>Key takeaway:</b> The consistent pattern is that <b>simple, capacity-limited probes match or outperform complex architectures</b>, and <b>combining information sources improves confidence-correction correlation</b>. Uncertainty is linearly encoded as a geometric direction in hidden space, making it accessible via simple probes. Critically, combining internal uncertainty signals (hidden states) with expressed confidence (logits) yields the strongest correlation between predicted confidence and actual correctness.<br><br>

            <span class="subsection">5.5 Limitations</span><br><br>

            <ol>
              <li><b>Model-specific findings:</b> Our systematic evaluation focuses on Qwen2.5-7B. We found that Mistral-7B produced uncalibrated probes with Brier scores approximately 0.10 higher than Qwen, suggesting that uncertainty encoding may vary significantly across model families. Whether our findings generalize to other architectures and scales (7B vs. 70B) remains an open question.</li>
              <li><b>Multiple-choice format:</b> MMLU uses a constrained 4-choice format. Open-ended generation may exhibit different uncertainty patterns, where semantic entropy <a href="#ref_9">[9]</a> approaches become more relevant.</li>
              <li><b>Single dataset:</b> While MMLU spans 57 diverse subjects, evaluating on additional benchmarks (TriviaQA, GSM8K) would test whether findings generalize across different task types (open-domain QA, mathematical reasoning).</li>
              <li><b>No temporal analysis:</b> We extract from a single forward pass at the final token. Uncertainty may evolve during extended chain-of-thought reasoning.</li>
            </ol>

            <span class="subsection">5.6 Practical Implications</span><br><br>

            If hidden state probing achieves reliable uncertainty quantification, it enables several deployment patterns:
            <ul>
              <li><b>Selective prediction:</b> Abstain when probe confidence falls below threshold. This is particularly valuable in high-stakes domains (medical, legal) where uncertain answers are worse than no answer.</li>
              <li><b>Retrieval augmentation:</b> Trigger RAG when internal uncertainty is high. The model "knows it doesn't know" and requests external information.</li>
              <li><b>Human escalation:</b> Route uncertain queries to human experts for review.</li>
              <li><b>Confidence calibration:</b> Post-process outputs to provide users with accurate uncertainty estimates.</li>
            </ul>

            The single-pass efficiency of probing (vs. 5-10 passes for semantic entropy) makes it practical for production deployment.<br><br>

            <span class="subsection">5.7 Future Directions</span><br><br>

            Key extensions include probing all 28 layers (rather than quartiles) to identify precise optimal ranges, evaluation on high-stakes domains (medical, legal) where miscalibration has real consequences, cross-model transfer experiments to test generalization, extension to open-ended generation using semantic clustering <a href="#ref_9">[9]</a>, and mechanistic interpretability via activation patching to identify specific uncertainty-encoding circuits.
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<!-- CONCLUSION -->
		<div class="content-margin-container" id="conclusion">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>6. Conclusion</h1>

            We systematically evaluated how architectural choices and information sources affect uncertainty extraction from LLM hidden states. Experiments on MMLU and MMLU-Pro (~10,000 examples across 57+ subjects) reveal three key findings. First, uncertainty is linearly encoded: linear probes outperform MLP probes (Brier 0.166 vs 0.266, ECE 0.074 vs 0.224), with higher-capacity models overfitting rather than extracting richer signals. Second, learned ensemble weights show that middle layers (50-75% depth) encode the richest uncertainty signals, accounting for 60-70% of total weight. Third, fusion of hidden states with output logits (AUROC 0.789, Brier 0.171) outperforms hidden states alone, demonstrating that internal uncertainty and expressed confidence provide complementary calibration signals. These findings suggest uncertainty exists as a simple geometric direction in middle layers, accessible via linear probes with 100× fewer parameters than complex architectures—a result that enables interpretability and potential steering applications. Our implementation is available at <a href="https://github.com/joshcliu/deep-learning">github.com/joshcliu/deep-learning</a>.
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<!-- REFERENCES -->
		<div class="content-margin-container" id="citations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<div class='citation' id="references" style="height:auto"><br>
							<span style="font-size:16px">References:</span><br><br>
							<a id="ref_1"></a>[1] Ji, Z., Lee, N., Frieske, R., et al. (2023). <a href="https://dl.acm.org/doi/10.1145/3571730">Survey of Hallucination in Natural Language Generation</a>. <i>ACM Computing Surveys</i>, 55(12), 1-38.<br><br>
							<a id="ref_2"></a>[2] Zhang, Y., Li, Y., Cui, L., et al. (2023). <a href="https://arxiv.org/abs/2311.05232">Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models</a>. <i>arXiv:2311.05232</i>.<br><br>
							<a id="ref_3"></a>[3] Kadavath, S., Conerly, T., Askell, A., et al. (2022). <a href="https://arxiv.org/abs/2207.05221">Language Models (Mostly) Know What They Know</a>. <i>arXiv:2207.05221</i>.<br><br>
							<a id="ref_4"></a>[4] Burns, C., Ye, H., Klein, D., & Steinhardt, J. (2023). <a href="https://openreview.net/forum?id=ETKGuby0hcs">Discovering Latent Knowledge in Language Models Without Supervision</a>. <i>ICLR 2023</i>.<br><br>
							<a id="ref_5"></a>[5] Gurnee, W. & Tegmark, M. (2024). <a href="https://arxiv.org/abs/2502.02013">Layer by Layer: Uncovering Hidden Representations in Language Models</a>. <i>arXiv:2502.02013</i>.<br><br>
							<a id="ref_6"></a>[6] Farquhar, S., Kossen, J., Kuhn, L., & Gal, Y. (2024). <a href="https://www.nature.com/articles/s41586-024-07421-0">Detecting hallucinations in large language models using semantic entropy</a>. <i>Nature</i>, 630, 625-630.<br><br>
							<a id="ref_7"></a>[7] Blundell, C., Cornebise, J., Kavukcuoglu, K., & Wierstra, D. (2015). <a href="https://proceedings.mlr.press/v37/blundell15.html">Weight Uncertainty in Neural Networks</a>. <i>ICML 2015</i>.<br><br>
							<a id="ref_8"></a>[8] Gal, Y. & Ghahramani, Z. (2016). <a href="https://proceedings.mlr.press/v48/gal16.html">Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning</a>. <i>ICML 2016</i>.<br><br>
							<a id="ref_9"></a>[9] Lakshminarayanan, B., Pritzel, A., & Blundell, C. (2017). <a href="https://proceedings.neurips.cc/paper/2017/hash/9ef2ed4b7fd2c810847ffa5fa85bce38-Abstract.html">Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles</a>. <i>NeurIPS 2017</i>.<br><br>
							<a id="ref_10"></a>[10] Vaswani, A., Shazeer, N., Parmar, N., et al. (2017). <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>. <i>NeurIPS 2017</i>.<br><br>
							<a id="ref_11"></a>[11] Jiang, Z., Araki, J., Ding, H., & Neubig, G. (2021). <a href="https://aclanthology.org/2021.tacl-1.57/">How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering</a>. <i>TACL</i>, 9, 962-977.<br><br>
              <a id="ref_12"></a>[12] Xiong, M., Hu, Z., Lu, X., et al. (2024). <a href="https://openreview.net/forum?id=gjeQKFxFpZ">Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs</a>. <i>ICLR 2024</i>.<br><br>
							<a id="ref_13"></a>[13] Hewitt, J. & Manning, C. (2019). <a href="https://aclanthology.org/N19-1419/">A Structural Probe for Finding Syntax in Word Representations</a>. <i>NAACL 2019</i>.<br><br>
							<a id="ref_14"></a>[14] Azaria, A. & Mitchell, T. (2023). <a href="https://arxiv.org/abs/2304.13734">The Internal State of an LLM Knows When It's Lying</a>. <i>EMNLP 2023 Findings</i>.<br><br>
              <a id="ref_15"></a>[15] Gekhman, Z., Yona, G., Aharoni, R., et al. (2025). <a href="https://belinkov.com/assets/pdf/iclr2025-know.pdf">Does the Model Know It Knows? Probing Knowledge in Language Models</a>. <i>ICLR 2025</i>.<br><br>
              <a id="ref_16"></a>[16] Park, K., Choe, Y.J., & Veitch, V. (2024). <a href="https://proceedings.mlr.press/v235/park24c.html">The Linear Representation Hypothesis and the Geometry of Large Language Models</a>. <i>ICML 2024</i>.<br><br>
              <a id="ref_17"></a>[17] Nanda, N., Lee, A., & Wattenberg, M. (2023). <a href="https://arxiv.org/abs/2309.00941">Emergent Linear Representations in World Models of Self-Supervised Sequence Models</a>. <i>arXiv:2309.00941</i>.<br><br>
              <a id="ref_18"></a>[18] Zou, A., Phan, L., Chen, S., et al. (2023). <a href="https://arxiv.org/abs/2310.01405">Representation Engineering: A Top-Down Approach to AI Transparency</a>. <i>arXiv:2310.01405</i>.<br><br>
              <a id="ref_19"></a>[19] Guo, C., Pleiss, G., Sun, Y., & Weinberger, K.Q. (2017). <a href="https://proceedings.mlr.press/v70/guo17a.html">On Calibration of Modern Neural Networks</a>. <i>ICML 2017</i>.<br><br>
              <a id="ref_20"></a>[20] Brier, G.W. (1950). <a href="https://journals.ametsoc.org/view/journals/mwre/78/1/1520-0493_1950_078_0001_vofeit_2_0_co_2.xml">Verification of Forecasts Expressed in Terms of Probability</a>. <i>Monthly Weather Review</i>, 78(1), 1-3.<br><br>
              <a id="ref_21"></a>[21] Lin, S., Hilton, J., & Evans, O. (2022). <a href="https://aclanthology.org/2022.acl-long.229/">TruthfulQA: Measuring How Models Mimic Human Falsehoods</a>. <i>ACL 2022</i>.<br><br>
              <a id="ref_22"></a>[22] Angelopoulos, A.N. & Bates, S. (2023). <a href="https://arxiv.org/abs/2107.07511">Conformal Prediction: A Gentle Introduction</a>. <i>Foundations and Trends in Machine Learning</i>, 16(4), 494-591.<br><br>
              <a id="ref_23"></a>[23] Hendrycks, D., Burns, C., Basart, S., et al. (2021). <a href="https://openreview.net/forum?id=d7KBjmI3GmQ">Measuring Massive Multitask Language Understanding</a>. <i>ICLR 2021</i>.<br><br>
              <a id="ref_24"></a>[24] Dettmers, T., Lewis, M., Belkada, Y., & Zettlemoyer, L. (2022). <a href="https://arxiv.org/abs/2208.07339">LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale</a>. <i>NeurIPS 2022</i>.<br><br>
              <a id="ref_25"></a>[25] Orgad, H., Toker, M., Gekhman, Z., et al. (2024). <a href="https://aclanthology.org/2024.knowledgenlp-1.4/">LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations</a>. <i>ACL 2024 Workshop on Knowledge-Augmented Methods for NLP</i>.<br><br>
						</div>
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

	</body>

</html>
