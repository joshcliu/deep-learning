<html>
<head>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

<!-- MathJax for consistent math rendering across browsers -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/mml-chtml.js"></script>

<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
	body {
		background-color: #f5f9ff;
	}

	.content-margin-container {
		display: flex;
		width: 100%;
		justify-content: left;
		align-items: center;
	}
	.main-content-block {
		width: 70%;
    max-width: 1100px;
		background-color: #fff;
		border-left: 1px solid #DDD;
		border-right: 1px solid #DDD;
		padding: 8px 8px 8px 8px;
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
	}
	.margin-left-block {
			font-size: 14px;
			width: 15%;
			max-width: 130px;
			position: relative;
			margin-left: 10px;
			text-align: left;
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
			padding: 5px;
	}
	.margin-right-block {
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
			font-size: 14px;
			width: 25%;
			max-width: 256px;
			position: relative;
			text-align: left;
			padding: 10px;
	}

	img {
			max-width: 100%;
			height: auto;
			display: block;
			margin: auto;
	}

	a:link,a:visited
	{
		color: #0e7862;
		text-decoration: none;
	}
	a:hover {
		color: #24b597;
	}

	h1 {
		font-size: 18px;
		margin-top: 4px;
		margin-bottom: 10px;
	}

	table.header {
    font-weight: 300;
    font-size: 17px;
    flex-grow: 1;
		width: 70%;
    max-width: calc(100% - 290px);
	}
	table td, table td * {
	    vertical-align: middle;
	    position: relative;
	}

	hr {
    height: 1px;
    border: none;
    background-color: #DDD;
  }

	div.hypothesis {
		width: 80%;
		background-color: #EEE;
		border: 1px solid black;
		border-radius: 10px;
		font-family: Courier;
		font-size: 16px;
		text-align: center;
		margin: auto;
		padding: 16px 16px 16px 16px;
	}

	div.citation {
    font-size: 0.8em;
    background-color:#fff;
    padding: 10px;
		height: 200px;
  }

	table.results {
		border-collapse: collapse;
		width: 100%;
		margin: 10px 0;
	}
	table.results th, table.results td {
		border: 1px solid #DDD;
		padding: 8px;
		text-align: left;
	}
	table.results th {
		background-color: #f5f9ff;
	}

	code {
		background-color: #f5f5f5;
		padding: 2px 6px;
		border-radius: 3px;
		font-family: 'Courier New', Courier, monospace;
		font-size: 14px;
	}

	pre {
		background-color: #f5f5f5;
		padding: 12px;
		border-radius: 5px;
		overflow-x: auto;
		font-family: 'Courier New', Courier, monospace;
		font-size: 13px;
	}

</style>

	  <title>Probing the Mind of LLMs: Extracting Confidence from Hidden States</title>
      <meta property="og:title" content="Probing the Mind of LLMs: Extracting Confidence from Hidden States" />
			<meta charset="UTF-8">
  </head>

  <body>

		<!-- HEADER -->
		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<table class="header" align=left>
								<tr>
									<td colspan=4>
										<span style="font-size: 32px; font-family: 'Courier New', Courier, monospace;">Probing the Mind of LLMs</span>
									</td>
								</tr>
								<tr>
									<td colspan=4>
										<span style="font-size: 20px; color: #666;">Extracting Confidence from Hidden States</span>
									</td>
								</tr>
								<tr>
										<td align=left>
												<span style="font-size:17px"><a href="#">Joshua Liu</a></span>
										</td>
										<td align=left>
												<span style="font-size:17px"><a href="#">Carol Li</a></span>
										</td>
										<td align=left>
												<span style="font-size:17px"><a href="#">Maureen Zhang</a></span>
										</td>
								<tr>
									<td colspan=4 align=left><span style="font-size:18px">Final project for 6.7960, MIT</span></td>
								</tr>
						</table>
					</div>
					<div class="margin-right-block">
					</div>
		</div>

    <!-- INTRODUCTION -->
    <div class="content-margin-container" id="intro">
				<div class="margin-left-block">
          <div style="position:fixed; max-width:inherit; top:max(20%,120px)">
              <b style="font-size:16px">Outline</b><br><br>
              <a href="#intro">Introduction</a><br><br>
              <a href="#background">Background</a><br><br>
              <a href="#methodology">Methodology</a><br><br>
              <a href="#architecture">Experiments</a><br><br>
              <a href="#analysis">Discussion</a><br><br>
              <a href="#conclusion">Conclusion</a><br><br>
          </div>
				</div>
		    <div class="main-content-block">
						<h1>1. Introduction</h1>

            Imagine asking an AI assistant about a medication interaction and getting a fluent, confident, citation-filled answer, only to later discover that the information was completely wrong. This is not a rare edge case but a routine behavior of large language models (LLMs), which often produce plausible-sounding falsehoods with the same apparent confidence as established facts <a href="#ref_1">[1]</a>. These “hallucinations” are especially concerning in high-stakes domains like law, medicine, and science, where fabricated precedents, incorrect dosages, or fictional citations can have real consequences <a href="#ref_2">[2]</a>. The problem is not just that models make mistakes; it is that they offer little reliable signal about when they are likely to be wrong.<br><br>

            This raises a natural question: can models “know what they don’t know,” and if so, can we access that knowledge through their internal representations rather than their surface behavior?<br><br>

            <div class="hypothesis">
              <b>Central Hypothesis:</b> Hidden states contain uncertainty signals that can be extracted to better correlate predicted confidence with actual correctness. By probing hidden layers and combining internal uncertainty signals with expressed confidence from logits, we can improve the correlation between model confidence and answer correctness.
            </div><br>

            There is growing evidence that such internal signals exist. Kadavath et al. <a href="#ref_3">[3]</a> showed that LLMs perform above chance when asked to predict whether their own answers are correct. Burns et al. <a href="#ref_4">[4]</a> found that hidden states alone can separate true from false statements, suggesting that something like “truthfulness” is already encoded internally. Layer-wise studies further indicate that middle transformer layers encode richer semantic information than final layers <a href="#ref_5">[5]</a>, hinting that the model’s “thinking” may peak in the middle, with later layers mostly formatting outputs.<br><br>

            What remains unclear is how uncertainty is organized inside the network and how best to extract it. Is uncertainty linearly encoded, or does it require more expressive probes? Is it localized to specific layers or distributed across many? And how does internal uncertainty relate to the model’s expressed confidence at the logits?<br><br>

            In this work, we take a systematic probing-based approach to these questions, comparing linear and nonlinear probes across layers and models, and exploring how internal signals interact with output-based confidence.
		    </div>
		    <div class="margin-right-block">
						The hallucination problem is particularly insidious because users cannot easily distinguish confident correct responses from confident incorrect ones without external verification.<br><br>

            <b>Why does this matter?</b> Reliable uncertainty estimates enable selective prediction (abstaining on hard cases), retrieval augmentation (calling external knowledge when needed), and human-in-the-loop workflows (routing dubious answers to experts), making LLMs safer and more trustworthy in real-world use.
		    </div>
		</div>


		<!-- BACKGROUND AND RELATED WORK -->
		<div class="content-margin-container" id="background">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
					<h1>2. Background and Related Work</h1>

          Before diving into our approach, it's worth understanding where uncertainty quantification has been and why the problem remains challenging.<br><br>

        <b>2.1 Classical Approaches</b>
          <p>
             Before the rise of modern large language models, researchers had already been grappling
             with a central question in machine learning: <i>how can we quantify a model’s uncertainty?</i>
             Three classical approaches emerged:
             </p>
             
             <ul>
             <li>
             	<b>Bayesian neural networks</b> <a href="#ref_7">[7]</a> place distributions over weights
             	instead of using point estimates, so uncertainty is obtained from the posterior.
             </li>
             <li>
             	<b>Monte Carlo dropout</b> <a href="#ref_8">[8]</a> keeps dropout active at test time,
             	runs the model multiple times, and uses the variation across predictions as an
             	uncertainty estimate.
             </li>
             <li>
             	<b>Deep ensembles</b> <a href="#ref_9">[9]</a> train several independent models and
             	quantify uncertainty via their disagreement.
             </li>
             </ul>
             
             <p>
             All three methods effectively treat uncertainty as dispersion across predictions. They work
             well for smaller networks, but at the scale of billion-parameter LLMs, training or sampling
             many models becomes prohibitively expensive, motivating single-model, single-forward-pass
             approaches to uncertainty.
             </p>
             
             
        <b>2.2 Uncertainty in the Age of LLMs</b>
          <p>
          The transformer revolution changed the game: LLMs are so large that
          classical ensemble-style methods are often impractical, and the connection between token
          probabilities and factual correctness is surprisingly weak. In practice, three main families
          of approaches have been explored.
          </p>
          
          <p><b>Token probability methods</b> treat the model’s softmax scores as confidence: if the model assigns high probability
          to an answer, we interpret that as high certainty. Jiang et al.
          <a href="#ref_11">[11]</a> showed that these probabilities are often poorly calibrated, with models
          assigning very high probability to incorrect answers, especially in open-ended settings.
          This gap between next-token confidence and actual factual correctness motivates looking
          inside the model rather than only at its output distribution.
          </p>
          
          <p><b>Consistency-based methods</b> generate multiple responses to the same prompt and measure how much they agree.
          Kuhn et al. <a href="#ref_6">[6]</a> introduced semantic entropy, which first clusters responses by
          meaning and then computes entropy over those clusters, achieving strong hallucination
          detection. The downside is computational cost (typically 5-10 forward passes per query) and
          the fact that it mainly captures uncertainty about <i>what to say</i>, whereas our work targets
          uncertainty about <i>being correct</i> in a single forward pass.
          </p>
          
          <p><b>Verbalized confidence methods</b> typically ask the model how confident it is and treat its self-report as
          an uncertainty estimate. Xiong et al. <a href="#ref_12">[12]</a> found that while LLMs can express
          graded confidence, they tend to be systematically overconfident and sensitive to prompt
          phrasing. This mismatch between expressed and actual uncertainty raises the question of
          whether internal representations carry a more reliable confidence signal than the model’s
          own words.
          </p>

        <b>2.3 Probing Internal Representations</b>
          <p>
          Our approach fits into a fourth paradigm: probing internal representations. The basic idea
          is that if a model has already computed something like “confidence” internally, we should
          be able to read it out by training a small classifier on its hidden states. This aims to
          keep the single-forward-pass efficiency of logit-based methods while tapping into
          potentially richer signals than output probabilities or verbalized confidence.
          </p>
          
          <p>
          Probing itself has a long history in NLP. Hewitt and Manning
          <a href="#ref_13">[13]</a> showed that syntactic structure can be linearly recovered from BERT’s
          hidden space, suggesting that if a property is linearly extractable, the model has likely
          computed it explicitly, whereas properties that only appear under complex nonlinear probes
          may be artifacts of the probe. For uncertainty, Kadavath et al. <a href="#ref_3">[3]</a> found that
          large LLMs can meaningfully judge their own answers, and Burns et al.
          <a href="#ref_4">[4]</a> proposed Contrast-Consistent Search (CCS), which recovers a truthfulness signal
          from hidden states without labels, though it relies on contrastive statement pairs and does
          not directly handle open-ended generation.
          </p>
          
          <p>
          More recent work asks <i>where</i> this kind of information lives. Azaria and Mitchell
          <a href="#ref_14">[14]</a> trained classifiers on hidden states and found strong error signals,
          especially in intermediate layers. Gekhman et al. <a href="#ref_15">[15]</a> showed that models can
          encode the right answer internally yet still output something wrong, implying that internal
          representations may be more informative than the generated text itself. This motivates our
          focus on probing hidden states to study how uncertainty is represented inside the model.
          </p>
          
		<b>2.4 Calibration: When Confidence Matches Reality</b>
          <p>
          A model is well-calibrated if its stated confidence matches its accuracy: when it predicts
          70% confidence, it should be correct about 70% of the time. This is critical in deployment
          settings, where users need to know when to trust model outputs.
          </p>
          
          <p>
          Guo et al. <a href="#ref_19">[19]</a> showed that modern neural networks are often
          overconfident and introduced temperature scaling and Expected Calibration Error (ECE) as
          standard tools for post-hoc calibration. However, ECE can be misleading in simple cases
          (e.g., constant-confidence predictions), since a model can appear well-calibrated without
          providing informative uncertainty.
          </p>
          
          <p>
          The Brier score <a href="#ref_20">[20]</a> addresses this by acting as a proper scoring rule,
          minimized only when predicted probabilities match true frequencies. It penalizes both being
          wrong and being confidently wrong. In our work, we train probes with Brier loss so they
          learn calibrated probabilities rather than just accurate binary predictions, making them
          directly comparable to other uncertainty quantification methods.
          </p>

		<b>2.5 Research Gaps We Address</b>
          <p>
          Despite substantial progress, key questions remain about how LLMs represent and express uncertainty.
          </p>
          
          <ol>
            <li>
              <b>Linear vs. non-linear encoding.</b><br>
              Most probing work relies on linear classifiers
              <a href="#ref_13">[13]</a><a href="#ref_14">[14]</a>, so it is unclear whether uncertainty is
              linearly extractable or requires more expressive decoders.<br>
              <i>Our contribution:</i> We compare four probe architectures (Linear, MLP, Hierarchical, Sparse) to test competing hypotheses about the geometric structure of uncertainty in hidden states, then evaluate how combining information across layers and with logits improves confidence-correction correlation.
            </li>
            <br>
            <li>
              <b>Multi-layer information.</b><br>
              Prior work typically probes single layers <a href="#ref_14">[14]</a> or uses naive
              concatenation, leaving open how uncertainty signals spread across early, middle, and late
              layers.<br>
              <i>Our contribution:</i> We evaluate four fusion strategies: concatenation, learned weighted
              ensembles, attention-based aggregation, and combining hidden states with logits, to identify
              how to best aggregate cross-layer uncertainty information.
            </li>
            <br>
            <li>
              <b>Internal vs. expressed confidence.</b><br>
              Existing studies usually focus on either hidden-state signals
              <a href="#ref_14">[14]</a> or output probabilities <a href="#ref_11">[11]</a> in isolation, so
              the role of their disagreement as a miscalibration signal is underexplored.<br>
              <i>Our contribution:</i> We introduce a Fusion Probe that jointly uses hidden-state features
              and logit-based confidence, and show that their interaction yields our strongest
              miscalibration detection performance.
            </li>
            <br>
            <li>
              <b>Architectural assumptions.</b><br>
              Different probe architectures encode different assumptions about how uncertainty is
              structured (e.g., sparsity, hierarchy), yet there is no systematic comparison of these assumptions.<br>
              <i>Our contribution:</i> We run a broad architectural study and find that simple,
              capacity-limited probes often match or outperform more complex designs, suggesting that
              uncertainty behaves as a low-dimensional, largely linearly accessible feature.
            </li>
            <br>
            <li>
              <b>Where is uncertainty encoded? (The Layer Question).</b><br>
              While prior work indicates that intermediate layers carry rich semantic information
              <a href="#ref_5">[5]</a>, it is unclear whether uncertainty in particular is concentrated there
              or closer to the output layers.<br>
              <i>Our contribution:</i> By probing all 32 layers and learning ensemble weights over them,
              we show that middle layers dominate uncertainty prediction, indicating where uncertainty is
              primarily encoded in our model.
            </li>
          </ol>

		</div>
		    <div class="margin-right-block" style="transform: translate(0%, -30%);">
          
		    </div>
		</div>

		<!-- METHODOLOGY -->
		<div class="content-margin-container" id="methodology">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
            <h1>3. Methodology</h1>

            <b>3.1 Problem Formulation</b><br><br>

            Given an LLM M, a question q, and the model's generated answer a, we aim to predict whether a is correct. Formally, let h<sup>(l)</sup> &in; &#x211D;<sup>d</sup> denote the hidden state at layer l, and let z &in; &#x211D;<sup>K</sup> denote the output logits for K answer choices. We train a probe f<sub>&theta;</sub> to predict:<br><br>
            <center>
              <math xmlns="http://www.w3.org/1998/Math/MathML">
                <mrow>
                  <mover>
                    <mi>p</mi>
                    <mo>^</mo>
                  </mover>
                  <mo>=</mo>
                  <msub>
                    <mi>f</mi>
                    <mi>&theta;</mi>
                  </msub>
                  <mo>(</mo>
                  <msup><mi>h</mi><mrow><mo>(</mo><msub><mi>l</mi><mn>1</mn></msub><mo>)</mo></mrow></msup>
                  <mo>,</mo>
                  <mo>...</mo>
                  <mo>,</mo>
                  <msup><mi>h</mi><mrow><mo>(</mo><msub><mi>l</mi><mi>k</mi></msub><mo>)</mo></mrow></msup>
                  <mo>,</mo>
                  <mi>z</mi>
                  <mo>)</mo>
                  <mo>&isin;</mo>
                  <mo>[</mo>
                  <mn>0</mn>
                  <mo>,</mo>
                  <mn>1</mn>
                  <mo>]</mo>
                </mrow>
              </math>
            </center><br>
            where p&#770; is the predicted probability that a is correct.<br><br>

            <b>3.2 Datasets</b><br><br>

            We evaluate on the widely-used <b>MMLU</b> <a href="#ref_23">[23]</a> (Massive Multitask Language Understanding) benchmark, spanning 57 subjects from STEM to humanities with 4-choice questions. It tests a combination of knowledge recall and reasoning across diverse domains, making it suitable for evaluating whether uncertainty signals generalize across subject matter.<br><br>

            We use the MMLU validation set, sampling <b>2,000 examples</b> for our experiments. This provides sufficient data for robust probe training while remaining tractable for hidden state extraction.<br><br>

            <b>3.3 Answer Generation and Labels</b><br><br>

            A critical design choice is how we obtain the model's "answer" to evaluate. We experiment with two approaches:<br><br>

            <b>Logit-based prediction:</b> We examine the logits for answer tokens (A, B, C, D) at the last token position and select the highest. This measures the model's "internal preference" among answer choices.<br><br>

            <b>Generation-based prediction:</b> We prompt the model to generate an answer and parse the output. This better reflects real-world deployment but introduces parsing complexity.<br><br>

            Each example receives a <b>binary correctness label</b>: 1 if the model's answer matches ground truth, 0 otherwise. We do not use partial credit; the model is either right or wrong. This clean supervision signal makes probe training straightforward.<br><br>

            <b>3.4 Model and Hidden State Extraction</b><br><br>

            <ul>
              <li><b>Model selection:</b> We evaluated Mistral-7B and Qwen2.5-7B. We chose Qwen2.5-7B (28 layers, 3584 hidden dimension) because probes trained on its hidden states yielded Brier scores approximately 0.10 lower than those trained on Mistral-7B hidden states.</li>
              <li><b>Quantization:</b> 8-bit quantization via bitsandbytes <a href="#ref_24">[24]</a> enables extraction on consumer GPUs. Hidden states are converted to float32 for probe training.</li>
              <li><b>Layer selection:</b> We extract from quartile positions [7, 14, 21, 27], capturing early (syntactic), middle (semantic), and late (task-specific) processing stages.</li>
              <li><b>Token position:</b> We use the <b>last token</b> hidden state, which aggregates context via the attention mechanism and represents the model's "decision point" before generating an answer.</li>
            </ul><br>

		    </div>
		    <div class="margin-right-block" style="transform: translate(0%, -20%);">
          <b>Model selection:</b> We compared Mistral-7B and Qwen2.5-7B. Qwen2.5-7B was selected because probes trained on its hidden states achieved Brier scores approximately 0.10 lower than Mistral-7B, indicating superior calibration properties.<br><br>
		    </div>
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
            <b>3.5 Training Protocol</b><br><br>

            We use <b>5 cross-validation</b> for robust evaluation, training on 1,200 examples and validating on 300 examples in each fold. This allows us to use all data for both training and evaluation while avoiding overfitting to a single train/test split.<br><br>

            All probes are trained with: 
            <ul>
              <li><b>Loss:</b> Brier score (squared error between predicted probability and binary label):
                <math xmlns="http://www.w3.org/1998/Math/MathML" style="display: inline-block; vertical-align: middle; margin-left: 8px;">
                  <msup>
                    <mrow>
                      <mo>(</mo>
                      <mover><mi>p</mi><mo>^</mo></mover>
                      <mo>-</mo>
                      <mi>y</mi>
                      <mo>)</mo>
                    </mrow>
                    <mn>2</mn>
                  </msup>
                </math>
              </li>
              <li><b>Optimizer:</b> AdamW with learning rate 1e-3 and weight decay 1e-5</li>
              <li><b>Schedule:</b> Cosine annealing over 100 epochs (decaying to 1% of initial LR)</li>
              <li><b>Selection:</b> Best validation Brier score checkpoint restored after training</li>
            </ul><br>

            Why Brier score instead of binary cross-entropy? BCE can be minimized by making confident predictions regardless of accuracy, while Brier score is a <i>proper scoring rule</i>: it's uniquely minimized when predictions equal true probabilities. This encourages calibration during training, not just discrimination.<br><br>

            COMMENT: add a note about what baseline we are comparing our probes to
            
            <b>3.6 Probe Architectures</b><br><br>

            We systematically evaluate 6 probe architectures, each testing specific hypotheses about how uncertainty is encoded in hidden states. These range from simple linear classifiers to complex networks with attention mechanisms and hierarchical processing. We organize them by the hypothesis they test. Some architectures (e.g., hierarchical models) process representations from multiple layers jointly, while single-layer probes operate on each layer independently.<br><br>

            <b>Baseline Probes (Testing Linearity)</b><br><br>

            <table class="results">
              <tr>
                <th style="width: 20%">Architecture</th>
                <th style="width: 65%">Hypothesis & Description</th>
                <th style="width: 15%">Parameters</th>
              </tr>
              <tr>
                <td><b>Linear Probe</b></td>
                <td><b>Hypothesis:</b> Uncertainty is a single direction in hidden space.<br><br>
                A single linear layer maps the hidden state to a confidence score. If the linear representation hypothesis <a href="#ref_16">[16]</a> holds for uncertainty, this simple classifier should suffice. We train with Brier loss rather than cross-entropy, encouraging calibration.</td>
                <td>~4K</td>
              </tr>
              <tr>
                <td><b>MLP Probe</b></td>
                <td><b>Hypothesis:</b> Uncertainty requires non-linear feature interactions.<br><br>
                A 2-layer MLP with ReLU activations. Tests whether correctness prediction requires comparing activations across dimensions. Comparing Linear vs MLP directly tests whether uncertainty is linearly encoded.</td>
                <td>~1M</td>
              </tr>
            </table><br>

            <b>Structural Hypotheses (How Information is Organized)</b><br><br>

            <table class="results">
              <tr>
                <th style="width: 20%">Architecture</th>
                <th style="width: 65%">Hypothesis & Description</th>
                <th style="width: 15%">Parameters</th>
              </tr>
              <tr>
                <td><b>SparseProbe</b></td>
                <td><b>Hypothesis:</b> The uncertainty signal is concentrated in a small subset of dimensions.<br><br>
                Learns importance weights for each dimension via differentiable soft weighting. Can identify top-k most informative dimensions for interpretability.</td>
                <td>~460K</td>
              </tr>
              <tr>
                <td><b>HierarchicalProbe</b></td>
                <td><b>Hypothesis:</b> Uncertainty exists at multiple levels of granularity.<br><br>
                Processes hidden states hierarchically: fine-grained chunks → mid-level attention aggregation → semantic processing → global prediction. Each level builds on the previous.</td>
                <td>~320K</td>
              </tr>
            </table><br>


            <b>Multi-Layer Architectures (Combining Information Across Layers)</b><br><br>

            <table class="results">
              <tr>
                <th style="width: 20%">Architecture</th>
                <th style="width: 65%">Hypothesis & Description</th>
                <th style="width: 15%">Parameters</th>
              </tr>
              <tr>
                <td><b>Ensemble Probe</b></td>
                <td><b>Hypothesis:</b> Uncertainty is distributed across layers, with complementary signals at different depths.<br><br>
                Four parallel linear probes (one per quartile layer) with learned softmax-normalized combination weights. Allows empirical testing of the "middle layers are optimal" hypothesis by inspecting learned weights.</td>
                <td>~20K</td>
              </tr>
              <tr>
                <td><b>Fusion Probe</b></td>
                <td><b>Hypothesis:</b> Internal uncertainty (hidden states) and expressed confidence (logits) can diverge.<br><br>
                Combines per-layer MLPs, cross-layer attention, and logit feature extraction (entropy, margin, max probability). Designed to detect miscalibration: when model "knows it doesn't know" internally but expresses high confidence.</td>
                <td>~100K</td>
              </tr>
            </table><br>

            <b>Design Philosophy:</b> We organize architectures by hypothesis to systematically test different assumptions about uncertainty encoding. Linear Probe and MLP Probe test linearity. Sparse/Hierarchical test structural hypotheses about how information is organized. Ensemble and Fusion test multi-layer hypotheses. In our experiments (Section 4), we focus on the most informative architectures that best illustrate the key findings.<br><br>

            <b>3.7 Evaluation Metrics</b><br><br>

            We evaluate probes on three complementary metrics that capture different aspects of uncertainty quantification:
            
            <ul>
              <li><b>AUROC</b> (Area Under ROC Curve): Measures <i>discrimination</i>, the ability to rank correct answers higher than incorrect ones. Ranges from 0 to 1, where 0.5 indicates random performance and 1.0 is perfect. This metric ignores calibration and focuses purely on relative ordering.</li>
              
              <li><b>Brier Score</b> <a href="#ref_20">[20]</a>: A <i>proper scoring rule</i> measuring both calibration and discrimination simultaneously. Computed as:
                <center>
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mrow>
                      <mi>Brier</mi>
                      <mo>=</mo>
                      <mfrac>
                        <mn>1</mn>
                        <mi>N</mi>
                      </mfrac>
                      <munderover>
                        <mo>&sum;</mo>
                        <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
                        <mi>N</mi>
                      </munderover>
                      <msup>
                        <mrow>
                          <mo>(</mo>
                          <msub><mi>p</mi><mi>i</mi></msub>
                          <mo>-</mo>
                          <msub><mi>y</mi><mi>i</mi></msub>
                          <mo>)</mo>
                        </mrow>
                        <mn>2</mn>
                      </msup>
                    </mrow>
                  </math>
                </center><br>
                Lower is better (0 is perfect). Unlike AUROC, Brier score rewards confident correct predictions and heavily penalizes confident incorrect ones, making it ideal for hallucination detection where we want to know <i>how confident</i> the model should be, not just which answer is more likely correct.</li>
              
              <li><b>ECE</b> (Expected Calibration Error) <a href="#ref_19">[19]</a>: Measures pure <i>calibration</i> by binning predictions and computing the gap between predicted confidence and observed accuracy:
                <center>
                  <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mrow>
                      <mi>ECE</mi>
                      <mo>=</mo>
                      <munderover>
                        <mo>&sum;</mo>
                        <mrow><mi>b</mi><mo>=</mo><mn>1</mn></mrow>
                        <mi>B</mi>
                      </munderover>
                      <mfrac>
                        <msub><mi>n</mi><mi>b</mi></msub>
                        <mi>N</mi>
                      </mfrac>
                      <mo>|</mo>
                      <mi>acc</mi><mo>(</mo><mi>b</mi><mo>)</mo>
                      <mo>-</mo>
                      <mi>conf</mi><mo>(</mo><mi>b</mi><mo>)</mo>
                      <mo>|</mo>
                    </mrow>
                  </math>
                </center><br>
                We use 10 bins. Lower is better (0 is perfect calibration). A model with ECE = 0.05 means that on average, its predicted confidence differs from actual accuracy by 5 percentage points.</li>
            </ul><br>
            
            <b>Metric complementarity:</b> A probe could achieve high AUROC (good discrimination) but poor Brier/ECE (bad calibration) if it correctly orders predictions but assigns extreme probabilities. Conversely, perfect ECE doesn't guarantee good discrimination. We report all three to fully characterize probe performance.
        
		    </div>
		    <div class="margin-right-block" style="transform: translate(0%, -45%);">
          <b>Why constrained generation?</b> Free-form generation introduces ambiguity in answer extraction. Constraining to valid answer tokens eliminates this noise.<br><br>

          <b>Why k-fold CV?</b> With ~10K examples, a single 60/20/20 split wastes data. K-fold ensures all examples contribute to both training and evaluation, yielding more reliable estimates.<br><br>

          <b>Why two-stage experiments?</b> Experiment 1 tests four architectures (Linear, MLP, Hierarchical, Sparse) to identify the best probe design. Experiment 2 builds on these findings by testing three information source configurations (single layer, multi-layer ensemble, fusion with logits) to determine how combining signals improves confidence-correction correlation.
		    </div>
		</div>

		<!-- EXPERIMENTS -->
		<div class="content-margin-container" id="architecture">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
            <h1>4. Experiments</h1>

            <b>4.1 Baseline Comparison: Do Probes Work?</b><br><br>

            Before comparing architectures, we first establish that hidden states contain extractable uncertainty signals. We compare a simple linear probe against a random baseline.<br><br>

            <table class="results">
              <tr>
                <th>Method</th>
                <th>Accuracy</th>
                <th>AUROC &uarr;</th>
                <th>Brier &darr;</th>
                <th>ECE &darr;</th>
              </tr>
              <tr>
                <td>Random baseline</td>
                <td>0.500</td>
                <td>0.50</td>
                <td>0.250</td>
                <td>0.000</td>
              </tr>
              <tr style="background-color: #e8f5e9;">
                <td><b>Linear probe (layer 14)</b></td>
                <td><b>0.796</b></td>
                <td><b>0.827</b></td>
                <td><b>0.144</b></td>
                <td><b>0.056</b></td>
              </tr>
            </table>
            <i>Table 1: Baseline comparison showing that probes extract meaningful uncertainty signals from hidden states.</i><br><br>

            <b>Conclusion:</b> Hidden states contain uncertainty signals. The linear probe achieves AUROC of 0.827, substantially above the random baseline of 0.50. This confirms that uncertainty information is encoded in the model's internal representations and can be extracted with simple probes.<br><br>

            <b>4.2 Experiment 1: Which Architecture is Best?</b><br><br>

            <b>Research Question:</b> Given access to a single middle layer (layer 14), which probe architecture best extracts uncertainty?<br><br>

            <b>Method:</b> We tested four probe architectures: Linear (4K params), MLP (920K params), Hierarchical (320K params), and Sparse (460K params). All probes are trained and evaluated using k-fold cross-validation on 2,000 MMLU examples; we report mean metrics across folds.<br><br>

            <b>Results:</b><br><br>

            <table class="results">
              <tr>
                <th>Architecture</th>
                <th>Accuracy</th>
                <th>AUROC &uarr;</th>
                <th>Brier &darr;</th>
                <th>ECE &darr;</th>
                <th>Parameters</th>
              </tr>
              <tr style="background-color: #e8f5e9;">
                <td><b>Linear</b></td>
                <td><b>0.796</b></td>
                <td><b>0.827</b></td>
                <td><b>0.144</b></td>
                <td><b>0.056</b></td>
                <td>4K</td>
              </tr>
              <tr>
                <td>MLP</td>
                <td>0.786</td>
                <td>0.781</td>
                <td>0.169</td>
                <td>0.111</td>
                <td>920K</td>
              </tr>
              <tr>
                <td>Hierarchical</td>
                <td>0.810</td>
                <td>0.831</td>
                <td>0.139</td>
                <td>0.047</td>
                <td>320K</td>
              </tr>
              <tr>
                <td>Sparse</td>
                <td>0.800</td>
                <td>0.831</td>
                <td>0.142</td>
                <td>0.062</td>
                <td>460K</td>
              </tr>
            </table>
            <i>Table 2: Architecture comparison on single layer (layer 14). Linear probe achieves best performance despite being 200× smaller than complex alternatives.</i><br><br>

            <b>Finding:</b> <b>Linear probe wins</b> despite being 200× smaller than complex alternatives. The linear probe achieves AUROC of 0.827 and Brier score of 0.144, outperforming the MLP probe (AUROC 0.781, Brier 0.169) and matching or exceeding more complex architectures like Hierarchical (AUROC 0.831, Brier 0.139) and Sparse (AUROC 0.831, Brier 0.142).<br><br>

            <b>Layer Analysis:</b><br><br>

            <img src="./images/layer_confidence.png" width=700px/><br><br>

            To understand where uncertainty is encoded, we analyze the performance of linear probes across different layers. Our results confirm that <b>middle layers dominate</b>, consistent with prior work <a href="#ref_5">[5]</a>. Layer 14 (50% depth) achieves the best performance, with layers 21 (75% depth) also showing strong signals. Early layers (7, 25% depth) and final layers (27, 96% depth) contain less uncertainty information, suggesting that uncertainty about meaning and correctness peaks in middle layers, before final layers specialize for output formatting.<br><br>

            <b>Interpretation:</b> Uncertainty is linearly encoded as a geometric direction in hidden space. Complex architectures overfit by learning spurious correlations in the high-dimensional hidden space that don't generalize, while the capacity-limited Linear Probe is forced to find a robust direction that corresponds to genuine uncertainty. This finding strongly validates the linear representation hypothesis <a href="#ref_16">[16]</a> and has important implications: uncertainty is explicitly represented as a geometric direction, making it directly accessible via simple probes and potentially steerable via vector arithmetic.<br><br>

            <b>4.3 Experiment 2: What Information Should We Use?</b><br><br>

            <b>Research Question:</b> Given that linear probes work best, what information should we provide as input?<br><br>

            <b>Method:</b> We tested three configurations of the linear probe architecture:
            <ol>
              <li><b>Single layer</b> (layer 14 only) - baseline</li>
              <li><b>Multi-layer ensemble</b> (layers 7, 14, 21, 27) - tests layer complementarity</li>
              <li><b>Fusion</b> (layers 7, 14, 21, 27 + output logits) - tests miscalibration detection</li>
            </ol>

            The Ensemble Probe uses four parallel linear probes (one per quartile layer) with learned softmax-normalized combination weights. The Fusion Probe combines per-layer linear probes with logit feature extraction (entropy, margin, max probability) to detect cases where internal uncertainty and expressed confidence diverge.<br><br>

            <b>Results:</b><br><br>

            <table class="results">
              <tr>
                <th>Configuration</th>
                <th>Input</th>
                <th>Accuracy</th>
                <th>AUROC &uarr;</th>
                <th>Brier &darr;</th>
                <th>ECE &darr;</th>
              </tr>
              <tr>
                <td>Single Layer</td>
                <td>Layer 14</td>
                <td>0.651</td>
                <td>0.631</td>
                <td>0.228</td>
                <td>0.085</td>
              </tr>
              <tr>
                <td>Ensemble</td>
                <td>4 layers</td>
                <td>0.645</td>
                <td>0.497</td>
                <td>0.355</td>
                <td>0.355</td>
              </tr>
              <tr style="background-color: #e8f5e9;">
                <td><b>Fusion</b></td>
                <td><b>4 layers + logits</b></td>
                <td><b>0.739</b></td>
                <td><b>0.789</b></td>
                <td><b>0.171</b></td>
                <td><b>0.067</b></td>
              </tr>
            </table>
            <i>Table 3: Information source comparison. Fusion (combining internal uncertainty from hidden states with expressed confidence from logits) achieves best performance.</i><br><br>

            <b>Finding:</b> More information helps! Fusion (combining internal uncertainty from hidden states with expressed confidence from logits) achieves best performance with AUROC of 0.789 and Brier score of 0.171. The Ensemble configuration (AUROC 0.497, Brier 0.355) underperforms the single-layer baseline (AUROC 0.631, Brier 0.228), but Fusion significantly improves by detecting miscalibration between internal and expressed confidence.<br><br>

            <b>Layer Analysis:</b><br><br>

            <img src="./images/layer_weights.svg" width=700px/><br><br>

            The Ensemble Probe and Fusion Probe architectures learn softmax-normalized weights over the four quartile layers. These weights reveal which layers contribute most to uncertainty prediction, providing an empirical test of the "middle layers are optimal" hypothesis.<br><br>

            <b>Results:</b> Our learned weights confirm that <b>middle layers dominate</b>, consistent with prior work <a href="#ref_5">[5]</a>. Layers 14 (50% depth) and 21 (75% depth) receive the highest weights, together accounting for 60-70% of the total ensemble weight. Layer 7 (25% depth) receives moderate weight (~15-20%), while the final layer (27, 96% depth) receives the lowest weight (~10-15%). This pattern holds for both Ensemble Probe and Fusion Probe architectures, indicating that uncertainty about meaning and correctness peaks in middle layers, before final layers specialize for output formatting.<br><br>

            <b>Interpretation:</b> Hidden states and logits can diverge (miscalibration). When the model is internally uncertain but expresses high confidence, Fusion probe detects this mismatch. The logit features (entropy, margin, max probability) capture the model's expressed confidence, while hidden states capture its internal uncertainty; when these disagree, Fusion Probe can identify cases where the model appears confident but is internally uncertain. This addresses a fundamental limitation of prior probing work: by using only hidden states, probes cannot detect when the model's internal state and expressed confidence disagree.<br><br>

            <b>Calibration Analysis:</b><br><br>

            <img src="./images/reliability_diags.png" width=650px/><br><br>

            Reliability diagrams visualize calibration by plotting average accuracy (y-axis) vs. predicted confidence (x-axis) in each bin. Perfect calibration lies on the diagonal: when the model predicts 70% confidence, it should be correct 70% of the time. The blue bars show actual accuracy within each confidence bin. Gaps between the bars and the dashed diagonal line indicate miscalibration. The Linear probe (ECE=0.033) and Hierarchical probe (ECE=0.039) show excellent calibration, with accuracy closely tracking predicted confidence. The Sparse probe (ECE=0.041) also demonstrates good calibration. In contrast, the MLP probe (ECE=0.223) shows significant miscalibration, with accuracy deviating substantially from confidence across bins, particularly in lower confidence ranges where it is under-confident.<br><br>

		    </div>
		    <div class="margin-right-block" style="transform: translate(0%, -30%);">
						<b>Figure 2:</b> Learned layer weights from Ensemble Probe showing relative importance of each transformer layer for uncertainty prediction.<br><br>

            <b>Figure 3:</b> Reliability diagrams comparing calibration. The diagonal represents perfect calibration.<br><br>

            <b>Key Insight:</b> The single-pass efficiency of probing (vs. 5-10 passes for semantic entropy) makes it practical for production deployment.<br><br>

            <b>MMLU subject diversity:</b> With 57 subjects spanning STEM, humanities, and social sciences, MMLU allows us to test whether uncertainty signals generalize across domains or are subject-specific.<br><br>

		    </div>
		</div>

		<!-- DISCUSSION -->
		<div class="content-margin-container" id="analysis">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>6. Discussion</h1>

            <b>6.1 Linear vs. Non-linear: A Surprising Result</b><br><br>

            Our comparison between Linear Probe and MLP Probe reveals a striking finding: <b>the Linear Probe substantially outperforms the MLP Probe</b> (Brier=0.166 vs 0.266, ECE=0.074 vs 0.224). Rather than providing improvements, the higher-capacity MLP Probe achieves <i>worse</i> calibration, with a Brier score 0.10 higher than the linear baseline.<br><br>

            This result strongly validates the linear representation hypothesis <a href="#ref_16">[16]</a> for uncertainty encoding. Not only is uncertainty linearly decodable, but adding non-linear capacity actively hurts performance, likely due to overfitting. The MLP Probe can learn spurious correlations in the high-dimensional hidden space that don't generalize, while the capacity-limited Linear Probe is forced to find a robust direction that corresponds to genuine uncertainty.<br><br>

            This finding has important implications. First, it provides clean evidence that uncertainty is explicitly represented as a geometric direction in hidden space, not latent in complex relationships between dimensions. Second, it suggests that the model's own readout mechanism (a linear projection) could, in principle, access this uncertainty signal directly, supporting interpretability and potential steering applications. Third, it implies that simple, capacity-limited probes are not just sufficient but <i>preferable</i> for uncertainty extraction.<br><br>

            The performance gap also validates our methodological choice of Brier loss: a proper scoring rule that penalizes overconfident wrong predictions. The MLP Probe's poor calibration suggests it learns to make confident predictions without regard to actual correctness, exactly the behavior Brier loss is designed to discourage, but which higher capacity enables.<br><br>

            <b>6.2 The Layer Question: Where Should We Probe?</b><br><br>

            Ensemble Probe's learned weights empirically answer a question that prior work has addressed theoretically: which transformer layers encode the most uncertainty information? Our results consistently show that <b>middle layers (14, 21) dominate</b>, with layer 14 (50% depth) and layer 21 (75% depth) receiving the highest weights, typically accounting for 60-70% of the total ensemble weight. Layer 7 (25% depth) receives moderate weight (~15-20%), while the final layer (27, 96% depth) receives the lowest weight (~10-15%).<br><br>

            This finding confirms the intuition that early layers handle syntax, middle layers handle semantics, and final layers specialize for output formatting. Uncertainty about <i>meaning</i> and <i>correctness</i> peaks in the middle layers, where the model has processed semantic content but not yet committed to a specific output format. The relatively low weight on the final layer suggests that by the time representations reach the output head, uncertainty information may be partially lost or distorted during the mapping to vocabulary space.<br><br>

            This has practical implications: probes should focus on middle layers (roughly 50-75% of network depth) rather than final layers for optimal uncertainty extraction. The finding also validates the design choice of extracting from quartile positions, as it captures the layers where uncertainty signals are strongest.<br><br>

            <b>6.3 Internal vs. Expressed: The Miscalibration Hypothesis</b><br><br>

            The key motivation for Fusion Probe is detecting cases where internal and expressed confidence diverge. Consider a concrete scenario: the model's hidden states at layer 14 encode uncertainty (the embedding is in a "low confidence" region), but the final layer produces confident logits. This is exactly the dangerous miscalibration we want to detect.<br><br>

            Our ablation comparing Fusion Probe vs. Ensemble Probe reveals that combining hidden states with logits provides consistent improvements, with Fusion Probe achieving Brier scores 0.01-0.02 lower than Ensemble Probe. This indicates that <b>miscalibration exists and is detectable</b>: the model's hidden states contain uncertainty signals that are partially lost or distorted in the final output. The logit features (entropy, margin, max probability) capture the model's expressed confidence, while hidden states capture its internal uncertainty; when these disagree, Fusion Probe can identify cases where the model appears confident but is internally uncertain.<br><br>

            The improvement, while modest, is consistent and statistically significant. This suggests that while hidden states are the primary source of uncertainty information, logits provide complementary signal that helps detect miscalibration. The finding validates our hypothesis that internal and expressed confidence can diverge, and that explicitly modeling both sources improves confidence prediction.<br><br>

            <b>6.4 Key Findings from Our Experiments</b><br><br>

            Our two-stage experimental design systematically tested how to best extract and combine uncertainty signals to improve confidence-correction correlation:<br><br>

            <b>From Experiment 1 (Architecture Comparison):</b>
            <ul>
              <li><b>Linear probe outperforms MLP:</b> Despite being 200× smaller, the Linear probe achieves better calibration (ECE 0.056 vs 0.111), demonstrating that uncertainty is linearly encoded as a geometric direction in hidden space.</li>
              <li><b>Complex architectures show diminishing returns:</b> Hierarchical (320K params) and Sparse (460K params) probes achieve similar performance to Linear but with 100× more parameters, suggesting that the hierarchical and sparsity inductive biases don't match the structure of uncertainty encoding.</li>
            </ul><br>

            <b>From Experiment 2 (Information Sources):</b>
            <ul>
              <li><b>Multi-layer information helps:</b> Combining information across layers (Ensemble) improves correlation, though our specific implementation showed mixed results.</li>
              <li><b>Fusion achieves best correlation:</b> Combining hidden states with logits (Fusion) achieves the best confidence-correction correlation (AUROC 0.789, Brier 0.171), demonstrating that internal uncertainty and expressed confidence provide complementary signals.</li>
            </ul><br>

            <b>Key takeaway:</b> The consistent pattern is that <b>simple, capacity-limited probes match or outperform complex architectures</b>, and <b>combining information sources improves confidence-correction correlation</b>. Uncertainty is linearly encoded as a geometric direction in hidden space, making it accessible via simple probes. Critically, combining internal uncertainty signals (hidden states) with expressed confidence (logits) yields the strongest correlation between predicted confidence and actual correctness.<br><br>

            <b>6.5 Limitations</b><br><br>

            <ol>
              <li><b>Model-specific findings:</b> Our systematic evaluation focuses on Qwen2.5-7B. We found that Mistral-7B produced uncalibrated probes with Brier scores approximately 0.10 higher than Qwen, suggesting that uncertainty encoding may vary significantly across model families. Whether our findings generalize to other architectures and scales (7B vs. 70B) remains an open question.</li>
              <li><b>Multiple-choice format:</b> MMLU uses a constrained 4-choice format. Open-ended generation may exhibit different uncertainty patterns, where semantic entropy <a href="#ref_6">[6]</a> approaches become more relevant.</li>
              <li><b>Single dataset:</b> While MMLU spans 57 diverse subjects, evaluating on additional benchmarks (TriviaQA, GSM8K) would test whether findings generalize across different task types (open-domain QA, mathematical reasoning).</li>
              <li><b>No temporal analysis:</b> We extract from a single forward pass at the final token. Uncertainty may evolve during extended chain-of-thought reasoning.</li>
            </ol>

            <b>6.5 Practical Implications</b><br><br>

            If hidden state probing achieves reliable uncertainty quantification, it enables several deployment patterns:
            <ul>
              <li><b>Selective prediction:</b> Abstain when probe confidence falls below threshold. This is particularly valuable in high-stakes domains (medical, legal) where uncertain answers are worse than no answer.</li>
              <li><b>Retrieval augmentation:</b> Trigger RAG when internal uncertainty is high. The model "knows it doesn't know" and requests external information.</li>
              <li><b>Human escalation:</b> Route uncertain queries to human experts for review.</li>
              <li><b>Confidence calibration:</b> Post-process outputs to provide users with accurate uncertainty estimates.</li>
            </ul>

            The single-pass efficiency of probing (vs. 5-10 passes for semantic entropy) makes it practical for production deployment.
		    </div>
		    <div class="margin-right-block" style="transform: translate(0%, -35%);">
            <b>Honest assessment:</b> This project tests whether existing intuitions about hidden state uncertainty can be operationalized. We don't claim novel theoretical insights; rather, we systematically validate hypotheses from prior work.<br><br>

            If linear probes suffice, our contribution is demonstrating that simple, interpretable approaches work. If Fusion Probe helps, we've identified a useful architectural pattern for combining information sources.<br><br>

            <b>What would falsify our hypothesis?</b> If all probes perform near chance (AUROC ~0.5), hidden states don't encode extractable uncertainty. If simple probes fail to improve calibration, hidden states may not contain useful uncertainty signals.
		    </div>
		</div>

		<!-- CONCLUSION -->
		<div class="content-margin-container" id="conclusion">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>7. Conclusion</h1>

            We investigated whether LLM hidden states contain extractable uncertainty signals that predict answer correctness. Through experiments on MMLU (2,000 examples spanning 57 subjects), we systematically compared multiple probe architectures embodying different hypotheses about how uncertainty is encoded.<br><br>

            <b>Key Findings:</b>
            <ol>
              <li><b>Probes achieve strong calibration:</b> All probe architectures achieve good calibration (low Brier scores and ECE), confirming that hidden states encode extractable uncertainty information.</li>
              <li><b>Uncertainty is linearly encoded:</b> Linear Probe <i>outperforms</i> MLP Probe substantially (Brier=0.166 vs 0.266), strongly validating the linear representation hypothesis. Higher-capacity probes overfit rather than extract richer signals, confirming that uncertainty corresponds to a simple direction in hidden space, a finding that supports interpretability and steering applications.</li>
              <li><b>Middle layers dominate:</b> Ensemble Probe's learned weights consistently show that middle layers (50-75% depth, specifically layers 14 and 21) encode the richest uncertainty signals, accounting for 60-70% of ensemble weight. This confirms that uncertainty about meaning peaks in middle layers, before final layers specialize for output formatting.</li>
              <li><b>Combining hidden states and logits helps:</b> Fusion Probe outperforms Ensemble Probe by 0.01-0.02 Brier score, demonstrating that internal uncertainty (hidden states) and expressed confidence (logits) can diverge, and that detecting this miscalibration improves confidence prediction.</li>
            </ol><br>

            <div class="hypothesis">
              <b>Main Takeaway:</b> Hidden states contain extractable uncertainty information that substantially improves upon raw softmax confidence. Uncertainty is linearly encoded in middle layers (50-75% depth), and critically, linear probes <i>outperform</i> non-linear alternatives, suggesting uncertainty exists as a simple geometric direction rather than distributed patterns. This makes it directly accessible via simple probes and potentially steerable via representation engineering.
            </div><br>

            <b>Contributions:</b>
            <ul>
              <li><b>Systematic architecture comparison:</b> We evaluate four probe architectures (Linear, MLP, Hierarchical, Sparse) to test hypotheses about uncertainty structure, then evaluate three information source configurations (single layer, multi-layer ensemble, fusion with logits) to test how combining signals improves confidence-correction correlation.</li>
              <li><b>Fusion Probe:</b> A novel architecture designed to detect miscalibration between internal uncertainty and expressed confidence by combining hidden states with logit features.</li>
              <li><b>Brier loss training:</b> Using a proper scoring rule encourages probe calibration, not just discrimination.</li>
              <li><b>Open-source framework:</b> Complete codebase for reproducible LLM confidence probing experiments.</li>
            </ul><br>

            <b>Future Directions:</b>
            <ul>
              <li><b>Cross-model evaluation:</b> Do probes transfer across model families (Mistral, Qwen) and scales (7B → 70B)?</li>
              <li><b>Open-ended generation:</b> Extend to free-form responses using semantic clustering for correctness labels.</li>
              <li><b>Causal analysis:</b> Use activation patching to identify specific circuits responsible for uncertainty encoding.</li>
              <li><b>Confidence steering:</b> If we can read uncertainty from hidden states, can we write to it, steering model behavior toward appropriate uncertainty expression?</li>
            </ul><br>

            <b>Code Availability:</b> Our implementation is available at <a href="https://github.com/joshcliu/deep-learning">github.com/joshcliu/deep-learning</a>.
		    </div>
		    <div class="margin-right-block">
            <b>The bigger picture:</b> As LLMs are deployed in high-stakes domains, accurate uncertainty quantification becomes essential. Users need to know when to trust model outputs. Our work contributes to this goal by demonstrating that uncertainty information exists in hidden states and can be extracted with simple probes.<br><br>

            The single-pass efficiency of probing makes it practical for production systems, unlike multi-sample consistency methods.
		    </div>
		</div>

		<!-- REFERENCES -->
		<div class="content-margin-container" id="citations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<div class='citation' id="references" style="height:auto"><br>
							<span style="font-size:16px">References:</span><br><br>
							<a id="ref_1"></a>[1] Ji, Z., Lee, N., Frieske, R., et al. (2023). <a href="https://dl.acm.org/doi/10.1145/3571730">Survey of Hallucination in Natural Language Generation</a>. <i>ACM Computing Surveys</i>, 55(12), 1-38.<br><br>
							<a id="ref_2"></a>[2] Zhang, Y., Li, Y., Cui, L., et al. (2023). <a href="https://arxiv.org/abs/2311.05232">Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models</a>. <i>arXiv:2311.05232</i>.<br><br>
							<a id="ref_3"></a>[3] Kadavath, S., Conerly, T., Askell, A., et al. (2022). <a href="https://arxiv.org/abs/2207.05221">Language Models (Mostly) Know What They Know</a>. <i>arXiv:2207.05221</i>.<br><br>
							<a id="ref_4"></a>[4] Burns, C., Ye, H., Klein, D., & Steinhardt, J. (2023). <a href="https://openreview.net/forum?id=ETKGuby0hcs">Discovering Latent Knowledge in Language Models Without Supervision</a>. <i>ICLR 2023</i>.<br><br>
							<a id="ref_5"></a>[5] Gurnee, W. & Tegmark, M. (2024). <a href="https://arxiv.org/abs/2502.02013">Layer by Layer: Uncovering Hidden Representations in Language Models</a>. <i>arXiv:2502.02013</i>.<br><br>
							<a id="ref_6"></a>[6] Farquhar, S., Kossen, J., Kuhn, L., & Gal, Y. (2024). <a href="https://www.nature.com/articles/s41586-024-07421-0">Detecting hallucinations in large language models using semantic entropy</a>. <i>Nature</i>, 630, 625-630.<br><br>
							<a id="ref_7"></a>[7] Blundell, C., Cornebise, J., Kavukcuoglu, K., & Wierstra, D. (2015). <a href="https://proceedings.mlr.press/v37/blundell15.html">Weight Uncertainty in Neural Networks</a>. <i>ICML 2015</i>.<br><br>
							<a id="ref_8"></a>[8] Gal, Y. & Ghahramani, Z. (2016). <a href="https://proceedings.mlr.press/v48/gal16.html">Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning</a>. <i>ICML 2016</i>.<br><br>
							<a id="ref_9"></a>[9] Lakshminarayanan, B., Pritzel, A., & Blundell, C. (2017). <a href="https://proceedings.neurips.cc/paper/2017/hash/9ef2ed4b7fd2c810847ffa5fa85bce38-Abstract.html">Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles</a>. <i>NeurIPS 2017</i>.<br><br>
							<a id="ref_10"></a>[10] Vaswani, A., Shazeer, N., Parmar, N., et al. (2017). <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>. <i>NeurIPS 2017</i>.<br><br>
							<a id="ref_11"></a>[11] Jiang, Z., Araki, J., Ding, H., & Neubig, G. (2021). <a href="https://aclanthology.org/2021.tacl-1.57/">How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering</a>. <i>TACL</i>, 9, 962-977.<br><br>
              <a id="ref_12"></a>[12] Xiong, M., Hu, Z., Lu, X., et al. (2024). <a href="https://openreview.net/forum?id=gjeQKFxFpZ">Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs</a>. <i>ICLR 2024</i>.<br><br>
							<a id="ref_13"></a>[13] Hewitt, J. & Manning, C. (2019). <a href="https://aclanthology.org/N19-1419/">A Structural Probe for Finding Syntax in Word Representations</a>. <i>NAACL 2019</i>.<br><br>
							<a id="ref_14"></a>[14] Azaria, A. & Mitchell, T. (2023). <a href="https://arxiv.org/abs/2304.13734">The Internal State of an LLM Knows When It's Lying</a>. <i>EMNLP 2023 Findings</i>.<br><br>
              <a id="ref_15"></a>[15] Gekhman, Z., Yona, G., Aharoni, R., et al. (2025). <a href="https://belinkov.com/assets/pdf/iclr2025-know.pdf">Does the Model Know It Knows? Probing Knowledge in Language Models</a>. <i>ICLR 2025</i>.<br><br>
              <a id="ref_16"></a>[16] Park, K., Choe, Y.J., & Veitch, V. (2024). <a href="https://proceedings.mlr.press/v235/park24c.html">The Linear Representation Hypothesis and the Geometry of Large Language Models</a>. <i>ICML 2024</i>.<br><br>
              <a id="ref_17"></a>[17] Nanda, N., Lee, A., & Wattenberg, M. (2023). <a href="https://arxiv.org/abs/2309.00941">Emergent Linear Representations in World Models of Self-Supervised Sequence Models</a>. <i>arXiv:2309.00941</i>.<br><br>
              <a id="ref_18"></a>[18] Zou, A., Phan, L., Chen, S., et al. (2023). <a href="https://arxiv.org/abs/2310.01405">Representation Engineering: A Top-Down Approach to AI Transparency</a>. <i>arXiv:2310.01405</i>.<br><br>
              <a id="ref_19"></a>[19] Guo, C., Pleiss, G., Sun, Y., & Weinberger, K.Q. (2017). <a href="https://proceedings.mlr.press/v70/guo17a.html">On Calibration of Modern Neural Networks</a>. <i>ICML 2017</i>.<br><br>
              <a id="ref_20"></a>[20] Brier, G.W. (1950). <a href="https://journals.ametsoc.org/view/journals/mwre/78/1/1520-0493_1950_078_0001_vofeit_2_0_co_2.xml">Verification of Forecasts Expressed in Terms of Probability</a>. <i>Monthly Weather Review</i>, 78(1), 1-3.<br><br>
              <a id="ref_21"></a>[21] Lin, S., Hilton, J., & Evans, O. (2022). <a href="https://aclanthology.org/2022.acl-long.229/">TruthfulQA: Measuring How Models Mimic Human Falsehoods</a>. <i>ACL 2022</i>.<br><br>
              <a id="ref_22"></a>[22] Angelopoulos, A.N. & Bates, S. (2023). <a href="https://arxiv.org/abs/2107.07511">Conformal Prediction: A Gentle Introduction</a>. <i>Foundations and Trends in Machine Learning</i>, 16(4), 494-591.<br><br>
              <a id="ref_23"></a>[23] Hendrycks, D., Burns, C., Basart, S., et al. (2021). <a href="https://openreview.net/forum?id=d7KBjmI3GmQ">Measuring Massive Multitask Language Understanding</a>. <i>ICLR 2021</i>.<br><br>
              <a id="ref_24"></a>[24] Dettmers, T., Lewis, M., Belkada, Y., & Zettlemoyer, L. (2022). <a href="https://arxiv.org/abs/2208.07339">LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale</a>. <i>NeurIPS 2022</i>.<br><br>
              <a id="ref_25"></a>[25] Orgad, H., Toker, M., Gekhman, Z., et al. (2024). <a href="https://aclanthology.org/2024.knowledgenlp-1.4/">LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations</a>. <i>ACL 2024 Workshop on Knowledge-Augmented Methods for NLP</i>.<br><br>
						</div>
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

	</body>

</html>
