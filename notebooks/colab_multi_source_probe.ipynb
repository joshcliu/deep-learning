{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Source Confidence Probe\n",
    "\n",
    "This notebook tests the **MultiSourceConfidenceNetwork** which combines:\n",
    "- Hidden states from **k quartile layers** (internal uncertainty)\n",
    "- Output **logits** for answer choices (expressed confidence)\n",
    "\n",
    "The hypothesis: Internal uncertainty (hidden states) may differ from expressed confidence (logits). By combining both, we can detect miscalibration.\n",
    "\n",
    "**Comparison architectures:**\n",
    "1. Linear (single layer baseline)\n",
    "2. LayerEnsemble (multi-layer, no logits)\n",
    "3. MultiSource (multi-layer + logits) \u2190 NEW\n",
    "\n",
    "**Setup:** Run on Google Colab with GPU (Runtime > Change runtime type > T4 GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /content/deep-learning\n",
    "!rm -rf deep-learning*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (Colab)\n",
    "!pip install -q transformers accelerate bitsandbytes datasets tqdm matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo and setup path\n",
    "!git clone https://github.com/joshcliu/deep-learning.git\n",
    "%cd deep-learning\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch for bfloat16 compatibility (8-bit quantized models)\n",
    "import numpy as np\n",
    "import torch\n",
    "from src.models import extractor as extractor_module\n",
    "\n",
    "def _patched_extract_batch(self, texts, layers, max_length, token_position):\n",
    "    \"\"\"Patched to handle bfloat16 safely.\"\"\"\n",
    "    encodings = self.tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    encodings = {k: v.to(self.device) for k, v in encodings.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = self.model(\n",
    "            **encodings,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "    hidden_states = outputs.hidden_states\n",
    "    batch_hiddens = []\n",
    "\n",
    "    for layer_idx in layers:\n",
    "        layer_hiddens = hidden_states[layer_idx + 1]\n",
    "\n",
    "        if token_position == \"last\":\n",
    "            attention_mask = encodings[\"attention_mask\"]\n",
    "            seq_lengths = attention_mask.sum(dim=1) - 1\n",
    "            token_hiddens = layer_hiddens[\n",
    "                torch.arange(layer_hiddens.size(0), device=self.device),\n",
    "                seq_lengths\n",
    "            ]\n",
    "        elif token_position == \"cls\":\n",
    "            token_hiddens = layer_hiddens[:, 0, :]\n",
    "        elif token_position == \"mean\":\n",
    "            attention_mask = encodings[\"attention_mask\"].unsqueeze(-1)\n",
    "            masked_hiddens = layer_hiddens * attention_mask\n",
    "            token_hiddens = masked_hiddens.sum(dim=1) / attention_mask.sum(dim=1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown token_position: {token_position}\")\n",
    "\n",
    "        token_hiddens = token_hiddens.detach().cpu().to(torch.float32).numpy()\n",
    "        batch_hiddens.append(token_hiddens)\n",
    "\n",
    "    return np.stack(batch_hiddens, axis=1)\n",
    "\n",
    "extractor_module.HiddenStateExtractor._extract_batch = _patched_extract_batch\n",
    "print(\"Patched HiddenStateExtractor for bfloat16 compatibility.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import ModelLoader, HiddenStateExtractor\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-7B\"\n",
    "loader = ModelLoader(model_name)\n",
    "model, tokenizer = loader.load(quantization=\"8bit\", device_map=\"auto\")\n",
    "\n",
    "print(f\"Loaded {model_name}\")\n",
    "print(f\"Layers: {loader.config.num_layers}, Hidden dim: {loader.config.hidden_dim}\")\n",
    "\n",
    "# Define quartile layers\n",
    "num_layers = loader.config.num_layers\n",
    "hidden_dim = loader.config.hidden_dim\n",
    "QUARTILE_LAYERS = [\n",
    "    num_layers // 4,      # Q1 (Early)\n",
    "    num_layers // 2,      # Q2 (Middle)\n",
    "    3 * num_layers // 4,  # Q3 (Late)\n",
    "    num_layers - 1        # Q4 (Final)\n",
    "]\n",
    "print(f\"Quartile layers: {QUARTILE_LAYERS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import MMLUDataset\n",
    "\n",
    "# IMPORTANT: Use 'test' split (14K examples), not 'validation' (1.5K)\n",
    "# If you see \"Loaded 1531 examples\", you're using validation by mistake\n",
    "dataset = MMLUDataset(split=\"test\")  # \u2190 Make sure this says 'test'\n",
    "print(f\"Loaded {len(dataset)} MMLU examples\")\n",
    "print(f\"  (If this shows 1531, change split to 'test' instead of 'validation')\")\n",
    "\n",
    "NUM_SAMPLES = 5000  # Use 5000 out of 14K for robust evaluation\n",
    "examples = dataset.sample(NUM_SAMPLES, seed=42)\n",
    "print(f\"Using {len(examples)} examples for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Hidden States + Logits\n",
    "\n",
    "This is the key difference: we extract BOTH hidden states from multiple layers AND the logits for each answer choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def get_answer_token_ids(tokenizer, choices=['A', 'B', 'C', 'D']):\n",
    "    \"\"\"Get token IDs for answer choices.\"\"\"\n",
    "    token_ids = []\n",
    "    for choice in choices:\n",
    "        # Try different formats\n",
    "        ids = tokenizer.encode(choice, add_special_tokens=False)\n",
    "        if len(ids) == 1:\n",
    "            token_ids.append(ids[0])\n",
    "        else:\n",
    "            # Try with space prefix\n",
    "            ids = tokenizer.encode(f\" {choice}\", add_special_tokens=False)\n",
    "            token_ids.append(ids[-1])  # Take last token\n",
    "    return token_ids\n",
    "\n",
    "ANSWER_TOKEN_IDS = get_answer_token_ids(tokenizer)\n",
    "print(f\"Answer token IDs: {ANSWER_TOKEN_IDS}\")\n",
    "print(f\"Decoded: {[tokenizer.decode([tid]) for tid in ANSWER_TOKEN_IDS]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hidden_states_and_logits(model, tokenizer, extractor, examples, layers, answer_token_ids, batch_size=4):\n",
    "    \"\"\"\n",
    "    Extract hidden states from multiple layers AND logits for answer choices.\n",
    "    \n",
    "    Returns:\n",
    "        hidden_states: (num_examples, num_layers, hidden_dim)\n",
    "        logits: (num_examples, num_choices)\n",
    "        correctness: (num_examples,) binary labels\n",
    "    \"\"\"\n",
    "    all_hidden_states = []\n",
    "    all_logits = []\n",
    "    all_correctness = []\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, len(examples), batch_size), desc=\"Extracting\"):\n",
    "        batch_examples = examples[i:i+batch_size]\n",
    "        prompts = [ex.format_prompt(style=\"multiple_choice\") for ex in batch_examples]\n",
    "        \n",
    "        # Tokenize\n",
    "        encodings = tokenizer(\n",
    "            prompts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                **encodings,\n",
    "                output_hidden_states=True,\n",
    "                return_dict=True,\n",
    "            )\n",
    "        \n",
    "        # Get last token position for each example\n",
    "        attention_mask = encodings[\"attention_mask\"]\n",
    "        seq_lengths = attention_mask.sum(dim=1) - 1\n",
    "        \n",
    "        # Extract hidden states from each layer\n",
    "        batch_hiddens = []\n",
    "        for layer_idx in layers:\n",
    "            layer_hiddens = outputs.hidden_states[layer_idx + 1]  # +1 for embeddings\n",
    "            # Get last token hidden state\n",
    "            token_hiddens = layer_hiddens[\n",
    "                torch.arange(layer_hiddens.size(0), device=model.device),\n",
    "                seq_lengths\n",
    "            ]\n",
    "            batch_hiddens.append(token_hiddens.cpu().to(torch.float32).numpy())\n",
    "        \n",
    "        # Stack: (batch, num_layers, hidden_dim)\n",
    "        batch_hidden_states = np.stack(batch_hiddens, axis=1)\n",
    "        all_hidden_states.append(batch_hidden_states)\n",
    "        \n",
    "        # Extract logits for answer choices at last token position\n",
    "        last_logits = outputs.logits[\n",
    "            torch.arange(outputs.logits.size(0), device=model.device),\n",
    "            seq_lengths\n",
    "        ]  # (batch, vocab_size)\n",
    "        \n",
    "        # Get logits for answer token IDs only\n",
    "        answer_logits = last_logits[:, answer_token_ids]  # (batch, num_choices)\n",
    "        all_logits.append(answer_logits.cpu().to(torch.float32).numpy())\n",
    "        \n",
    "        # Determine correctness based on which answer has highest logit\n",
    "        predicted_answers = answer_logits.argmax(dim=-1).cpu().numpy()\n",
    "        correct_answers = np.array([ex.answer for ex in batch_examples])\n",
    "        batch_correctness = (predicted_answers == correct_answers).astype(np.float32)\n",
    "        all_correctness.append(batch_correctness)\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    hidden_states = np.concatenate(all_hidden_states, axis=0)\n",
    "    logits = np.concatenate(all_logits, axis=0)\n",
    "    correctness = np.concatenate(all_correctness, axis=0)\n",
    "    \n",
    "    return hidden_states, logits, correctness\n",
    "\n",
    "# Extract everything\n",
    "extractor = HiddenStateExtractor(model, tokenizer)\n",
    "hidden_states, logits, correctness = extract_hidden_states_and_logits(\n",
    "    model, tokenizer, extractor, examples, \n",
    "    QUARTILE_LAYERS, ANSWER_TOKEN_IDS,\n",
    "    batch_size=4\n",
    ")\n",
    "\n",
    "print(f\"\\nExtracted data shapes:\")\n",
    "print(f\"  Hidden states: {hidden_states.shape}  (examples, layers, hidden_dim)\")\n",
    "print(f\"  Logits: {logits.shape}  (examples, num_choices)\")\n",
    "print(f\"  Correctness: {correctness.shape}\")\n",
    "print(f\"\\nAccuracy (argmax logits): {correctness.mean():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Data for Different Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For Linear probe: use only middle layer\n",
    "middle_layer_idx = 1  # Index 1 = Q2 (middle layer) in our 4-layer extraction\n",
    "X_linear = hidden_states[:, middle_layer_idx, :]  # (examples, hidden_dim)\n",
    "\n",
    "# For LayerEnsemble: flatten all layers\n",
    "X_ensemble = hidden_states.reshape(hidden_states.shape[0], -1)  # (examples, num_layers * hidden_dim)\n",
    "\n",
    "# For MultiSource: flatten hidden states + concatenate logits\n",
    "X_multisource = np.concatenate([\n",
    "    hidden_states.reshape(hidden_states.shape[0], -1),  # (examples, num_layers * hidden_dim)\n",
    "    logits  # (examples, num_choices)\n",
    "], axis=1)\n",
    "\n",
    "y = correctness\n",
    "\n",
    "print(\"Data shapes for each architecture:\")\n",
    "print(f\"  Linear (single layer): {X_linear.shape}\")\n",
    "print(f\"  LayerEnsemble (multi-layer): {X_ensemble.shape}\")\n",
    "print(f\"  MultiSource (multi-layer + logits): {X_multisource.shape}\")\n",
    "\n",
    "# Split data (60% train, 20% val, 20% test)\n",
    "# Split Linear data\n",
    "X_train_lin, X_temp_lin, y_train, y_temp = train_test_split(\n",
    "    X_linear, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "X_val_lin, X_test_lin, y_val, y_test = train_test_split(\n",
    "    X_temp_lin, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "# Split Ensemble data (same indices)\n",
    "X_train_ens, X_temp_ens, _, _ = train_test_split(\n",
    "    X_ensemble, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "X_val_ens, X_test_ens, _, _ = train_test_split(\n",
    "    X_temp_ens, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "# Split MultiSource data (same indices)\n",
    "X_train_ms, X_temp_ms, _, _ = train_test_split(\n",
    "    X_multisource, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "X_val_ms, X_test_ms, _, _ = train_test_split(\n",
    "    X_temp_ms, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"\\nSplit sizes:\")\n",
    "print(f\"  Train: {len(X_train_lin)} (acc: {y_train.mean():.1%})\")\n",
    "print(f\"  Val:   {len(X_val_lin)} (acc: {y_val.mean():.1%})\")\n",
    "print(f\"  Test:  {len(X_test_lin)} (acc: {y_test.mean():.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.probes import (\n",
    "    CalibratedProbe,\n",
    "    build_default_network,\n",
    "    build_layer_ensemble_network,\n",
    "    build_multi_source_network,\n",
    ")\n",
    "\n",
    "NUM_LAYERS = len(QUARTILE_LAYERS)\n",
    "NUM_CHOICES = 4\n",
    "\n",
    "# Define architectures with their data\n",
    "ARCHITECTURES = {\n",
    "    \"Linear (middle layer)\": {\n",
    "        \"build_fn\": lambda: build_default_network(hidden_dim, hidden_dim=None),\n",
    "        \"X_train\": X_train_lin,\n",
    "        \"X_val\": X_val_lin,\n",
    "        \"X_test\": X_test_lin,\n",
    "    },\n",
    "    \"LayerEnsemble (4 layers)\": {\n",
    "        \"build_fn\": lambda: build_layer_ensemble_network(\n",
    "            input_dim=NUM_LAYERS * hidden_dim,\n",
    "            num_layers=NUM_LAYERS,\n",
    "            layer_probe_hidden=64,\n",
    "        ),\n",
    "        \"X_train\": X_train_ens,\n",
    "        \"X_val\": X_val_ens,\n",
    "        \"X_test\": X_test_ens,\n",
    "    },\n",
    "    \"MultiSource (4 layers + logits)\": {\n",
    "        \"build_fn\": lambda: build_multi_source_network(\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_layers=NUM_LAYERS,\n",
    "            num_choices=NUM_CHOICES,\n",
    "            use_logits=True,\n",
    "        ),\n",
    "        \"X_train\": X_train_ms,\n",
    "        \"X_val\": X_val_ms,\n",
    "        \"X_test\": X_test_ms,\n",
    "    },\n",
    "}\n",
    "\n",
    "print(f\"Testing {len(ARCHITECTURES)} architectures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train All Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, brier_score_loss\n",
    "\n",
    "def compute_ece(confidences, labels, num_bins=10):\n",
    "    \"\"\"Compute Expected Calibration Error.\"\"\"\n",
    "    bin_boundaries = np.linspace(0, 1, num_bins + 1)\n",
    "    ece = 0.0\n",
    "    for i in range(num_bins):\n",
    "        mask = (confidences > bin_boundaries[i]) & (confidences <= bin_boundaries[i + 1])\n",
    "        if mask.sum() > 0:\n",
    "            bin_conf = confidences[mask].mean()\n",
    "            bin_acc = labels[mask].mean()\n",
    "            ece += mask.sum() * abs(bin_conf - bin_acc)\n",
    "    return ece / len(confidences)\n",
    "\n",
    "# Training settings\n",
    "NUM_EPOCHS = 200\n",
    "PATIENCE = None  # No early stopping\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, config in ARCHITECTURES.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training: {name}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Build network and probe\n",
    "    network = config[\"build_fn\"]()\n",
    "    probe = CalibratedProbe(network=network)\n",
    "    \n",
    "    # Count parameters\n",
    "    num_params = sum(p.numel() for p in probe.parameters())\n",
    "    print(f\"Parameters: {num_params:,}\")\n",
    "    \n",
    "    # Train\n",
    "    history = probe.fit(\n",
    "        config[\"X_train\"], y_train,\n",
    "        config[\"X_val\"], y_val,\n",
    "        batch_size=32,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        patience=PATIENCE,\n",
    "        use_scheduler=True,\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    confidences = probe.predict(config[\"X_test\"])\n",
    "    predictions = (confidences > 0.5).astype(int)\n",
    "    \n",
    "    # Compute metrics\n",
    "    accuracy = (predictions == y_test).mean()\n",
    "    auroc = roc_auc_score(y_test, confidences)\n",
    "    brier = brier_score_loss(y_test, confidences)\n",
    "    ece = compute_ece(confidences, y_test)\n",
    "    \n",
    "    results[name] = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"auroc\": auroc,\n",
    "        \"brier\": brier,\n",
    "        \"ece\": ece,\n",
    "        \"num_params\": num_params,\n",
    "        \"best_epoch\": history[\"best_epoch\"],\n",
    "        \"confidences\": confidences,\n",
    "        \"probe\": probe,\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nTest Results:\")\n",
    "    print(f\"  Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"  AUROC:    {auroc:.3f}\")\n",
    "    print(f\"  Brier:    {brier:.4f}\")\n",
    "    print(f\"  ECE:      {ece:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create results DataFrame\n",
    "df = pd.DataFrame({\n",
    "    name: {\n",
    "        \"Accuracy\": f\"{r['accuracy']:.3f}\",\n",
    "        \"AUROC\": f\"{r['auroc']:.3f}\",\n",
    "        \"Brier Score\": f\"{r['brier']:.4f}\",\n",
    "        \"ECE\": f\"{r['ece']:.4f}\",\n",
    "        \"Parameters\": f\"{r['num_params']:,}\",\n",
    "    }\n",
    "    for name, r in results.items()\n",
    "}).T\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ARCHITECTURE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "names = list(results.keys())\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c']  # Blue, Green, Red\n",
    "\n",
    "# 1. AUROC comparison\n",
    "ax1 = axes[0]\n",
    "aurocs = [results[n][\"auroc\"] for n in names]\n",
    "bars = ax1.barh(names, aurocs, color=colors)\n",
    "ax1.set_xlabel(\"AUROC (higher is better)\")\n",
    "ax1.set_title(\"Discrimination: AUROC\")\n",
    "ax1.set_xlim(0.5, 1.0)\n",
    "for bar, val in zip(bars, aurocs):\n",
    "    ax1.text(val + 0.01, bar.get_y() + bar.get_height()/2, f\"{val:.3f}\", va='center')\n",
    "\n",
    "# 2. Brier Score comparison\n",
    "ax2 = axes[1]\n",
    "briers = [results[n][\"brier\"] for n in names]\n",
    "bars = ax2.barh(names, briers, color=colors)\n",
    "ax2.set_xlabel(\"Brier Score (lower is better)\")\n",
    "ax2.set_title(\"Calibration: Brier Score\")\n",
    "for bar, val in zip(bars, briers):\n",
    "    ax2.text(val + 0.005, bar.get_y() + bar.get_height()/2, f\"{val:.4f}\", va='center')\n",
    "\n",
    "# 3. ECE comparison\n",
    "ax3 = axes[2]\n",
    "eces = [results[n][\"ece\"] for n in names]\n",
    "bars = ax3.barh(names, eces, color=colors)\n",
    "ax3.set_xlabel(\"ECE (lower is better)\")\n",
    "ax3.set_title(\"Calibration: ECE\")\n",
    "for bar, val in zip(bars, eces):\n",
    "    ax3.text(val + 0.005, bar.get_y() + bar.get_height()/2, f\"{val:.4f}\", va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"multi_source_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSaved: multi_source_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Reliability Diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reliability_diagram(confidences, labels, title, ax, num_bins=10):\n",
    "    \"\"\"Plot reliability diagram.\"\"\"\n",
    "    bin_boundaries = np.linspace(0, 1, num_bins + 1)\n",
    "    bin_centers = (bin_boundaries[:-1] + bin_boundaries[1:]) / 2\n",
    "\n",
    "    bin_accs = []\n",
    "    bin_confs = []\n",
    "\n",
    "    for i in range(num_bins):\n",
    "        mask = (confidences > bin_boundaries[i]) & (confidences <= bin_boundaries[i + 1])\n",
    "        if mask.sum() > 0:\n",
    "            bin_accs.append(labels[mask].mean())\n",
    "            bin_confs.append(confidences[mask].mean())\n",
    "        else:\n",
    "            bin_accs.append(np.nan)\n",
    "            bin_confs.append(np.nan)\n",
    "\n",
    "    ax.bar(bin_centers, bin_accs, width=0.08, alpha=0.7, label='Accuracy')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', label='Perfect calibration')\n",
    "    ax.set_xlabel('Confidence')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend(loc='lower right')\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for i, (name, r) in enumerate(results.items()):\n",
    "    plot_reliability_diagram(\n",
    "        r[\"confidences\"], y_test,\n",
    "        f\"{name}\\nECE={r['ece']:.4f}\",\n",
    "        axes[i]\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"multi_source_reliability.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSaved: multi_source_reliability.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Analyze MultiSource Layer Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get learned layer weights from MultiSource probe\n",
    "ms_probe = results[\"MultiSource (4 layers + logits)\"][\"probe\"]\n",
    "layer_weights = ms_probe.network.get_layer_weights().numpy()\n",
    "\n",
    "print(\"Learned Layer Weights (MultiSource):\")\n",
    "print(\"=\"*40)\n",
    "layer_names = [\"Early (Q1)\", \"Middle (Q2)\", \"Late (Q3)\", \"Final (Q4)\"]\n",
    "for i, (layer_idx, weight) in enumerate(zip(QUARTILE_LAYERS, layer_weights)):\n",
    "    print(f\"  Layer {layer_idx:2d} ({layer_names[i]:12s}): {weight:.4f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "\n",
    "bar_labels = [f\"Layer {idx}\\n({name})\" for idx, name in zip(QUARTILE_LAYERS, layer_names)]\n",
    "colors = ['lightblue', 'blue', 'darkblue', 'navy']\n",
    "\n",
    "ax.bar(bar_labels, layer_weights, color=colors, edgecolor='black', linewidth=1.5)\n",
    "ax.set_ylabel('Learned Weight', fontsize=12)\n",
    "ax.set_title('MultiSource: Learned Layer Importance', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim([0, max(layer_weights) * 1.2])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, weight in enumerate(layer_weights):\n",
    "    ax.text(i, weight + 0.01, f'{weight:.3f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"multi_source_layer_weights.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSaved: multi_source_layer_weights.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"SUMMARY: Multi-Source Confidence Probe\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find best for each metric\n",
    "best_auroc_name = max(results, key=lambda x: results[x]['auroc'])\n",
    "best_brier_name = min(results, key=lambda x: results[x]['brier'])\n",
    "best_ece_name = min(results, key=lambda x: results[x]['ece'])\n",
    "\n",
    "print(f\"\\nBest AUROC:  {best_auroc_name}\")\n",
    "print(f\"  AUROC = {results[best_auroc_name]['auroc']:.4f}\")\n",
    "\n",
    "print(f\"\\nBest Brier:  {best_brier_name}\")\n",
    "print(f\"  Brier = {results[best_brier_name]['brier']:.4f}\")\n",
    "\n",
    "print(f\"\\nBest ECE:    {best_ece_name}\")\n",
    "print(f\"  ECE = {results[best_ece_name]['ece']:.4f}\")\n",
    "\n",
    "# Compare MultiSource to baselines\n",
    "ms_results = results[\"MultiSource (4 layers + logits)\"]\n",
    "lin_results = results[\"Linear (middle layer)\"]\n",
    "ens_results = results[\"LayerEnsemble (4 layers)\"]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MultiSource vs Baselines:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nAUROC improvement over Linear:      {ms_results['auroc'] - lin_results['auroc']:+.4f}\")\n",
    "print(f\"AUROC improvement over LayerEnsemble: {ms_results['auroc'] - ens_results['auroc']:+.4f}\")\n",
    "\n",
    "print(f\"\\nBrier improvement over Linear:      {lin_results['brier'] - ms_results['brier']:+.4f} (lower is better)\")\n",
    "print(f\"Brier improvement over LayerEnsemble: {ens_results['brier'] - ms_results['brier']:+.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY INSIGHT:\")\n",
    "print(\"=\"*70)\n",
    "if ms_results['auroc'] > max(lin_results['auroc'], ens_results['auroc']):\n",
    "    print(\"MultiSource (hidden states + logits) outperforms both baselines!\")\n",
    "    print(\"\u2192 Combining internal uncertainty with expressed confidence helps.\")\n",
    "elif ms_results['auroc'] > lin_results['auroc']:\n",
    "    print(\"MultiSource outperforms single-layer Linear but not LayerEnsemble.\")\n",
    "    print(\"\u2192 Multi-layer information helps, but logits may not add much.\")\n",
    "else:\n",
    "    print(\"Simple Linear probe performs competitively.\")\n",
    "    print(\"\u2192 Task may be linearly separable in hidden space.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}