{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probe Architecture Comparison\n",
    "\n",
    "This notebook compares 8 novel probe architectures for predicting model confidence.\n",
    "\n",
    "**Architectures tested:**\n",
    "1. Default MLP (baseline)\n",
    "2. AttentionProbe - Self-attention over hidden state chunks\n",
    "3. ResidualProbe - Deep MLP with skip connections\n",
    "4. BottleneckProbe - Compression to low-rank representation\n",
    "5. MultiHeadProbe - Multiple experts with learned aggregation\n",
    "6. GatedProbe - GLU-style gating\n",
    "7. SparseProbe - Top-k dimension selection\n",
    "8. HeteroscedasticProbe - Per-example uncertainty\n",
    "9. BilinearProbe - Explicit feature interactions\n",
    "\n",
    "**Setup:** Run on Google Colab with GPU (Runtime > Change runtime type > T4 GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (Colab)\n",
    "!pip install -q transformers accelerate bitsandbytes datasets tqdm matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo and setup path\n",
    "!git clone https://github.com/joshcliu/deep-learning.git\n",
    "%cd deep-learning\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch for bfloat16 compatibility (8-bit quantized models)\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Monkey-patch the extractor to handle bfloat16\n",
    "from src.models import extractor as extractor_module\n",
    "\n",
    "_original_extract_batch = extractor_module.HiddenStateExtractor._extract_batch\n",
    "\n",
    "def _patched_extract_batch(self, texts, layers, max_length):\n",
    "    \"\"\"Patched to handle bfloat16 tensors.\"\"\"\n",
    "    inputs = self.tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    ).to(self.model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = self.model(**inputs, output_hidden_states=True)\n",
    "\n",
    "    hidden_states = outputs.hidden_states\n",
    "    batch_hiddens = []\n",
    "\n",
    "    for layer_idx in layers:\n",
    "        layer_hidden = hidden_states[layer_idx + 1]\n",
    "        token_hiddens = layer_hidden[:, -1, :]\n",
    "        # Fix: Convert via .tolist() to handle bfloat16\n",
    "        batch_hiddens.append(np.array(token_hiddens.detach().cpu().tolist(), dtype=np.float32))\n",
    "\n",
    "    return np.stack(batch_hiddens, axis=1)\n",
    "\n",
    "extractor_module.HiddenStateExtractor._extract_batch = _patched_extract_batch\n",
    "print(\"Patched HiddenStateExtractor for bfloat16 compatibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.models import ModelLoader, HiddenStateExtractor\n\n# Load Mistral 7B (ungated, no approval needed)\nmodel_name = \"mistralai/Mistral-7B-v0.1\"\nloader = ModelLoader(model_name)\nmodel, tokenizer = loader.load(quantization=\"8bit\", device_map=\"auto\")\n\nprint(f\"Loaded {model_name}\")\nprint(f\"Layers: {loader.config.num_layers}, Hidden dim: {loader.config.hidden_dim}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import MMLUDataset\n",
    "\n",
    "# Load MMLU validation set\n",
    "dataset = MMLUDataset(split=\"validation\")\n",
    "print(f\"Loaded {len(dataset)} examples\")\n",
    "\n",
    "# Sample for experiment\n",
    "NUM_SAMPLES = 300  # Increase for better results\n",
    "examples = dataset.sample(NUM_SAMPLES, seed=42)\n",
    "print(f\"Using {len(examples)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Answers & Check Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def generate_answer(model, tokenizer, prompt, max_new_tokens=32):\n",
    "    \"\"\"Generate model's answer to a question.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,  # Greedy for reproducibility\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    generated = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    return generated.strip()\n",
    "\n",
    "def check_correctness(generated: str, example) -> bool:\n",
    "    \"\"\"Check if generated answer matches correct answer.\"\"\"\n",
    "    correct_answer = example.choices[example.answer]\n",
    "    correct_letter = chr(65 + example.answer)  # A, B, C, D\n",
    "    \n",
    "    generated_lower = generated.lower().strip()\n",
    "    \n",
    "    # Check for letter match\n",
    "    if generated_lower.startswith(correct_letter.lower()):\n",
    "        return True\n",
    "    \n",
    "    # Check for answer text match\n",
    "    if correct_answer.lower() in generated_lower:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Generate answers for all examples\n",
    "print(\"Generating model answers...\")\n",
    "prompts = []\n",
    "correctness = []\n",
    "\n",
    "for example in tqdm(examples):\n",
    "    prompt = example.format_prompt(style=\"multiple_choice\")\n",
    "    prompts.append(prompt)\n",
    "    \n",
    "    generated = generate_answer(model, tokenizer, prompt)\n",
    "    is_correct = check_correctness(generated, example)\n",
    "    correctness.append(int(is_correct))\n",
    "\n",
    "correctness = np.array(correctness)\n",
    "print(f\"\\nAccuracy: {correctness.mean():.1%} ({correctness.sum()}/{len(correctness)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract Hidden States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract from middle layer (where uncertainty signal is strongest)\n",
    "LAYER = 16  # Middle of 32 layers\n",
    "\n",
    "extractor = HiddenStateExtractor(model, tokenizer)\n",
    "hidden_states = extractor.extract(\n",
    "    texts=prompts,\n",
    "    layers=[LAYER],\n",
    "    batch_size=8,\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "# Shape: (num_examples, 1, hidden_dim) -> (num_examples, hidden_dim)\n",
    "X = hidden_states[:, 0, :]\n",
    "y = correctness\n",
    "\n",
    "print(f\"Hidden states shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train/Val/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 60% train, 20% val, 20% test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(f\"Train: {len(X_train)} (acc: {y_train.mean():.1%})\")\n",
    "print(f\"Val:   {len(X_val)} (acc: {y_val.mean():.1%})\")\n",
    "print(f\"Test:  {len(X_test)} (acc: {y_test.mean():.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.probes import (\n",
    "    CalibratedProbe,\n",
    "    build_default_network,\n",
    "    build_attention_network,\n",
    "    build_residual_network,\n",
    "    build_bottleneck_network,\n",
    "    build_multihead_network,\n",
    "    build_gated_network,\n",
    "    build_sparse_network,\n",
    "    build_heteroscedastic_network,\n",
    "    build_bilinear_network,\n",
    ")\n",
    "\n",
    "INPUT_DIM = X.shape[1]  # 4096 for Mistral\n",
    "\n",
    "# Define all architectures to test\n",
    "ARCHITECTURES = {\n",
    "    \"Default MLP\": lambda: build_default_network(INPUT_DIM, hidden_dim=256),\n",
    "    \"Attention\": lambda: build_attention_network(INPUT_DIM, num_chunks=16, num_heads=4),\n",
    "    \"Residual\": lambda: build_residual_network(INPUT_DIM, hidden_dim=256, num_blocks=3),\n",
    "    \"Bottleneck\": lambda: build_bottleneck_network(INPUT_DIM, bottleneck_dim=64),\n",
    "    \"MultiHead\": lambda: build_multihead_network(INPUT_DIM, num_heads=4, head_dim=128),\n",
    "    \"Gated\": lambda: build_gated_network(INPUT_DIM, hidden_dim=256, num_layers=2),\n",
    "    \"Sparse (k=256)\": lambda: build_sparse_network(INPUT_DIM, k=256, hidden_dim=128),\n",
    "    \"Heteroscedastic\": lambda: build_heteroscedastic_network(INPUT_DIM, hidden_dim=256),\n",
    "    \"Bilinear\": lambda: build_bilinear_network(INPUT_DIM, num_factors=32, hidden_dim=128),\n",
    "}\n",
    "\n",
    "print(f\"Testing {len(ARCHITECTURES)} architectures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train All Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.metrics import roc_auc_score, brier_score_loss\n\ndef compute_ece(confidences, labels, num_bins=10):\n    \"\"\"Compute Expected Calibration Error.\"\"\"\n    bin_boundaries = np.linspace(0, 1, num_bins + 1)\n    ece = 0.0\n    \n    for i in range(num_bins):\n        mask = (confidences > bin_boundaries[i]) & (confidences <= bin_boundaries[i + 1])\n        if mask.sum() > 0:\n            bin_conf = confidences[mask].mean()\n            bin_acc = labels[mask].mean()\n            ece += mask.sum() * abs(bin_conf - bin_acc)\n    \n    return ece / len(confidences)\n\n# Store results\nresults = {}\n\n# Training settings - NO early stopping, use scheduler for better convergence\nNUM_EPOCHS = 200\nPATIENCE = None  # Disable early stopping - train for full epochs\n\nfor name, build_fn in ARCHITECTURES.items():\n    print(f\"\\n{'='*50}\")\n    print(f\"Training: {name}\")\n    print('='*50)\n    \n    # Build network and probe\n    network = build_fn()\n    probe = CalibratedProbe(network=network)\n    \n    # Count parameters\n    num_params = sum(p.numel() for p in probe.parameters())\n    print(f\"Parameters: {num_params:,}\")\n    \n    # Train with NO early stopping\n    history = probe.fit(\n        X_train, y_train,\n        X_val, y_val,\n        batch_size=32,\n        num_epochs=NUM_EPOCHS,\n        patience=PATIENCE,  # None = no early stopping\n        use_scheduler=True,  # Cosine annealing LR\n        verbose=True,\n    )\n    \n    # Evaluate on test set\n    confidences = probe.predict(X_test)\n    predictions = (confidences > 0.5).astype(int)\n    \n    # Compute metrics\n    accuracy = (predictions == y_test).mean()\n    auroc = roc_auc_score(y_test, confidences)\n    brier = brier_score_loss(y_test, confidences)\n    ece = compute_ece(confidences, y_test)\n    \n    results[name] = {\n        \"accuracy\": accuracy,\n        \"auroc\": auroc,\n        \"brier\": brier,\n        \"ece\": ece,\n        \"num_params\": num_params,\n        \"best_epoch\": history[\"best_epoch\"],\n        \"confidences\": confidences,\n    }\n    \n    print(f\"\\nTest Results:\")\n    print(f\"  Accuracy: {accuracy:.3f}\")\n    print(f\"  AUROC:    {auroc:.3f}\")\n    print(f\"  Brier:    {brier:.4f}\")\n    print(f\"  ECE:      {ece:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create results DataFrame\n",
    "df = pd.DataFrame({\n",
    "    name: {\n",
    "        \"Accuracy\": f\"{r['accuracy']:.3f}\",\n",
    "        \"AUROC\": f\"{r['auroc']:.3f}\",\n",
    "        \"Brier Score\": f\"{r['brier']:.4f}\",\n",
    "        \"ECE\": f\"{r['ece']:.4f}\",\n",
    "        \"Parameters\": f\"{r['num_params']:,}\",\n",
    "    }\n",
    "    for name, r in results.items()\n",
    "}).T\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ARCHITECTURE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "names = list(results.keys())\n",
    "colors = sns.color_palette(\"husl\", len(names))\n",
    "\n",
    "# 1. AUROC comparison\n",
    "ax1 = axes[0, 0]\n",
    "aurocs = [results[n][\"auroc\"] for n in names]\n",
    "bars = ax1.barh(names, aurocs, color=colors)\n",
    "ax1.set_xlabel(\"AUROC (higher is better)\")\n",
    "ax1.set_title(\"Discrimination: AUROC\")\n",
    "ax1.set_xlim(0.5, 1.0)\n",
    "for bar, val in zip(bars, aurocs):\n",
    "    ax1.text(val + 0.01, bar.get_y() + bar.get_height()/2, f\"{val:.3f}\", va='center')\n",
    "\n",
    "# 2. Brier Score comparison\n",
    "ax2 = axes[0, 1]\n",
    "briers = [results[n][\"brier\"] for n in names]\n",
    "bars = ax2.barh(names, briers, color=colors)\n",
    "ax2.set_xlabel(\"Brier Score (lower is better)\")\n",
    "ax2.set_title(\"Calibration: Brier Score\")\n",
    "for bar, val in zip(bars, briers):\n",
    "    ax2.text(val + 0.005, bar.get_y() + bar.get_height()/2, f\"{val:.4f}\", va='center')\n",
    "\n",
    "# 3. ECE comparison\n",
    "ax3 = axes[1, 0]\n",
    "eces = [results[n][\"ece\"] for n in names]\n",
    "bars = ax3.barh(names, eces, color=colors)\n",
    "ax3.set_xlabel(\"ECE (lower is better)\")\n",
    "ax3.set_title(\"Calibration: Expected Calibration Error\")\n",
    "for bar, val in zip(bars, eces):\n",
    "    ax3.text(val + 0.005, bar.get_y() + bar.get_height()/2, f\"{val:.4f}\", va='center')\n",
    "\n",
    "# 4. Parameters vs AUROC (efficiency)\n",
    "ax4 = axes[1, 1]\n",
    "params = [results[n][\"num_params\"] for n in names]\n",
    "for i, name in enumerate(names):\n",
    "    ax4.scatter(params[i], aurocs[i], s=150, c=[colors[i]], label=name, edgecolors='black')\n",
    "ax4.set_xlabel(\"Number of Parameters\")\n",
    "ax4.set_ylabel(\"AUROC\")\n",
    "ax4.set_title(\"Efficiency: Parameters vs Performance\")\n",
    "ax4.set_xscale('log')\n",
    "ax4.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"architecture_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSaved: architecture_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reliability Diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reliability_diagram(confidences, labels, title, ax, num_bins=10):\n",
    "    \"\"\"Plot reliability diagram.\"\"\"\n",
    "    bin_boundaries = np.linspace(0, 1, num_bins + 1)\n",
    "    bin_centers = (bin_boundaries[:-1] + bin_boundaries[1:]) / 2\n",
    "    \n",
    "    bin_accs = []\n",
    "    bin_confs = []\n",
    "    bin_counts = []\n",
    "    \n",
    "    for i in range(num_bins):\n",
    "        mask = (confidences > bin_boundaries[i]) & (confidences <= bin_boundaries[i + 1])\n",
    "        if mask.sum() > 0:\n",
    "            bin_accs.append(labels[mask].mean())\n",
    "            bin_confs.append(confidences[mask].mean())\n",
    "            bin_counts.append(mask.sum())\n",
    "        else:\n",
    "            bin_accs.append(np.nan)\n",
    "            bin_confs.append(np.nan)\n",
    "            bin_counts.append(0)\n",
    "    \n",
    "    # Plot\n",
    "    ax.bar(bin_centers, bin_accs, width=0.08, alpha=0.7, label='Accuracy')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', label='Perfect calibration')\n",
    "    ax.set_xlabel('Confidence')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend(loc='lower right')\n",
    "\n",
    "# Plot reliability diagrams for top architectures\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (name, r) in enumerate(results.items()):\n",
    "    if i < 9:\n",
    "        plot_reliability_diagram(\n",
    "            r[\"confidences\"], y_test,\n",
    "            f\"{name}\\nECE={r['ece']:.4f}\",\n",
    "            axes[i]\n",
    "        )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"reliability_diagrams.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSaved: reliability_diagrams.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best architectures for each metric\n",
    "best_auroc = max(results.items(), key=lambda x: x[1][\"auroc\"])\n",
    "best_brier = min(results.items(), key=lambda x: x[1][\"brier\"])\n",
    "best_ece = min(results.items(), key=lambda x: x[1][\"ece\"])\n",
    "best_efficiency = max(results.items(), key=lambda x: x[1][\"auroc\"] / np.log10(x[1][\"num_params\"]))\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBest Discrimination (AUROC): {best_auroc[0]}\")\n",
    "print(f\"  AUROC = {best_auroc[1]['auroc']:.4f}\")\n",
    "\n",
    "print(f\"\\nBest Calibration (Brier): {best_brier[0]}\")\n",
    "print(f\"  Brier = {best_brier[1]['brier']:.4f}\")\n",
    "\n",
    "print(f\"\\nBest Calibration (ECE): {best_ece[0]}\")\n",
    "print(f\"  ECE = {best_ece[1]['ece']:.4f}\")\n",
    "\n",
    "print(f\"\\nMost Efficient (AUROC/log(params)): {best_efficiency[0]}\")\n",
    "print(f\"  AUROC = {best_efficiency[1]['auroc']:.4f}, Params = {best_efficiency[1]['num_params']:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NOTES\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "- Lower Brier/ECE = better calibration (confidence matches accuracy)\n",
    "- Higher AUROC = better discrimination (separating correct/incorrect)\n",
    "- SparseProbe is most interpretable (shows which dimensions matter)\n",
    "- HeteroscedasticProbe handles varying example difficulty\n",
    "- For production: prioritize calibration (Brier/ECE)\n",
    "- For research: prioritize AUROC and interpretability\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Sparse Probe Analysis (Interpretability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# If SparseProbe performed well, analyze which dimensions it selected\nfrom src.probes.architectures import TopKSparseNetwork\n\n# Retrain sparse probe to access its learned importance weights\nsparse_network = build_sparse_network(INPUT_DIM, k=256, hidden_dim=128)\nsparse_probe = CalibratedProbe(network=sparse_network)\nsparse_probe.fit(X_train, y_train, X_val, y_val, num_epochs=200, patience=None, verbose=False)\n\n# Get importance scores using the new API\nimportance = sparse_probe.network.get_importance_scores().cpu().numpy()\n\n# Find top dimensions\ntop_k = 20\ntop_indices = np.argsort(importance)[-top_k:][::-1]\ntop_scores = importance[top_indices]\n\nprint(f\"Top {top_k} most important dimensions for confidence:\")\nprint(\"=\"*40)\nfor idx, score in zip(top_indices, top_scores):\n    print(f\"  Dim {idx:4d}: importance = {score:.4f}\")\n\n# Plot importance distribution\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.bar(range(top_k), top_scores)\nplt.xlabel(\"Rank\")\nplt.ylabel(\"Importance Score\")\nplt.title(f\"Top {top_k} Dimensions\")\nplt.xticks(range(top_k), top_indices, rotation=45)\n\nplt.subplot(1, 2, 2)\nplt.hist(importance, bins=50, edgecolor='black')\nplt.xlabel(\"Importance Score\")\nplt.ylabel(\"Count\")\nplt.title(\"Importance Distribution (All Dimensions)\")\n# Show threshold for top-256 dimensions\nsorted_importance = np.sort(importance)[::-1]\nif len(sorted_importance) > 256:\n    threshold = sorted_importance[255]\n    plt.axvline(threshold, color='r', linestyle='--', label=f'Top 256 threshold ({threshold:.3f})')\n    plt.legend()\n\nplt.tight_layout()\nplt.savefig(\"sparse_probe_importance.png\", dpi=150)\nplt.show()\n\nprint(\"\\nSaved: sparse_probe_importance.png\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}