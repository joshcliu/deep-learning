{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Multi-Scale Probe Analysis\n",
    "\n",
    "This notebook analyzes the results from the hierarchical probing experiment and compares\n",
    "them against baseline methods (linear and MLP probes).\n",
    "\n",
    "## Contents\n",
    "1. Load experiment results and probes\n",
    "2. Calibration analysis and reliability diagrams\n",
    "3. Baseline comparisons\n",
    "4. Hierarchical level analysis (token, span, semantic, global)\n",
    "5. Performance by subject/category\n",
    "6. Uncertainty distribution analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from src.utils import load_config\n",
    "from src.models import ModelLoader, HiddenStateExtractor\n",
    "from src.data import MMLUDataset\n",
    "from src.probes import HierarchicalProbe, LinearProbe, MLPProbe\n",
    "from src.evaluation import CalibrationMetrics\n",
    "from src.evaluation.calibration import plot_reliability_diagram, plot_roc_curve\n",
    "\n",
    "# Configure plotting\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Experiment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify experiment output directory\n",
    "experiment_dir = project_root / \"outputs\" / \"hierarchical\" / \"llama-3.1-8b-hierarchical-probe\"\n",
    "\n",
    "# Load configuration\n",
    "config = load_config(project_root / \"configs\" / \"hierarchical_probe.yaml\")\n",
    "print(f\"Experiment: {config.experiment.name}\")\n",
    "print(f\"Model: {config.model.name}\")\n",
    "print(f\"Dataset: {config.data.dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved probes\n",
    "hierarchical_probe_path = experiment_dir / \"hierarchical\" / \"probe.pt\"\n",
    "linear_probe_path = experiment_dir / \"linear\" / \"probe.pt\"\n",
    "mlp_probe_path = experiment_dir / \"mlp\" / \"probe.pt\"\n",
    "\n",
    "# Check which probes exist\n",
    "print(f\"Hierarchical probe exists: {hierarchical_probe_path.exists()}\")\n",
    "print(f\"Linear probe exists: {linear_probe_path.exists()}\")\n",
    "print(f\"MLP probe exists: {mlp_probe_path.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset and Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MMLU dataset\n",
    "dataset = MMLUDataset(\n",
    "    split=\"test\",\n",
    "    subjects=config.data.get(\"subjects\", None),\n",
    "    max_samples=config.data.get(\"num_samples\", None),\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(dataset)} examples\")\n",
    "\n",
    "# Get statistics\n",
    "stats = dataset.get_statistics()\n",
    "print(f\"\\nCategories: {list(stats['category_counts'].keys())}\")\n",
    "print(f\"Number of subjects: {stats['num_subjects']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot category distribution\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "categories = list(stats['category_counts'].keys())\n",
    "counts = list(stats['category_counts'].values())\n",
    "\n",
    "ax.bar(categories, counts)\n",
    "ax.set_xlabel(\"Category\")\n",
    "ax.set_ylabel(\"Number of Examples\")\n",
    "ax.set_title(\"MMLU Dataset Distribution by Category\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calibration Analysis\n",
    "\n",
    "Visualize calibration quality using reliability diagrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test predictions (you'll need to save these during experiment)\n",
    "# For now, we'll demonstrate with synthetic data\n",
    "\n",
    "# TODO: Load actual predictions from experiment\n",
    "# hierarchical_confidences = np.load(experiment_dir / \"hierarchical\" / \"test_confidences.npy\")\n",
    "# linear_confidences = np.load(experiment_dir / \"linear\" / \"test_confidences.npy\")\n",
    "# mlp_confidences = np.load(experiment_dir / \"mlp\" / \"test_confidences.npy\")\n",
    "# test_labels = np.load(experiment_dir / \"test_labels.npy\")\n",
    "\n",
    "print(\"NOTE: Load actual prediction data from the experiment directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reliability diagrams for comparison\n",
    "# Uncomment when you have actual data\n",
    "\n",
    "# fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# # Hierarchical\n",
    "# plot_reliability_diagram(\n",
    "#     (hierarchical_confidences > 0.5).astype(int),\n",
    "#     hierarchical_confidences,\n",
    "#     test_labels,\n",
    "#     num_bins=10,\n",
    "#     ax=axes[0]\n",
    "# )\n",
    "# axes[0].set_title(\"Hierarchical Probe\")\n",
    "\n",
    "# # Linear\n",
    "# plot_reliability_diagram(\n",
    "#     (linear_confidences > 0.5).astype(int),\n",
    "#     linear_confidences,\n",
    "#     test_labels,\n",
    "#     num_bins=10,\n",
    "#     ax=axes[1]\n",
    "# )\n",
    "# axes[1].set_title(\"Linear Probe\")\n",
    "\n",
    "# # MLP\n",
    "# plot_reliability_diagram(\n",
    "#     (mlp_confidences > 0.5).astype(int),\n",
    "#     mlp_confidences,\n",
    "#     test_labels,\n",
    "#     num_bins=10,\n",
    "#     ax=axes[2]\n",
    "# )\n",
    "# axes[2].set_title(\"MLP Probe\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Metrics Comparison\n",
    "\n",
    "Compare ECE, Brier, AUROC, and Accuracy across methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example results (replace with actual results)\n",
    "results = {\n",
    "    \"Method\": [\"Hierarchical\", \"Linear\", \"MLP\"],\n",
    "    \"ECE\": [0.0450, 0.0820, 0.0680],  # Example values\n",
    "    \"Brier\": [0.1200, 0.1580, 0.1420],\n",
    "    \"AUROC\": [0.8650, 0.8120, 0.8350],\n",
    "    \"Accuracy\": [0.8420, 0.8200, 0.8310],\n",
    "    \"Params\": [450_000, 4_100, 2_100_000],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metrics comparison\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n",
    "\n",
    "metrics = [\"ECE\", \"Brier\", \"AUROC\", \"Accuracy\"]\n",
    "colors = ['red', 'blue', 'green']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    axes[i].bar(df[\"Method\"], df[metric], color=colors)\n",
    "    axes[i].set_ylabel(metric)\n",
    "    axes[i].set_title(f\"{metric} Comparison\")\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for j, v in enumerate(df[metric]):\n",
    "        axes[i].text(j, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hierarchical Level Analysis\n",
    "\n",
    "Analyze confidence scores at different levels of the hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze intermediate predictions from hierarchical probe\n",
    "# This requires saving intermediate predictions during inference\n",
    "\n",
    "# Example: Load intermediate predictions\n",
    "# token_confidences = np.load(experiment_dir / \"hierarchical\" / \"token_confidences.npy\")\n",
    "# span_confidences = np.load(experiment_dir / \"hierarchical\" / \"span_confidences.npy\")\n",
    "# semantic_confidences = np.load(experiment_dir / \"hierarchical\" / \"semantic_confidences.npy\")\n",
    "# global_confidences = np.load(experiment_dir / \"hierarchical\" / \"global_confidences.npy\")\n",
    "\n",
    "print(\"NOTE: Analyze intermediate predictions from hierarchical levels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confidence distributions at each level\n",
    "# Uncomment when you have actual data\n",
    "\n",
    "# fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# levels = [\n",
    "#     (\"Token\", token_confidences.mean(axis=1)),\n",
    "#     (\"Span\", span_confidences),\n",
    "#     (\"Semantic\", semantic_confidences),\n",
    "#     (\"Global\", global_confidences),\n",
    "# ]\n",
    "\n",
    "# for idx, (level_name, confidences) in enumerate(levels):\n",
    "#     ax = axes[idx // 2, idx % 2]\n",
    "#     ax.hist(confidences, bins=50, alpha=0.7, edgecolor='black')\n",
    "#     ax.set_xlabel(\"Confidence\")\n",
    "#     ax.set_ylabel(\"Frequency\")\n",
    "#     ax.set_title(f\"{level_name}-Level Confidence Distribution\")\n",
    "#     ax.axvline(confidences.mean(), color='red', linestyle='--', \n",
    "#                label=f'Mean: {confidences.mean():.3f}')\n",
    "#     ax.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance by Subject/Category\n",
    "\n",
    "Analyze how well the hierarchical probe performs across different subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Break down performance by MMLU category\n",
    "# Group test examples by category and compute metrics separately\n",
    "\n",
    "# Example structure:\n",
    "# for category in [\"STEM\", \"Humanities\", \"Social Sciences\", \"Other\"]:\n",
    "#     category_examples = dataset.get_by_category(category)\n",
    "#     # Compute metrics for this category\n",
    "\n",
    "print(\"NOTE: Analyze performance breakdown by MMLU categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ECE Improvement Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate improvement percentages\n",
    "linear_ece = 0.0820  # Example value\n",
    "hierarchical_ece = 0.0450  # Example value\n",
    "\n",
    "improvement = (linear_ece - hierarchical_ece) / linear_ece * 100\n",
    "\n",
    "print(f\"ECE Improvement over Linear Baseline: {improvement:.1f}%\")\n",
    "print(f\"\\nLinear ECE: {linear_ece:.4f}\")\n",
    "print(f\"Hierarchical ECE: {hierarchical_ece:.4f}\")\n",
    "print(f\"Absolute reduction: {linear_ece - hierarchical_ece:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Selective Prediction Analysis\n",
    "\n",
    "Analyze coverage-accuracy tradeoff for selective prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement selective prediction curves\n",
    "# Sort by confidence, progressively exclude low-confidence predictions\n",
    "# Plot accuracy vs. coverage\n",
    "\n",
    "print(\"NOTE: Implement selective prediction analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Complexity vs Performance\n",
    "\n",
    "Visualize the tradeoff between model complexity and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ECE vs. number of parameters\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "methods = df[\"Method\"]\n",
    "params = df[\"Params\"]\n",
    "ece_values = df[\"ECE\"]\n",
    "\n",
    "scatter = ax.scatter(params, ece_values, s=200, alpha=0.6)\n",
    "\n",
    "# Add labels\n",
    "for i, method in enumerate(methods):\n",
    "    ax.annotate(method, (params[i], ece_values[i]), \n",
    "                xytext=(10, 10), textcoords='offset points',\n",
    "                fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel(\"Number of Parameters\")\n",
    "ax.set_ylabel(\"Expected Calibration Error (ECE)\")\n",
    "ax.set_title(\"Model Complexity vs Calibration Performance\")\n",
    "ax.set_xscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusions\n",
    "\n",
    "Summary of findings:\n",
    "\n",
    "1. **Calibration Quality**: The hierarchical probe achieves X% improvement in ECE over the linear baseline\n",
    "2. **Multi-Scale Benefits**: Different levels of the hierarchy capture complementary uncertainty signals\n",
    "3. **Complexity Tradeoff**: Hierarchical probe offers good balance between performance and model size\n",
    "4. **Domain Generalization**: Performance analysis across MMLU categories shows...\n",
    "\n",
    "### Next Steps\n",
    "- Test on other datasets (TriviaQA, GSM8K)\n",
    "- Experiment with different model architectures\n",
    "- Analyze failure cases\n",
    "- Compare with CCPS and semantic entropy methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
